= Curso de Fine-Tuning Eficiente con Unsloth
:doctype: book
:toc:
:toclevels: 4
:sectnums:
:icons: font
:source-highlighter: highlight.js
:highlightjsdir: https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0
:highlightjs-theme: atom-one-light
:data-uri:

== MÃ³dulo 1: Fundamentos y ConfiguraciÃ³n

=== 1.1 Â¿QuÃ© es Unsloth?

Unsloth es una librerÃ­a de optimizaciÃ³n que te permite entrenar modelos de lenguaje grande (LLMs) de forma **2x mÃ¡s rÃ¡pida y usando 80% menos memoria** en comparaciÃ³n con mÃ©todos tradicionales. EstÃ¡ construida sobre Hugging Face ğŸ¤— pero optimiza la computaciÃ³n en GPU.

==== El Problema del Fine-Tuning Tradicional

Cuando quieres adaptar un modelo pre-entrenado como Llama 2 a tu tarea especÃ­fica, enfrentas estos desafÃ­os:

[source, python]
----
# Consumo de memoria tÃ­pico SIN Unsloth
Modelo Llama 2 7B:
  - ParÃ¡metros del modelo: 28 GB (en float32)
  - Gradientes durante backprop: 28 GB
  - Optimizer states (Adam): 56 GB
  - Activaciones intermedias: 12 GB
  - Batch x Secuencia: 8 GB
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  TOTAL APROXIMADO: 132 GB âŒ

# Con GPU tÃ­pica (A100 80GB):
  La mayorÃ­a de parÃ¡metros no caben en GPU directamente
----

Esto obliga a usar tÃ©cnicas como **Gradient Checkpointing** (lento) o entrenar en **CPU** (muy lento).

==== CÃ³mo Unsloth Resuelve el Problema

Unsloth reescribe los kernels CUDA (operaciones en GPU) de forma mÃ¡s eficiente:

[source, python]
----
# OPTIMIZACIÃ“N 1: Forward pass combinado
# Tradicional: AtenciÃ³n + FFN = 2 kernels separados = copias innecesarias
# Unsloth: AtenciÃ³n + FFN = 1 kernel fusionado = sin copias

# OPTIMIZACIÃ“N 2: Menos precisiÃ³n donde no importa
# Los cÃ¡lculos intermedios usan float32, pero estratÃ©gicamente
# se reduce a float16 en partes que no afectan precisiÃ³n

# OPTIMIZACIÃ“N 3: RoPE (Rotary Position Embedding) vectorizado
# En lugar de NÂ² cÃ¡lculos de atenciÃ³n, vectoriza operaciones

# RESULTADO:
# - Velocidad: 2x mÃ¡s rÃ¡pido âš¡
# - Memoria: 80% menos consumo ğŸ’¾
# - PrecisiÃ³n: Mantiene la misma calidad de entrenamiento âœ“
----

==== Comparativa de Rendimiento: Hugging Face vs. Unsloth

[cols="30,20,20,20"]
|===
| MÃ©trica | Baseline (HF) | Unsloth | Mejora

| Tiempo por epoch (Llama 2 7B)
| 120 minutos
| 60 minutos
| 2x mÃ¡s rÃ¡pido

| Memoria GPU usada
| 40 GB (A100)
| 8 GB (A100)
| 5x menos

| Throughput (tokens/seg)
| 500 tokens
| 1000 tokens
| 2x mejor

| Costo (AWS, por hora)
| $10.32 (A100 80GB)
| $3.06 (A10 24GB)
| 3x mÃ¡s barato

| PrecisiÃ³n de salida
| 100%
| 100%
| IdÃ©ntica
|===

**Â¿CÃ³mo es esto posible?** Unsloth usa *kernel fusion* - combina operaciones GPU separadas en una sola operaciÃ³n, reduciendo transferencias de memoria.

==== Modelos Soportados Actualmente

Unsloth soporta los modelos mÃ¡s populares:

[source, text]
----
âœ… Totalmente optimizados (mejor rendimiento):
  â€¢ Llama 2 (7B, 13B, 70B)
  â€¢ Llama 3 / 3.1 (8B, 70B)
  â€¢ Mistral (7B, 8x7B)
  â€¢ Gemma (2B, 7B)
  â€¢ Qwen (7B, 14B)
  â€¢ Yi (6B, 34B)

âœ… Funcionan (optimizaciones parciales):
  â€¢ Phi (2.7B)
  â€¢ DPO / Reward models
  â€¢ StableLM
  â€¢ OLMo

ğŸ“‹ Otros modelos de Hugging Face funcionan pero sin optimizaciones
----

**Ejemplo: Cargar un modelo soportado**

[source, python]
----
from unsloth import FastLanguageModel

# Unsloth detecta automÃ¡ticamente la arquitectura
modelo, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-2-7b",
    max_seq_length=2048,
    load_in_4bit=True,  # CuantizaciÃ³n 4-bit para menor memoria
)

print(f"Modelo cargado: {modelo.config.model_type}")
print(f"ParÃ¡metros: {sum(p.numel() for p in modelo.parameters())} M")
----

==== Casos de Uso Ideales para Unsloth

[source, text]
----
âœ“ Fine-tuning en Colab (GPU limitada)
âœ“ Empresas con presupuesto limitado en cloud
âœ“ InvestigaciÃ³n rÃ¡pida con iteraciones frecuentes
âœ“ Datasets grandes (100K+ ejemplos)
âœ“ Modelos medianos a grandes (7B-70B parÃ¡metros)

âœ— No es ideal para:
  - Modelos muy pequeÃ±os (<1B) - sobrecarga setup
  - Entrenamiento desde cero (usa pretrained)
----

=== 1.2 Entorno de Desarrollo

==== Requisitos de Hardware

Para usar Unsloth necesitas una GPU NVIDIA con capacidad de cÃ¡lculo suficiente:

[cols="15,15,20,20,20"]
|===
| GPU | VRAM | Modelo mÃ¡s grande | Colab | Precio/hora (AWS)

| **T4** (Free)
| 16 GB
| Llama 2 7B (cuantizado)
| âœ… SÃ­ (Free)
| $0.35

| **L4** (Pro)
| 24 GB
| Llama 2 13B (cuantizado)
| âœ… SÃ­ (Pro)
| $0.60

| **A100**
| 40-80 GB
| Llama 3 70B
| âŒ No
| $10.32

| **H100**
| 80-141 GB
| Cualquiera
| âŒ No
| $33.00
|===

**Importante:** Unsloth requiere CUDA 11.8+ y cuDNN 8.0+. No funciona con GPUs AMD o Intel (por ahora).

[source, bash]
----
# Verificar tu GPU NVIDIA en terminal
nvidia-smi

# Salida esperada:
# | NVIDIA-SMI 535.104.05             Driver Version: 535.104.05 |
# | GPU Name         Persistence-M | Bus-Id        Disp.A | Volatile... |
# | 0  Tesla T4               Off  |   00:1E.0     Off |                  0 |
----

==== OpciÃ³n 1: InstalaciÃ³n en Google Colab (Recomendado para Principiantes)

Google Colab es la forma mÃ¡s sencilla de empezar. No requiere instalaciÃ³n local ni GPU propia.

**Paso 1: Crear un cuaderno**
. Ve a https://colab.research.google.com
. Crea un nuevo notebook
. Selecciona: `Runtime â†’ Change runtime type â†’ GPU (T4 para free, A100 para Pro)`

**Paso 2: Instalar Unsloth**

[source, python]
----
# En la primera celda de Colab
!pip install unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git
!pip install xformers
!pip install datasets

# Espera a que termine (2-3 minutos)
# VerÃ¡s: "Successfully installed unsloth..."
----

**Paso 3: Verificar instalaciÃ³n**

[source, python]
----
from unsloth import FastLanguageModel
import torch

print(f"PyTorch Version: {torch.__version__}")
print(f"CUDA disponible: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name()}")

# Salida esperada:
# PyTorch Version: 2.1.0
# CUDA disponible: True
# GPU: Tesla T4
----

**Ventajas:**
âœ… Gratis (T4)
âœ… Sin instalaciÃ³n compleja
âœ… Pre-configurado (CUDA, dependencias)
âœ… Almacenamiento cloud automÃ¡tico
âœ… Acceso a GPU potentes (Pro)

**Desventajas:**
âŒ T4 limitado (mÃ¡ximo Llama 7B)
âŒ SesiÃ³n de mÃ¡x 12 horas
âŒ Restricciones de uso continuo

==== OpciÃ³n 2: InstalaciÃ³n Local (Linux/WSL)

Si tienes GPU propia o necesitas entrenamientos prolongados.

**Requisitos Previos:**
. NVIDIA Driver actualizado (versiÃ³n 450+)
. CUDA 11.8 o superior
. cuDNN 8.0+
. Python 3.9+

**Paso 1: Verificar Drivers NVIDIA**

[source, bash]
----
# Mostrar versiÃ³n del driver
nvidia-smi

# Mostrar versiÃ³n de CUDA
nvcc --version

# Si no tienes CUDA instalado, descÃ¡rgalo desde:
# https://developer.nvidia.com/cuda-11-8-0-download-archive
----

**Paso 2: Crear entorno Conda (Recomendado)**

[source, bash]
----
# Crear entorno aislado para el proyecto
conda create --name unsloth python=3.10

# Activar entorno
conda activate unsloth

# Instalar PyTorch con CUDA support
conda install pytorch pytorch-cuda=11.8 -c pytorch -c nvidia

# Verificar instalaciÃ³n
python -c "import torch; print(torch.cuda.is_available())"
----

**Paso 3: Instalar Unsloth**

[source, bash]
----
# OpciÃ³n A: InstalaciÃ³n desde PyPI (mÃ¡s simple)
pip install unsloth[colab]

# OpciÃ³n B: InstalaciÃ³n desde repositorio (Ãºltima versiÃ³n)
pip install git+https://github.com/unslothai/unsloth.git

# Instalar dependencias adicionales
pip install xformers datasets transformers accelerate
----

**Paso 4: Test de InstalaciÃ³n**

[source, python]
----
# test_unsloth.py
from unsloth import FastLanguageModel
import torch

# Intentar cargar un modelo pequeÃ±o
modelo, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/tinyllama",
    max_seq_length=1024,
    load_in_4bit=True,
)

print("âœ… Unsloth instalado correctamente")
print(f"Modelo: {modelo.config.model_type}")
print(f"ParÃ¡metros: {sum(p.numel() for p in modelo.parameters()) / 1e6:.1f}M")
----

[source, bash]
----
python test_unsloth.py

# Salida esperada:
# âœ… Unsloth instalado correctamente
# Modelo: llama
# ParÃ¡metros: 1.1M
----

**SoluciÃ³n de Problemas ComÃºn:**

[source, text]
----
Error: "No CUDA GPUs detected"
â†’ Verifica: nvidia-smi
â†’ Reinstala drivers NVIDIA

Error: "ModuleNotFoundError: No module named 'unsloth'"
â†’ Activa el entorno: conda activate unsloth
â†’ Reinstala: pip install unsloth --upgrade

Error: "CUDA out of memory"
â†’ Reduce batch_size en configuraciÃ³n
â†’ Usa load_in_4bit=True
----

==== OpciÃ³n 3: Docker (Para Reproducibilidad)

Si quieres un entorno perfectamente reproducible:

[source, dockerfile]
----
# Dockerfile
FROM nvidia/cuda:11.8.0-devel-ubuntu22.04

RUN apt-get update && apt-get install -y python3-pip

RUN pip install unsloth[colab] xformers datasets transformers

WORKDIR /workspace
----

[source, bash]
----
# Construir imagen
docker build -t unsloth-env .

# Ejecutar contenedor
docker run --gpus all -it -v $(pwd):/workspace unsloth-env bash
----

==== PrÃ¡ctica: "Hola Mundo" en Unsloth

Vamos a crear tu primer script que carga un modelo y genera texto.

**Archivo: `1_hola_mundo.py`**

[source, python]
----
"""
PrÃ¡ctica 1.1: Cargar un modelo base en Unsloth
Este script demuestra la carga y uso bÃ¡sico de Unsloth
"""

from unsloth import FastLanguageModel
import torch

# PASO 1: Cargar el modelo
print("â³ Cargando modelo... (esto toma 1-2 minutos en la primera ejecuciÃ³n)")

modelo, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-2-7b-bnb-4bit",  # Modelo cuantizado en 4-bit
    max_seq_length=2048,  # Longitud mÃ¡xima de secuencia
    load_in_4bit=True,    # Usar cuantizaciÃ³n 4-bit para ahorrar memoria
)

print("âœ… Modelo cargado exitosamente")

# PASO 2: InformaciÃ³n del modelo
print("\nğŸ“Š INFORMACIÃ“N DEL MODELO:")
print(f"  Arquitectura: {modelo.config.model_type}")
print(f"  ParÃ¡metros totales: {sum(p.numel() for p in modelo.parameters()) / 1e9:.1f}B")
print(f"  Memoria usada: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
print(f"  Max sequence length: {modelo.config.max_position_embeddings}")

# PASO 3: Preparar el modelo para inferencia
modelo = FastLanguageModel.for_inference(modelo)

# PASO 4: Generar texto
print("\nğŸ¤– GENERANDO TEXTO:")
print("-" * 50)

prompts = [
    "La inteligencia artificial es",
    "Python es un lenguaje de programaciÃ³n",
    "El fine-tuning de modelos permite"
]

for prompt in prompts:
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    outputs = modelo.generate(
        **inputs,
        max_new_tokens=50,
        temperature=0.7,
        top_p=0.9,
    )

    texto_generado = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"\nPrompt: {prompt}")
    print(f"Respuesta: {texto_generado}\n")

print("-" * 50)
print("âœ… EjecuciÃ³n completada exitosamente!")

# PASO 5: Mostrar informaciÃ³n de uso
print(f"\nMemoria GPU despuÃ©s: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
print(f"Memoria GPU mÃ¡xima: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")
----

**Ejecutar:**

[source, bash]
----
# En Colab o terminal local
python 1_hola_mundo.py

# Salida esperada:
# â³ Cargando modelo... (esto toma 1-2 minutos)
# âœ… Modelo cargado exitosamente
#
# ğŸ“Š INFORMACIÃ“N DEL MODELO:
#   Arquitectura: llama
#   ParÃ¡metros totales: 7.2B
#   Memoria usada: 3.89 GB
#   Max sequence length: 4096
#
# ğŸ¤– GENERANDO TEXTO:
# --------------------------------------------------
#
# Prompt: La inteligencia artificial es
# Respuesta: La inteligencia artificial es una rama de la informÃ¡tica...
# ...
----

**QuÃ© aprendiste:**
âœ… CÃ³mo cargar un modelo con Unsloth
âœ… CÃ³mo verificar la memoria usada
âœ… CÃ³mo generar texto con parÃ¡metros personalizados
âœ… CÃ³mo usar el tokenizer correctamente

**ParÃ¡metros importantes explicados:**

[cols="20,40,25"]
|===
| ParÃ¡metro | Significado | Rango tÃ­pico

| `max_seq_length`
| Longitud mÃ¡xima del contexto
| 512-4096

| `load_in_4bit`
| Cuantizar a 4-bit (menor memoria)
| True/False

| `max_new_tokens`
| Tokens a generar
| 1-1000

| `temperature`
| Aleatoriedad (0=determinÃ­stico, 1=aleatorio)
| 0.0-1.0

| `top_p`
| Nucleus sampling (solo consider top p%)
| 0.8-1.0
|===

==== Checklist: Setup Completado âœ…

[source, text]
----
â–¡ GPU NVIDIA verificada (nvidia-smi)
â–¡ PyTorch instalado con CUDA support
â–¡ Unsloth instalado (pip show unsloth)
â–¡ Script 1_hola_mundo.py ejecutado sin errores
â–¡ Modelo cargado exitosamente en GPU
â–¡ GeneraciÃ³n de texto funcionando

Si completaste todo, Â¡estÃ¡s listo para el MÃ³dulo 2! ğŸš€
----

== MÃ³dulo 2: Conceptos Clave de Fine-Tuning

=== 2.1 TÃ©cnicas de AdaptaciÃ³n

Cuando fine-tuneamos un modelo, tenemos varias estrategias. Vamos a entender la diferencia entre la "ruta cara" (full fine-tuning) y la "ruta eficiente" (PEFT).

==== Full Fine-Tuning vs. PEFT: La Gran DecisiÃ³n

**Full Fine-Tuning (Actualizar TODOS los parÃ¡metros)**

[source, python]
----
# Full Fine-Tuning: Actualiza todos los 7B parÃ¡metros
Llama 2 7B original
  â”œâ”€â”€ Token Embedding: 128M params
  â”œâ”€â”€ 32 Transformer Layers (cada una):
  â”‚   â”œâ”€â”€ AtenciÃ³n: 450M params
  â”‚   â”œâ”€â”€ FFN (Feed Forward): 1.2B params
  â”‚   â””â”€â”€ Layer Norm: 4M params
  â””â”€â”€ Output Head: 5M params

Total params entrenable: 7.0B (100%)
Gradientes a calcular: 7.0B
Optimizador states (Adam): 14.0B (2x params)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Memoria total: ~21 GB
Velocidad: âš ï¸ Lenta
----

**Â¿CuÃ¡ndo usar?**
âœ… Datasets ENORMES (>10M ejemplos)
âœ… Puedes gastar dinero en GPU potentes
âœ… Necesitas mÃ¡xima precisiÃ³n

âŒ Colab: No cabe
âŒ Laptop con GPU: No cabe
âŒ Budget limitado: Muy caro

**PEFT: Parameter-Efficient Fine-Tuning (Solo adapta un 0.8% de params)**

[source, python]
----
# PEFT: Solo actualiza matrices pequeÃ±as (adaptadores)
Llama 2 7B original (congelado, NO se actualiza)
  â”‚
  â””â”€â”€ LoRA Adapters (agregados, SÃ se actualizan)
      â””â”€â”€ Matrices de bajo rango (r=64)
          â”œâ”€â”€ ParÃ¡metros LoRA: ~6M (0.08% del modelo)
          â””â”€â”€ Memoria total: 180 MB ğŸ‰

Memoria total: ~4-8 GB
Velocidad: âš¡ 2x mÃ¡s rÃ¡pido
----

**Â¿CuÃ¡ndo usar?**
âœ… Recursos limitados (Colab, laptop)
âœ… Datasets medianos (10K-100K ejemplos)
âœ… IteraciÃ³n rÃ¡pida (experimentos)
âœ… Presupuesto ajustado

âŒ Datasets minÃºsculos (<1K)
âŒ Tareas muy diferentes del preentrenamiento

==== LoRA: Low-Rank Adaptation (La Magia detrÃ¡s de PEFT)

LoRA es una tÃ©cnica genial que fue introducida en un paper de 2021. La idea es: **No necesitas actualizar todos los parÃ¡metros, solo adaptar a bajo rango.**

**Â¿CÃ³mo funciona internamente?**

Normalmente, en una matriz de peso `W (d_out Ã— d_in)`, durante fine-tuning actualizamos:

[source, python]
----
# MÃ©todo tradicional (Full Fine-Tuning)
W_nuevo = W_original + Î”W
donde Î”W es una matriz de d_out Ã— d_in (7.0B params)

Esto requiere mucha memoria y cÃ¡lculo ğŸ˜«

# MÃ©todo LoRA (La soluciÃ³n inteligente)
W_nuevo = W_original + BA
donde:
  - B es matriz de d_out Ã— r  (pequeÃ±a, r=64)
  - A es matriz de r Ã— d_in   (pequeÃ±a, r=64)

Si d_out=4096, d_in=4096, r=64:
  Full: 4096 Ã— 4096 = 16M params
  LoRA: (4096 Ã— 64) + (64 Ã— 4096) = 524K params  â† 30x menos!
----

**VisualizaciÃ³n:**

[source, text]
----
Entrada (d_in)
    â”‚
    â”œâ”€â”€â”€â”€â”€â†’ W_original (congelado) â”€â”€â”
    â”‚                                  â”œâ”€â†’ Salida (d_out)
    â””â”€â†’ A (r Ã— d_in) â†’ B (d_out Ã— r) â”˜
        â†‘ Se entrena      â†‘ Se entrena

El adaptador LoRA actÃºa como un "parche" sobre el modelo.
----

**Ejemplo prÃ¡ctico en cÃ³digo:**

[source, python]
----
from unsloth import FastLanguageModel

# ConfiguraciÃ³n LoRA
lora_config = {
    "r": 64,              # Rango de descomposiciÃ³n
    "lora_alpha": 16,     # Factor de escala
    "target_modules": ["q_proj", "v_proj"],  # Aplicar LoRA a estas capas
    "lora_dropout": 0.05, # Dropout para regularizaciÃ³n
    "bias": "none",       # No entrenar sesgo
    "task_type": "CAUSAL_LM"
}

# Cargar modelo con LoRA
modelo, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-2-7b-bnb-4bit",
    max_seq_length=2048,
    load_in_4bit=True,
    **lora_config
)

# Verificar quÃ© se va a entrenar
trainable_params = sum(p.numel() for p in modelo.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in modelo.parameters())

print(f"ParÃ¡metros entrenable: {trainable_params / 1e6:.2f}M")
print(f"ParÃ¡metros totales: {total_params / 1e9:.2f}B")
print(f"Porcentaje entrenable: {100 * trainable_params / total_params:.3f}%")

# Salida esperada:
# ParÃ¡metros entrenable: 4.19M
# ParÃ¡metros totales: 7.14B
# Porcentaje entrenable: 0.059%
----

==== QLoRA: LoRA + CuantizaciÃ³n 4-bit

QLoRA es LoRA pero entrenando modelos cuantizados en 4-bit. Esto te permite entrenar **modelos muy grandes en GPUs pequeÃ±as.**

**Â¿QuÃ© es cuantizaciÃ³n?**

Transformar nÃºmeros de 32 bits a 4 bits:

[source, python]
----
# Original (float32)
peso = 0.3254916 (4 bytes)

# Cuantizado (int4)
peso_q4 = [0-15]  (0.5 bytes) â† 8x menos memoria!

Durante backward pass, se descomprimen a float32 solo donde se necesita.
----

**Comparativa: Full vs LoRA vs QLoRA**

[cols="20,20,20,20"]
|===
| MÃ©todo | Memoria | Velocidad | Calidad

| Full Fine-Tuning
| 40+ GB (A100)
| âš ï¸ Lenta
| 100% (baseline)

| LoRA
| 24 GB
| âš¡ 2x rÃ¡pido
| 99%

| QLoRA (LoRA + 4bit)
| 8 GB (T4/L4)
| âš¡âš¡ 3x rÃ¡pido
| 98%
|===

**CuÃ¡ndo usar cada una:**

[source, text]
----
â”Œâ”€ Â¿Tienes GPU potente (A100+)?
â”‚  â””â”€ NO â†’ Usa QLoRA (0.059% params)
â”‚  â””â”€ SÃ â†’ Â¿Presupuesto ilimitado?
â”‚        â””â”€ NO â†’ Usa LoRA (0.08% params)
â”‚        â””â”€ SÃ â†’ Full Fine-Tuning
â””â”€ En general, 99% de casos: Usa LoRA o QLoRA ğŸ¯
----

==== HiperparÃ¡metros Clave de LoRA Explicados

**`r` (rango de descomposiciÃ³n) - La decisiÃ³n mÃ¡s importante**

[source, python]
----
r=8    â†’ Muy bajo. AdaptaciÃ³n dÃ©bil. ParÃ¡metros: 1.0M
r=16   â†’ Bajo. AdaptaciÃ³n mÃ­nima. ParÃ¡metros: 2.1M
r=32   â†’ Medio. Balance. ParÃ¡metros: 4.2M â† Recomendado
r=64   â†’ Alto. AdaptaciÃ³n fuerte. ParÃ¡metros: 8.4M
r=128  â†’ Muy alto. Casi LoRA completo. ParÃ¡metros: 16.8M

Regla prÃ¡ctica:
- r=8-16: Tareas simples (clasificaciÃ³n, traducciÃ³n)
- r=32-64: Tareas generales (chat, escritura creativa)
- r=128+: Tareas muy especializadas o datasets muy grandes
----

**`lora_alpha` (factor de escala)**

[source, python]
----
# El adaptador se suma como:
output = W_original @ input + (lora_alpha / r) * BA @ input
                               â†‘ Este factor controla el "peso"

lora_alpha = 16, r = 64:
  factor = 16/64 = 0.25 (contribuye 25%)

lora_alpha = 32, r = 64:
  factor = 32/64 = 0.5  (contribuye 50%)

Regla: lora_alpha = r * 2 (proporciÃ³n buena 2:1)

Si usas r=64 â†’ usa lora_alpha=128
Si usas r=32 â†’ usa lora_alpha=64
Si usas r=16 â†’ usa lora_alpha=32
----

**`lora_dropout` (regularizaciÃ³n)**

[source, python]
----
lora_dropout = 0.0   â†’ Sin dropout. Riesgo de overfitting
lora_dropout = 0.05  â†’ 5% de dropout â† Recomendado
lora_dropout = 0.1   â†’ 10% de dropout. Para datasets pequeÃ±os
lora_dropout = 0.2   â†’ 20% de dropout. Muy agresivo

MÃ¡s dropout = menos overfitting, pero convergencia lenta
----

**`target_modules` (dÃ³nde aplicar LoRA)**

[source, python]
----
# LoRA aplicado a TODAS las capas:
target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "up_proj", "down_proj"]
â†’ MÃ¡xima capacidad. ParÃ¡metros: +15M

# LoRA aplicado SOLO a atenciÃ³n (recomendado):
target_modules = ["q_proj", "v_proj"]
â†’ Balance. ParÃ¡metros: +4M. Costo-beneficio Ã³ptimo

# LoRA mÃ­nimo (solo para datasets gigantes):
target_modules = ["q_proj"]
â†’ MÃ­nimo. ParÃ¡metros: +2M
----

**ConfiguraciÃ³n Recomendada por Caso:**

[source, python]
----
# CASO 1: Colab T4 (16GB), dataset pequeÃ±o (5K ejemplos)
config_minimo = {
    "r": 8,
    "lora_alpha": 16,
    "target_modules": ["q_proj", "v_proj"],
    "lora_dropout": 0.05,
}

# CASO 2: GPU Local (24GB), dataset mediano (50K ejemplos)
config_medio = {
    "r": 32,
    "lora_alpha": 64,
    "target_modules": ["q_proj", "v_proj"],
    "lora_dropout": 0.05,
}

# CASO 3: A100 (80GB), dataset grande (1M ejemplos)
config_maximo = {
    "r": 64,
    "lora_alpha": 128,
    "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"],
    "lora_dropout": 0.05,
}
----

=== 2.2 PreparaciÃ³n de Datos

El "garbage in, garbage out" es especialmente cierto en LLMs. Datos malos â†’ Modelo malo. Vamos a aprender a preparar datos correctamente.

==== Formatos de Datasets Soportados

Los modelos de lenguaje aceptan diferentes formatos:

**Formato 1: Alpaca (Pregunta-Respuesta)**

[source, json]
----
{
  "instruction": "Â¿CuÃ¡l es la capital de EspaÃ±a?",
  "input": "",
  "output": "La capital de EspaÃ±a es Madrid."
}
----

Usado por: Llama, Alpaca, Guanaco
Mejor para: Tareas de Q&A, clasificaciÃ³n

**Formato 2: ShareGPT (Conversaciones)**

[source, json]
----
{
  "conversations": [
    {
      "from": "human",
      "value": "Hola, Â¿cÃ³mo estÃ¡s?"
    },
    {
      "from": "gpt",
      "value": "Â¡Hola! Estoy muy bien, gracias por preguntar."
    },
    {
      "from": "human",
      "value": "Â¿QuÃ© puedes hacer?"
    },
    {
      "from": "gpt",
      "value": "Puedo ayudarte con muchas tareas..."
    }
  ]
}
----

Usado por: VicuÃ±a, otros chat models
Mejor para: Fine-tuning conversacional

**Formato 3: ChatML (Chat Markup Language)**

[source, text]
----
<|im_start|>user
Â¿CuÃ¡l es la capital de EspaÃ±a?
<|im_end|>
<|im_start|>assistant
La capital de EspaÃ±a es Madrid.
<|im_end|>
----

Usado por: OpenAI, Mistral
Mejor para: Chat models, instrucciones

**Formato 4: Plain Text (Pretraining)**

[source, text]
----
Este es un documento de texto plano.
Se usa para continued pretraining.
Cada lÃ­nea o pÃ¡rrafo es un ejemplo.
----

==== Ejemplo: Convertir tu dataset a formato Alpaca

Supongamos que tienes datos en CSV:

[source, csv]
----
pregunta,respuesta
"Â¿CuÃ¡l es la capital de Francia?","ParÃ­s es la capital de Francia."
"Â¿CuÃ¡nto es 2+2?","2+2 = 4"
----

Convertirlo a Alpaca:

[source, python]
----
import json
import pandas as pd

# Leer CSV
df = pd.read_csv("mi_dataset.csv")

# Convertir a Alpaca
alpaca_data = []
for idx, row in df.iterrows():
    alpaca_data.append({
        "instruction": row["pregunta"],
        "input": "",
        "output": row["respuesta"]
    })

# Guardar como JSON
with open("dataset_alpaca.json", "w", encoding="utf-8") as f:
    json.dump(alpaca_data, f, ensure_ascii=False, indent=2)

print(f"Convertidos {len(alpaca_data)} ejemplos")
----

==== Uso de Hugging Face Datasets

La librerÃ­a `datasets` de Hugging Face te permite:
1. Usar datasets pÃºblicos
2. Cargar datasets locales
3. Hacer streaming sin descargar todo

[source, python]
----
from datasets import load_dataset

# OPCIÃ“N 1: Dataset pÃºblico de Hugging Face
dataset = load_dataset("imdb")  # Ejemplo: dataset de pelÃ­culas
print(f"Train split: {len(dataset['train'])} ejemplos")

# OPCIÃ“N 2: Dataset local (JSON)
dataset = load_dataset(
    "json",
    data_files="dataset_alpaca.json",
    split="train"
)

# OPCIÃ“N 3: Dataset local (CSV)
dataset = load_dataset(
    "csv",
    data_files="mi_dataset.csv",
    split="train"
)

# OPCIÃ“N 4: Dataset local (carpeta con mÃºltiples archivos)
dataset = load_dataset(
    "json",
    data_files={
        "train": "train_*.json",
        "val": "val_*.json"
    }
)

# Ver estructura
print(dataset[0])  # Primer ejemplo
print(dataset.column_names)  # Nombres de columnas
----

==== Plantillas de Chat (Chat Templates)

Los modelos modernos usan "plantillas de chat" para formatear conversaciones correctamente. Cada modelo tiene su propia plantilla.

**Â¿Por quÃ© importa?**

[source, python]
----
# SIN plantilla (MALO âŒ)
"Usuario: Hola. Asistente: Hola, Â¿cÃ³mo estÃ¡s?"
â†’ El modelo se confunde sobre dÃ³nde termina input y empieza output

# CON plantilla (BUENO âœ…)
"<|im_start|>user\nHola<|im_end|>\n<|im_start|>assistant\nHola, Â¿cÃ³mo estÃ¡s?<|im_end|>"
â†’ Tokens especiales indican claramente los roles
----

**Chat Templates por modelo:**

[source, python]
----
# Llama 2 / Llama 3
template_llama = """<s>[INST] {instrucciÃ³n} [/INST] {respuesta} </s>"""

# Mistral
template_mistral = """[INST] {instrucciÃ³n} [/INST] {respuesta}</s>"""

# Gemma
template_gemma = """<start_of_turn>user\n{instrucciÃ³n}<end_of_turn>\n<start_of_turn>model\n{respuesta}<end_of_turn>"""

# OpenAI ChatML
template_chatml = """<|im_start|>user\n{instrucciÃ³n}<|im_end|>\n<|im_start|>assistant\n{respuesta}<|im_end|>"""
----

**Usar la plantilla correcta automÃ¡ticamente:**

[source, python]
----
from unsloth import FastLanguageModel

modelo, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-2-7b",
    ...
)

# El tokenizer ya conoce la plantilla correcta
plantilla = tokenizer.chat_template
print(f"Plantilla detectada: {plantilla}")

# Formatear una conversaciÃ³n
mensaje = [
    {"role": "user", "content": "Â¿CuÃ¡l es 2+2?"},
]

# Esto aplica la plantilla automÃ¡ticamente
tokens = tokenizer.apply_chat_template(
    mensaje,
    tokenize=True,
    add_generation_prompt=True
)
----

==== PrÃ¡ctica: Preparar un Dataset Personalizado

Vamos a crear un script que prepara un dataset completo.

**Archivo: `2_preparar_dataset.py`**

[source, python]
----
"""
PrÃ¡ctica 2.1: PreparaciÃ³n de Dataset
Convierte datos brutos en formato listo para fine-tuning
"""

import json
from datasets import Dataset, DatasetDict
from typing import List, Dict

def crear_dataset_alpaca(ejemplos: List[Dict]) -> Dataset:
    """
    Crear dataset en formato Alpaca

    Ejemplo de entrada:
    {
        "instruction": "Â¿CuÃ¡l es la capital de EspaÃ±a?",
        "input": "",
        "output": "Madrid"
    }
    """
    # Validar formato
    for ej in ejemplos:
        assert "instruction" in ej, "Falta 'instruction'"
        assert "output" in ej, "Falta 'output'"
        if "input" not in ej:
            ej["input"] = ""

    dataset = Dataset.from_dict({
        "instruction": [e["instruction"] for e in ejemplos],
        "input": [e.get("input", "") for e in ejemplos],
        "output": [e["output"] for e in ejemplos]
    })

    return dataset

def crear_dataset_sharegpt(conversaciones: List[Dict]) -> Dataset:
    """
    Crear dataset en formato ShareGPT

    Ejemplo de entrada:
    {
        "conversations": [
            {"from": "human", "value": "Hola"},
            {"from": "gpt", "value": "Hola, Â¿cÃ³mo estÃ¡s?"}
        ]
    }
    """
    dataset = Dataset.from_dict({
        "conversations": [c["conversations"] for c in conversaciones]
    })

    return dataset

def limpiar_dataset(dataset: Dataset) -> Dataset:
    """
    Limpiar dataset: remover duplicados, ejemplos vacÃ­os, etc.
    """
    print(f"Dataset original: {len(dataset)} ejemplos")

    # Remover duplicados
    dataset = dataset.map(
        lambda x: x,  # No mapear, solo mantener
        remove_duplicates=True
    )
    print(f"DespuÃ©s de remover duplicados: {len(dataset)} ejemplos")

    # Remover ejemplos con campos vacÃ­os
    def no_vacio(ejemplo):
        if "instruction" in ejemplo:
            return len(ejemplo["instruction"]) > 0
        if "conversations" in ejemplo:
            return len(ejemplo["conversations"]) > 0
        return True

    dataset = dataset.filter(no_vacio)
    print(f"DespuÃ©s de remover vacÃ­os: {len(dataset)} ejemplos")

    return dataset

def dividir_train_val(dataset: Dataset, ratio_val=0.1) -> DatasetDict:
    """
    Dividir dataset en train (90%) y validation (10%)
    """
    split_dataset = dataset.train_test_split(test_size=ratio_val, seed=42)

    return DatasetDict({
        "train": split_dataset["train"],
        "validation": split_dataset["test"]
    })

# EJEMPLO DE USO
if __name__ == "__main__":
    # Crear algunos ejemplos de ejemplo
    ejemplos = [
        {
            "instruction": "Â¿CuÃ¡l es la capital de EspaÃ±a?",
            "input": "",
            "output": "La capital de EspaÃ±a es Madrid."
        },
        {
            "instruction": "CuÃ©ntame un chiste",
            "input": "",
            "output": "Â¿Por quÃ© los pÃ¡jaros no se pierden? Porque usan GPS: Global Posicionamiento de Suelos."
        },
        {
            "instruction": "Explica quÃ© es el fine-tuning",
            "input": "",
            "output": "El fine-tuning es el proceso de entrenar un modelo pre-entrenado..."
        },
    ]

    print("ğŸ“Š CREANDO DATASET")
    print("-" * 50)

    # Crear dataset
    dataset = crear_dataset_alpaca(ejemplos)
    print(f"Dataset creado: {len(dataset)} ejemplos")
    print(f"Columnas: {dataset.column_names}")
    print(f"\nEjemplo 1:")
    print(json.dumps(dataset[0], ensure_ascii=False, indent=2))

    # Limpiar
    print("\nğŸ§¹ LIMPIEZA")
    print("-" * 50)
    dataset_limpio = limpiar_dataset(dataset)

    # Dividir
    print("\nâœ‚ï¸ DIVISIÃ“N TRAIN/VAL")
    print("-" * 50)
    split = dividir_train_val(dataset_limpio, ratio_val=0.2)
    print(f"Train: {len(split['train'])} ejemplos")
    print(f"Validation: {len(split['validation'])} ejemplos")

    # Guardar
    print("\nğŸ’¾ GUARDANDO")
    print("-" * 50)
    split.save_to_disk("./dataset_preparado")
    print("âœ… Dataset guardado en './dataset_preparado'")

    # InformaciÃ³n estadÃ­stica
    print("\nğŸ“ˆ ESTADÃSTICAS")
    print("-" * 50)
    for split_name, split_data in split.items():
        print(f"\n{split_name.upper()}:")
        for col in split_data.column_names:
            if col != "input":  # input es frecuentemente vacÃ­o
                longitudes = [len(str(e)) for e in split_data[col]]
                print(f"  {col}: promedio {sum(longitudes)/len(longitudes):.0f} chars")
----

**Ejecutar:**

[source, bash]
----
python 2_preparar_dataset.py

# Salida esperada:
# ğŸ“Š CREANDO DATASET
# --------------------------------------------------
# Dataset creado: 3 ejemplos
# Columnas: ['instruction', 'input', 'output']
#
# Ejemplo 1:
# {
#   "instruction": "Â¿CuÃ¡l es la capital de EspaÃ±a?",
#   "input": "",
#   "output": "La capital de EspaÃ±a es Madrid."
# }
#
# ğŸ§¹ LIMPIEZA
# --------------------------------------------------
# Dataset original: 3 ejemplos
# DespuÃ©s de remover duplicados: 3 ejemplos
# DespuÃ©s de remover vacÃ­os: 3 ejemplos
#
# âœ‚ï¸ DIVISIÃ“N TRAIN/VAL
# --------------------------------------------------
# Train: 2 ejemplos
# Validation: 1 ejemplos
#
# ğŸ’¾ GUARDANDO
# --------------------------------------------------
# âœ… Dataset guardado en './dataset_preparado'
#
# ğŸ“ˆ ESTADÃSTICAS
# --------------------------------------------------
# TRAIN:
#   instruction: promedio 45 chars
#   output: promedio 85 chars
----

**Checklist: Datos Preparados âœ…**

[source, text]
----
â–¡ Datos en formato Alpaca/ShareGPT/ChatML
â–¡ Dataset limpio (sin duplicados, vacÃ­os)
â–¡ Dividido en train (90%) y validation (10%)
â–¡ EstadÃ­sticas revisadas
â–¡ Ejemplos verificados manualmente

EstÃ¡s listo para el MÃ³dulo 3 (Fine-tuning real) ğŸš€
----

== MÃ³dulo 3: Flujo de Trabajo de Fine-Tuning (SFT)

=== 3.1 ConfiguraciÃ³n del Entrenamiento

El fine-tuning requiere configurar correctamente mÃºltiples componentes. Vamos a aprender quÃ© significa cada parÃ¡metro y cÃ³mo elegirlos.

==== Cargar el Modelo con FastLanguageModel

[source, python]
----
from unsloth import FastLanguageModel
from peft import LoraConfig, get_peft_model

# PASO 1: Cargar el modelo base
modelo, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-2-7b-bnb-4bit",
    max_seq_length=2048,
    load_in_4bit=True,  # CuantizaciÃ³n 4-bit
    dtype=torch.float16  # Dtype de cÃ¡lculo
)

# PASO 2: Aplicar LoRA
modelo = FastLanguageModel.get_peft_model(
    modelo,
    r=32,              # Rango LoRA
    lora_alpha=64,     # Factor escala
    lora_dropout=0.05, # RegularizaciÃ³n
    bias="none",
    use_gradient_checkpointing=True,
    use_rslora=True,   # RSlora para mejor convergencia
    task_type="CAUSAL_LM"
)

# PASO 3: Preparar para entrenamiento
modelo.print_trainable_parameters()

# Salida:
# trainable params: 4194304 || all params: 7171139584 || trainable%: 0.05847
----

**ExplicaciÃ³n:**

- `max_seq_length`: Longitud mÃ¡xima de las secuencias (2048 tokens = ~500 palabras)
- `load_in_4bit`: Cuantizar modelo a 4-bit (8x menos memoria)
- `r=32`: Matrices LoRA de rango 32 (balance memoria/capacidad)
- `lora_alpha=64`: Factor 64/32=2 (buena proporciÃ³n)
- `use_gradient_checkpointing=True`: Ahorrar memoria (lento pero necesario en GPU pequeÃ±a)
- `use_rslora=True`: LoRA mejorado con scaling por rango

==== TrainingArguments: La ConfiguraciÃ³n CrÃ­tica

[source, python]
----
from transformers import TrainingArguments

training_args = TrainingArguments(
    # 1. RUTAS Y GUARDADO
    output_dir="./llama2_finetuned",          # DÃ³nde guardar checkpoints
    save_strategy="steps",                     # Guardar cada N steps
    save_steps=100,                            # Guardar cada 100 steps
    save_total_limit=3,                        # Mantener solo 3 Ãºltimos checkpoints

    # 2. EVALUACIÃ“N
    eval_strategy="steps",                     # Evaluar cada N steps
    eval_steps=50,                             # Evaluar cada 50 steps

    # 3. LEARNING RATE
    learning_rate=5e-4,                        # Tasa de aprendizaje (imporant!)
    lr_scheduler_type="cosine",                # Cosine scheduling
    warmup_steps=100,                          # Calentar 100 steps

    # 4. BATCH SIZE Y GRADIENTES
    per_device_train_batch_size=4,             # Batch size por GPU
    per_device_eval_batch_size=4,              # Eval batch size
    gradient_accumulation_steps=4,             # Acumular 4 batches (eff batch=16)

    # 5. REGULARIZACIÃ“N
    weight_decay=0.01,                         # RegularizaciÃ³n L2
    max_grad_norm=1.0,                         # Clip gradientes

    # 6. ENTRENAMIENTO
    num_train_epochs=3,                        # 3 pasadas sobre los datos
    max_steps=-1,                              # -1 = usar num_train_epochs

    # 7. HARDWARE
    fp16=True,                                 # Usar float16 (mÃ¡s rÃ¡pido)

    # 8. LOGGING
    logging_steps=10,                          # Log cada 10 steps
    logging_dir="./logs",
)
----

**Tabla de hiperparÃ¡metros recomendados:**

[cols="25,15,15,20"]
|===
| ParÃ¡metro | Colab T4 | GPU Local (24GB) | A100 (80GB)

| per_device_train_batch_size
| 1-2
| 4-8
| 16-32

| gradient_accumulation_steps
| 8-16
| 2-4
| 1-2

| learning_rate
| 5e-4
| 2e-4
| 1e-4

| warmup_steps
| 100
| 500
| 1000

| max_seq_length
| 1024
| 2048
| 4096
|===

==== Learning Rate y Schedulers: Ajustando la Velocidad de Aprendizaje

La learning rate (tasa de aprendizaje) es **EL PARÃMETRO MÃS IMPORTANTE**.

**Â¿Por quÃ©?** Determina cuÃ¡n grande es cada paso del entrenamiento:

[source, python]
----
# Learning rate muy alto (1e-2 = 0.01)
# Los pesos "saltan" demasiado lejos
# Resultado: Entrenamiento inestable, loss diverge
loss = [2.5, 2.4, 5.3, NaN]  âŒ

# Learning rate correcto (5e-4 = 0.0005)
# Los pesos se ajustan gradualmente
# Resultado: Convergencia suave
loss = [2.5, 2.4, 2.2, 1.9, 1.7]  âœ…

# Learning rate muy bajo (1e-6)
# Los pesos casi no cambian
# Resultado: Entrenamiento muy lento
loss = [2.5, 2.499, 2.498, 2.497]  âš ï¸
----

**Schedulers: Variar la learning rate durante entrenamiento**

[source, python]
----
# OpciÃ³n 1: Constante (simple pero no Ã³ptimo)
lr_scheduler_type="constant"
lr_schedule = [5e-4, 5e-4, 5e-4, 5e-4, ...]

# OpciÃ³n 2: Linear decay (disminuye linealmente)
lr_scheduler_type="linear"
lr_schedule = [5e-4, 4e-4, 3e-4, 2e-4, ...]

# OpciÃ³n 3: Cosine (disminuye suavemente, RECOMENDADO)
lr_scheduler_type="cosine"
lr_schedule = [5e-4, 4.9e-4, 4.8e-4, 4.5e-4, ..., 1e-6]
â†‘ Empieza rÃ¡pido, termina lentamente para convergencia fina

# OpciÃ³n 4: Cosine con warmup (recomendado para fine-tuning)
warmup_steps=100
lr_schedule:
  Primeros 100 steps: 0 â†’ 5e-4  (calentar)
  Siguientes N steps: 5e-4 â†’ 0  (cosine decay)
----

**RecomendaciÃ³n:**
- Fine-tuning LoRA: `cosine` con `warmup_steps=100-500`
- Full fine-tuning: `linear` con `warmup_steps=500-1000`
- Continued pretraining: `constant` o `cosine`

==== Batch Size y Gradient Accumulation

**Batch Size: Â¿CuÃ¡ntos ejemplos procesar simultÃ¡neamente?**

[source, python]
----
# Batch size = 1 (un ejemplo a la vez)
# Ruido alto, actualizaciÃ³n frecuente, muy lento
for ejemplo in dataset:
    loss = forward(ejemplo)
    backward()  â† Actualizar cada paso

# Batch size = 16 (16 ejemplos juntos)
# MÃ¡s estable, menos ruido, mÃ¡s rÃ¡pido
for batch en dataset(batch_size=16):
    loss = forward(batch)
    backward()  â† Actualizar cada 16 ejemplos

# Â¿MÃ¡s grande es mejor?
batch_size = 64  âœ… MÃ¡s rÃ¡pido
batch_size = 256 âŒ OOM (Out of Memory)
----

**Gradient Accumulation: El truco para simular batch size grande**

Si quieres batch size = 16 pero tu GPU solo aguanta batch size = 4:

[source, python]
----
# Sin accumulation:
per_device_batch_size = 16
GPU memory: 24 GB âŒ No cabe!

# Con accumulation:
per_device_batch_size = 4
gradient_accumulation_steps = 4
Batch efectivo = 4 * 4 = 16
GPU memory: 6 GB âœ… Cabe!

CÃ³mo funciona:
paso 1: forward(batch1) + backward() â†’ guardar gradientes
paso 2: forward(batch2) + backward() â†’ acumular gradientes
paso 3: forward(batch3) + backward() â†’ acumular gradientes
paso 4: forward(batch4) + backward() â†’ actualizar pesos (Â¡ahora!)
----

**Regla prÃ¡ctica:**

`batch_efectivo = per_device_batch_size Ã— gradient_accumulation_steps`

Para que sea alrededor de 16-32:
- GPU 16GB: 2-4 per_device + 4-8 accumulation steps
- GPU 24GB: 4-8 per_device + 2-4 accumulation steps
- GPU 80GB: 16-32 per_device + 1-2 accumulation steps

==== Optimizadores: AdamW 8-bit vs Standard

[source, python]
----
# Optimizador estÃ¡ndar (Adam)
states_por_parametro = 2  (momentum + variance)
# 7B modelo â†’ 14B memoria solo en optimizer states âŒ

# AdamW 8-bit (8-bit quantized)
states_por_parametro = 2  (pero cuantizados a 8-bit)
# 7B modelo â†’ 1.75B memoria en optimizer states âœ…

training_args = TrainingArguments(
    optim="adamw_8bit",  # Usar 8-bit quantized Adam
    ...
)
----

Ventaja: 8x menos memoria en los optimizer states, casi sin pÃ©rdida de precisiÃ³n.

==== Ejemplo Completo de ConfiguraciÃ³n

[source, python]
----
"""
ConfiguraciÃ³n lista para usar en diferentes escenarios
"""

# ESCENARIO 1: Colab T4 (16GB), Dataset pequeÃ±o
config_colab_t4 = {
    "per_device_train_batch_size": 2,
    "gradient_accumulation_steps": 8,  # batch efectivo = 16
    "learning_rate": 5e-4,
    "num_train_epochs": 3,
    "warmup_steps": 50,
    "max_seq_length": 1024,
    "r": 8,  # LoRA reducido
}

# ESCENARIO 2: GPU Local RTX 3080 (10GB), Dataset mediano
config_local = {
    "per_device_train_batch_size": 4,
    "gradient_accumulation_steps": 4,  # batch efectivo = 16
    "learning_rate": 2e-4,
    "num_train_epochs": 2,
    "warmup_steps": 100,
    "max_seq_length": 2048,
    "r": 32,
}

# ESCENARIO 3: A100 (80GB), Dataset grande
config_a100 = {
    "per_device_train_batch_size": 16,
    "gradient_accumulation_steps": 1,  # batch efectivo = 16
    "learning_rate": 1e-4,
    "num_train_epochs": 1,
    "warmup_steps": 500,
    "max_seq_length": 4096,
    "r": 64,
}
----

=== 3.2 EjecuciÃ³n y Monitoreo

Ahora vamos a ejecutar el fine-tuning con SFTTrainer de TRL.

==== Usando SFTTrainer

[source, python]
----
from trl import SFTTrainer
from transformers import TrainingArguments, DataCollatorForLanguageModeling

# Ya tenemos:
# - modelo (con LoRA)
# - tokenizer
# - dataset (train y eval)

training_args = TrainingArguments(
    output_dir="./llama2_finetuned",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=5e-4,
    num_train_epochs=1,
    eval_strategy="steps",
    eval_steps=50,
    save_steps=100,
    logging_steps=10,
    warmup_steps=50,
    optim="adamw_8bit",
    fp16=True,
)

# Crear trainer
trainer = SFTTrainer(
    model=modelo,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    dataset_text_field="output",  # Campo a entrenar
    max_seq_length=2048,
    packing=False,  # True = empacar secuencias para eficiencia
)

# Entrenar
print("â³ Iniciando entrenamiento...")
trainer.train()

print("âœ… Entrenamiento completado!")
print(f"Modelo guardado en: {training_args.output_dir}")
----

==== Monitoreo de Loss y MÃ©tricas

Durante el entrenamiento, el loss (pÃ©rdida) disminuirÃ¡:

[source, text]
----
Epoch 1/3:
  step 10: loss=2.45 (disminuyendo âœ“)
  step 20: loss=2.15
  step 30: loss=1.95
  step 40: loss=1.80
  step 50: loss=1.70 (evaluaciÃ³n: val_loss=1.75)

Epoch 2/3:
  step 60: loss=1.62
  step 70: loss=1.58
  ...

InterpretaciÃ³n:
- Loss en train disminuye â†’ entrenamiento funciona âœ…
- Loss en val disminuye â†’ generalizando bien âœ…
- Loss en train disminuye pero val aumenta â†’ overfitting âš ï¸
- Loss no cambia â†’ learning rate demasiado bajo âŒ
----

==== Usando Weights & Biases para Tracking Avanzado

Weights & Biases (W&B) es un servicio gratuito para monitorear entrenamientos:

[source, python]
----
# 1. Crear cuenta en wandb.ai
# 2. Instalar:
pip install wandb

# 3. Login
import wandb
wandb.login()  # Pedir API key

# 4. Integrar con trainer
training_args = TrainingArguments(
    output_dir="./llama2_finetuned",
    report_to=["wandb"],  # Enviar a W&B
    run_name="llama2-finetuning-v1",
    project_name="llama-finetuning",
    ...
)

trainer = SFTTrainer(
    model=modelo,
    ...
)

trainer.train()

# W&B crearÃ¡ automÃ¡ticamente:
# - GrÃ¡ficas de loss
# - ComparaciÃ³n entre entrenamientos
# - DuraciÃ³n estimada
# - Dashboard en tiempo real
----

==== PrÃ¡ctica: Fine-tuning Completo

**Archivo: `3_finetuning_completo.py`**

[source, python]
----
"""
PrÃ¡ctica 3.1: Fine-tuning completo de un modelo
Este script realiza fine-tuning end-to-end
"""

import torch
from unsloth import FastLanguageModel
from transformers import TrainingArguments
from trl import SFTTrainer
from datasets import load_dataset

def preparar_modelo():
    """Cargar modelo con LoRA"""
    modelo, tokenizer = FastLanguageModel.from_pretrained(
        model_name="unsloth/llama-2-7b-bnb-4bit",
        max_seq_length=2048,
        load_in_4bit=True,
        dtype=torch.float16
    )

    modelo = FastLanguageModel.get_peft_model(
        modelo,
        r=32,
        lora_alpha=64,
        lora_dropout=0.05,
        bias="none",
        use_gradient_checkpointing=True,
        task_type="CAUSAL_LM"
    )

    return modelo, tokenizer

def entrenar(modelo, tokenizer, dataset):
    """Ejecutar entrenamiento"""

    training_args = TrainingArguments(
        output_dir="./llama2_finetuned",
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        learning_rate=5e-4,
        num_train_epochs=1,
        logging_steps=10,
        eval_steps=50,
        save_steps=100,
        save_total_limit=3,
        warmup_steps=50,
        lr_scheduler_type="cosine",
        optim="adamw_8bit",
        fp16=True,
        eval_strategy="steps",
    )

    trainer = SFTTrainer(
        model=modelo,
        tokenizer=tokenizer,
        args=training_args,
        train_dataset=dataset["train"],
        eval_dataset=dataset["validation"],
        dataset_text_field="output",
        max_seq_length=2048,
        packing=False,
    )

    print("ğŸš€ Iniciando entrenamiento...")
    resultado = trainer.train()

    print(f"âœ… Entrenamiento completado!")
    print(f"PÃ©rdida final: {resultado.training_loss:.4f}")

    return trainer

if __name__ == "__main__":
    # Cargar dataset de ejemplo
    dataset = load_dataset("json", data_files="./dataset_preparado", split="train")

    # Dividir
    split = dataset.train_test_split(test_size=0.1, seed=42)
    dataset = split

    print("ğŸ“Š Dataset cargado")
    print(f"  Train: {len(dataset['train'])} ejemplos")
    print(f"  Test: {len(dataset['test'])} ejemplos")

    # Preparar modelo
    print("\nâ³ Preparando modelo...")
    modelo, tokenizer = preparar_modelo()
    modelo.print_trainable_parameters()

    # Entrenar
    print("\nğŸ”¥ Entrenando...")
    trainer = entrenar(modelo, tokenizer, {
        "train": dataset["train"],
        "validation": dataset["test"]
    })

    # Guardar
    print("\nğŸ’¾ Guardando...")
    modelo.save_pretrained("./mi_modelo_finetuned")
    tokenizer.save_pretrained("./mi_modelo_finetuned")
    print("âœ… Modelo guardado en ./mi_modelo_finetuned")
----

**Checklist: Fine-tuning Configurado âœ…**

[source, text]
----
â–¡ Modelo cargado con LoRA
â–¡ Datos en formato correcto
â–¡ TrainingArguments configurados
â–¡ Batch size y lr establecidos apropiadamente
â–¡ SFTTrainer funciona sin errores
â–¡ Loss disminuye durante entrenamiento

Â¡EstÃ¡s listo para el MÃ³dulo 4 (Inferencia) ğŸš€
----

== MÃ³dulo 4: Inferencia y EvaluaciÃ³n

=== 4.1 Inferencia Nativa

Una vez que el modelo estÃ¡ fine-tuneado, necesitamos usarlo (inferencia). Unsloth optimiza esta fase tambiÃ©n.

==== Inferencia RÃ¡pida con FastLanguageModel.for_inference

[source, python]
----
from unsloth import FastLanguageModel
import torch

# Cargar modelo fine-tuneado
modelo, tokenizer = FastLanguageModel.from_pretrained(
    model_name="./mi_modelo_finetuned",
    max_seq_length=2048,
    load_in_4bit=True,
)

# âœ… IMPORTANTE: Convertir a modo inferencia (no entrenamiento)
modelo = FastLanguageModel.for_inference(modelo)

# Ahora el modelo es:
# 1. MÃ¡s rÃ¡pido (sin overhead de entrenamiento)
# 2. MÃ¡s memoria eficiente
# 3. Compatible con generaciÃ³n continua
----

**Â¿QuÃ© hace `for_inference`?**

[source, python]
----
# Sin for_inference:
# - CÃ¡lculo de gradientes: ACTIVADO (para entrenamiento)
# - Checkpoint gradient: ACTIV ADO (guarda activaciones)
# - Dropout: ACTIVO (aÃ±ade ruido)
# - Memoria: 8GB
# - Velocidad: Lenta

# Con for_inference:
# - CÃ¡lculo de gradientes: DESACTIVADO
# - Checkpoint gradient: DESACTIVADO
# - Dropout: DESACTIVADO
# - Memoria: 3GB
# - Velocidad: 2-3x mÃ¡s rÃ¡pido
----

==== GeneraciÃ³n de Texto Paso a Paso

[source, python]
----
# Input: Preparar el prompt
prompt = "La inteligencia artificial es"

# Paso 1: Tokenizar
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
print(f"Input tokens: {inputs['input_ids'].shape}")  # [1, 8] = 1 batch, 8 tokens

# Paso 2: Generar
outputs = modelo.generate(
    **inputs,
    max_new_tokens=50,        # Generar 50 tokens nuevos
    temperature=0.7,          # Aleatoriedad
    top_p=0.9,                # Nucleus sampling
    do_sample=True,           # Muestreo (vs greedy)
)

# Paso 3: Decodificar
texto_generado = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(texto_generado)
# Output: "La inteligencia artificial es una rama de la informÃ¡tica que se enfoca en..."
----

**ParÃ¡metros de GeneraciÃ³n Explicados:**

[source, python]
----
max_new_tokens: 50      # MÃ¡ximo tokens a generar
                        # MÃ¡s tokens = mÃ¡s texto, mÃ¡s tiempo

temperature: 0.7        # Controla aleatoriedad
                        # 0.0 = determinÃ­stico (siempre lo mismo)
                        # 0.5 = bajo aleatoriedad (conservador)
                        # 1.0 = medio aleatoriedad (equilibrado)
                        # >1.0 = mucho aleatoriedad (creativo)

top_p: 0.9              # Nucleus sampling
                        # Solo considera tokens que acumulan 90% de probabilidad
                        # 0.5 = muy restrictivo (pocas opciones)
                        # 0.9 = mÃ¡s diverso
                        # 1.0 = sin restricciÃ³n

top_k: 50               # Top-K sampling
                        # Solo considera los 50 tokens mÃ¡s probables
                        # Alternativa a top_p

repetition_penalty: 1.0 # Penalizar tokens repetidos
                        # > 1.0 = evita repeticiones
                        # 1.2 = penalidad moderada

do_sample: True         # Muestreo aleatorio
                        # False = greedy (siempre pick el mejor)
----

**Configuraciones Recomendadas:**

[source, python]
----
# OPCIÃ“N 1: Respuestas consistentes (Q&A, clasificaciÃ³n)
respuesta_consistente = {
    "max_new_tokens": 100,
    "temperature": 0.3,      # Bajo aleatoriedad
    "top_p": 0.95,
    "do_sample": False,      # DeterminÃ­stico
}

# OPCIÃ“N 2: Respuestas balanceadas (chat, conversaciÃ³n)
chat_balanc = {
    "max_new_tokens": 200,
    "temperature": 0.7,      # Medio aleatoriedad
    "top_p": 0.9,
    "do_sample": True,
    "repetition_penalty": 1.1
}

# OPCIÃ“N 3: Respuestas creativas (escritura, poesÃ­a)
creativo = {
    "max_new_tokens": 500,
    "temperature": 0.9,      # Alto aleatoriedad
    "top_p": 0.95,
    "do_sample": True,
    "top_k": 100
}

# Usar:
outputs = modelo.generate(**inputs, **chat_balance)
----

==== GeneraciÃ³n con Streaming

Para aplicaciones de chat en tiempo real, quieres ver el texto aparecer carÃ¡cter por carÃ¡cter (como ChatGPT):

[source, python]
----
from transformers import TextIteratorStreamer
from threading import Thread

# Configurar streamer
streamer = TextIteratorStreamer(
    tokenizer,
    skip_special_tokens=True
)

# Generar en un thread separado (no bloquea)
generation_kwargs = dict(
    **inputs,
    streamer=streamer,
    max_new_tokens=200,
    temperature=0.7,
)

thread = Thread(target=modelo.generate, kwargs=generation_kwargs)
thread.start()

# Imprimir tokens mientras se generan
print("Respuesta: ", end="", flush=True)
for text in streamer:
    print(text, end="", flush=True)
print()

thread.join()
----

Esto es especialmente Ãºtil para:
- Interfaces web (mostrar respuesta en tiempo real)
- Aplicaciones mÃ³viles (no esperar a que terminen 200 tokens)
- Mejor UX (feedback inmediato)

==== EvaluaciÃ³n Cualitativa

Antes de evaluar cuantitativamente, una evaluaciÃ³n manual:

[source, python]
----
# Test cases: ejemplo de entrada â†’ salida esperada
test_cases = [
    {
        "prompt": "Â¿CuÃ¡l es la capital de Francia?",
        "esperado": "Paris",
        "correcto": True  # Manualment establecido
    },
    {
        "prompt": "Escribe un poema sobre la luna",
        "esperado": "Cualquier poema coherente",
        "correcto": True
    },
]

print("ğŸ“‹ EVALUACIÃ“N CUALITATIVA")
print("-" * 60)

for i, test in enumerate(test_cases, 1):
    inputs = tokenizer(test["prompt"], return_tensors="pt").to("cuda")

    outputs = modelo.generate(
        **inputs,
        max_new_tokens=100,
        temperature=0.7,
    )

    respuesta = tokenizer.decode(outputs[0], skip_special_tokens=True)

    print(f"\nTest {i}:")
    print(f"  Prompt: {test['prompt']}")
    print(f"  Respuesta: {respuesta}")
    print(f"  Â¿Correcto?: {test['correcto']} âœ“" if test['correcto'] else f"  Â¿Correcto?: {test['correcto']} âœ—")
----

=== 4.2 Benchmarks Automatizados

Evaluar modelos necesita mÃ©tricas objetivas. AquÃ­ vemos las mÃ¡s comunes.

==== MÃ©tricas AutomÃ¡ticas Comunes

**1. Perplexity (Perplejidad)**

Mide quÃ© tan "sorprendido" es el modelo por el texto:

[source, python]
----
# Perplexity = e^(avg_loss)
# Loss = -log(P(siguiente token))

# Modelo A: perplexity = 10
#   "Bastante seguro de sus predicciones"

# Modelo B: perplexity = 100
#   "Muy inseguro, predice mal"

# CÃ³mo calcular:
from math import exp

def calcular_perplexity(logits, labels):
    loss = cross_entropy_loss(logits, labels)
    perplexity = exp(loss)
    return perplexity

# Benchmark:
# GPT-2: ~20
# BERT: ~76
# Tu modelo fine-tuned: Esperado <50
----

**2. BLEU (Bilingual Evaluation Understudy)**

Mide similitud entre respuesta generada y referencia:

[source, python]
----
from nltk.translate.bleu_score import sentence_bleu

referencia = ["el", "gato", "estÃ¡", "en", "la", "casa"]
generada = ["el", "gato", "estÃ¡", "en", "un", "casa"]

# 4-gram BLEU (compara n-gramas de 1-4 palabras)
score = sentence_bleu([referencia], generada, weights=(0.25, 0.25, 0.25, 0.25))

print(f"BLEU Score: {score:.4f}")
# 0 = completamente diferente
# 1 = idÃ©ntica

# InterpretaciÃ³n:
# BLEU < 0.1: Muy malo
# 0.1-0.3: Malo
# 0.3-0.5: Aceptable
# 0.5-0.7: Bueno
# 0.7+: Excelente
----

**3. ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**

Similar a BLEU pero enfocado en recuperaciÃ³n:

[source, python]
----
from rouge_score import rouge_scorer

scorer = rouge_scorer.RougeScorer(
    ['rouge1', 'rougeL'],  # Unigramas y Longest Common Subsequence
    use_stemmer=True
)

referencia = "El gato estÃ¡ en la casa"
generada = "El gato estÃ¡ en una casa"

scores = scorer.score(referencia, generada)

for metric, score in scores.items():
    print(f"{metric}: precision={score.precision:.3f}, recall={score.recall:.3f}, fmeasure={score.fmeasure:.3f}")

# ROUGE-1:  precision=0.833, recall=0.833, fmeasure=0.833
# ROUGE-L:  precision=0.833, recall=0.833, fmeasure=0.833
----

==== LLM-as-a-Judge: EvaluaciÃ³n con un LLM Superior

A veces, comparar con referencias es insuficiente. Usamos un LLM mÃ¡s poderoso como juez:

[source, python]
----
from transformers import AutoModelForCausalLM, AutoTokenizer

# Modelo juez (un modelo mÃ¡s grande/poderoso)
juez_modelo, juez_tokenizer = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-70b-chat"  # O GPT-4, Claude, etc.
)

def evaluar_con_llm(prompt, respuesta_modelo):
    """Un LLM grande evalÃºa la respuesta de tu modelo"""

    criterios = """
    EvalÃºa esta respuesta en escala 1-10:
    - PrecisiÃ³n: Â¿es factualmente correcta?
    - Claridad: Â¿es fÃ¡cil de entender?
    - Complecitud: Â¿responde toda la pregunta?

    Prompt: {prompt}
    Respuesta: {respuesta_modelo}

    PuntuaciÃ³n (1-10):
    """.format(prompt=prompt, respuesta_modelo=respuesta_modelo)

    inputs = juez_tokenizer(criterios, return_tensors="pt")
    outputs = juez_modelo.generate(**inputs, max_new_tokens=5)
    evaluacion = juez_tokenizer.decode(outputs[0])

    # Extraer nÃºmero
    import re
    match = re.search(r'\d+', evaluacion)
    score = int(match.group()) if match else 0

    return score

# Usar:
prompts = [
    "Â¿CuÃ¡l es la capital de Francia?",
    "Explica quÃ© es machine learning",
]

print("ğŸ“Š EVALUACIÃ“N CON LLM")
print("-" * 60)

scores = []
for prompt in prompts:
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = modelo.generate(**inputs, max_new_tokens=100)
    respuesta = tokenizer.decode(outputs[0])

    score = evaluar_con_llm(prompt, respuesta)
    scores.append(score)

    print(f"Prompt: {prompt}")
    print(f"Score: {score}/10")
    print()

print(f"Score promedio: {sum(scores) / len(scores):.1f}/10")
----

==== PrÃ¡ctica: Sistema de EvaluaciÃ³n Completo

**Archivo: `4_evaluacion_completa.py`**

[source, python]
----
"""
PrÃ¡ctica 4.1: EvaluaciÃ³n completa del modelo fine-tuneado
"""

import torch
from unsloth import FastLanguageModel
from rouge_score import rouge_scorer
import json

class EvaluadorModelo:
    def __init__(self, modelo_path):
        self.modelo, self.tokenizer = FastLanguageModel.from_pretrained(
            model_name=modelo_path,
            max_seq_length=2048,
            load_in_4bit=True,
        )
        self.modelo = FastLanguageModel.for_inference(self.modelo)
        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'])

    def generar(self, prompt, max_tokens=100):
        """Generar respuesta"""
        inputs = self.tokenizer(prompt, return_tensors="pt").to("cuda")
        outputs = self.modelo.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=0.7,
            top_p=0.9,
        )
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)

    def evaluar_rouge(self, referencia, generada):
        """Calcular ROUGE score"""
        scores = self.rouge_scorer.score(referencia, generada)
        return {
            'rouge1_f': scores['rouge1'].fmeasure,
            'rougeL_f': scores['rougeL'].fmeasure,
        }

    def evaluar_manualmente(self, test_cases):
        """EvaluaciÃ³n cualitativa"""
        resultados = []

        for i, test in enumerate(test_cases, 1):
            print(f"\n{'='*60}")
            print(f"Test {i}/{len(test_cases)}")
            print(f"{'='*60}")

            prompt = test['prompt']
            referencia = test.get('referencia', '')

            # Generar
            print(f"ğŸ“ Prompt: {prompt}")
            respuesta = self.generar(prompt)
            print(f"ğŸ¤– Respuesta: {respuesta}\n")

            # Evaluar ROUGE
            if referencia:
                rouge = self.evaluar_rouge(referencia, respuesta)
                print(f"ğŸ“Š ROUGE-1 F: {rouge['rouge1_f']:.3f}")
                print(f"ğŸ“Š ROUGE-L F: {rouge['rougeL_f']:.3f}\n")

                resultados.append({
                    'test': i,
                    'prompt': prompt,
                    'respuesta': respuesta,
                    'rouge': rouge
                })
            else:
                resultados.append({
                    'test': i,
                    'prompt': prompt,
                    'respuesta': respuesta,
                })

            # Pedir evaluaciÃ³n manual
            print("Â¿Correcto? (S/N): ", end="")

        return resultados

# Usar:
if __name__ == "__main__":
    evaluador = EvaluadorModelo("./mi_modelo_finetuned")

    test_cases = [
        {
            "prompt": "Â¿CuÃ¡l es la capital de EspaÃ±a?",
            "referencia": "La capital de EspaÃ±a es Madrid",
        },
        {
            "prompt": "Explica quÃ© es el machine learning",
            "referencia": "Machine learning es una rama de la IA...",
        },
    ]

    resultados = evaluador.evaluar_manualmente(test_cases)

    # Guardar resultados
    with open("evaluacion_resultados.json", "w") as f:
        json.dump(resultados, f, indent=2, ensure_ascii=False)

    print("\nâœ… EvaluaciÃ³n completada. Resultados guardados en evaluacion_resultados.json")
----

**Checklist: Modelo Evaluado âœ…**

[source, text]
----
â–¡ Modelo cargado en modo inferencia
â–¡ GeneraciÃ³n de texto funcionando
â–¡ ParÃ¡metros de generaciÃ³n ajustados
â–¡ MÃ©tricas automÃ¡ticas (ROUGE/BLEU) calculadas
â–¡ EvaluaciÃ³n manual realizada
â–¡ Resultados documentados

Â¡EstÃ¡s listo para el MÃ³dulo 5 (ExportaciÃ³n) ğŸš€
----

== MÃ³dulo 5: ExportaciÃ³n y Despliegue

=== 5.1 Guardado de Modelos

Una vez fine-tuneado el modelo, necesitas guardarlo en un formato que puedas compartir y desplegar.

==== Guardar Adaptadores LoRA

Los adaptadores LoRA son **muy pequeÃ±os** (4-50MB), perfectos para compartir:

[source, python]
----
# GuardaciÃ³n simple
modelo.save_pretrained("./mi_modelo_finetuned")
tokenizer.save_pretrained("./mi_modelo_finetuned")

# Estructura generada:
# mi_modelo_finetuned/
# â”œâ”€â”€ adapter_config.json       # ConfiguraciÃ³n LoRA (1KB)
# â”œâ”€â”€ adapter_model.bin          # Pesos LoRA (4-50MB) â† MUY PEQUEÃ‘O!
# â”œâ”€â”€ tokenizer.json
# â”œâ”€â”€ tokenizer.model
# â””â”€â”€ tokenizer_config.json

# Ventajas:
# âœ… PequeÃ±o (puedes subirlo a GitHub, HuggingFace, etc)
# âœ… RÃ¡pido de cargar
# âœ… FÃ¡cil de compartir

# Desventaja:
# âŒ Necesita el modelo base original para usar
----

**Subir a Hugging Face Hub:**

[source, python]
----
from huggingface_hub import login, create_repo

# 1. Crear token en huggingface.co/settings/tokens
login()  # Pedir token

# 2. Crear repositorio
repo_id = "tu_usuario/llama2-finetuned-chat"
create_repo(repo_id, repo_type="model")

# 3. Subir
modelo.push_to_hub(repo_id)
tokenizer.push_to_hub(repo_id)

print(f"Modelo subido a: https://huggingface.co/{repo_id}")

# Para cargar despuÃ©s:
# modelo, tokenizer = FastLanguageModel.from_pretrained(
#     model_name=f"{repo_id}",
# )
----

==== Fusionar Adaptadores (Merge LoRA)

A veces necesitas un modelo **completo** que no requiera el original:

[source, python]
----
# LoRA aplicado (lo que tenÃ­as antes)
modelo = FastLanguageModel.get_peft_model(...)

# FUSIONAR: Combinar modelo base + adaptadores LoRA
modelo = modelo.merge_and_unload()

# Ahora tienes un modelo "normal" completo
# Sin necesidad del modelo base original

# Guardar modelo fusionado (archivo grande)
modelo.save_pretrained("./llama2_finetuned_merged")

# Resultado:
# model.safetensors: 14GB (7B params Ã— 2 para seguridad)
# âŒ Mucho mÃ¡s grande
# âœ… Completamente independiente
# âœ… Compatible con cualquier herramienta

# Decidir:
# - Solo LoRA (pequeÃ±o): Ideal para dev, research
# - Merged (grande): Ideal para producciÃ³n, distribuciÃ³n
----

==== Guardar en Diferentes Formatos

**OpciÃ³n 1: Float16 (Recomendado para balance)**

[source, python]
----
import torch

# Model en float16 (menor precisiÃ³n, menos memoria)
modelo = modelo.to(torch.float16)
modelo.save_pretrained("./modelo_fp16")

# TamaÃ±o: ~7GB (50% menos que float32)
# PrecisiÃ³n: 99% igual a float32
----

**OpciÃ³n 2: Float32 (MÃ¡xima precisiÃ³n)**

[source, python]
----
# Float32 (mÃ¡xima precisiÃ³n)
modelo = modelo.to(torch.float32)
modelo.save_pretrained("./modelo_fp32")

# TamaÃ±o: ~14GB
# PrecisiÃ³n: 100%
# Uso: Poca. Float32 es el estÃ¡ndar pero costoso en memoria
----

**OpciÃ³n 3: BFloat16 (PrecisiÃ³n+Velocidad)**

[source, python]
----
# BFloat16 (balance perfecto en Ampere+ GPUs)
modelo = modelo.to(torch.bfloat16)
modelo.save_pretrained("./modelo_bf16")

# TamaÃ±o: ~7GB (como float16)
# PrecisiÃ³n: Mejor que float16
# Velocidad: Mejor que float32
# Nota: Solo funciona en GPU Ampere (A100, RTX 3000+)
----

**Comparativa:**

[cols="20,15,15,20"]
|===
| Formato | TamaÃ±o | Velocidad | PrecisiÃ³n

| Float32
| 14GB
| Lenta
| 100%

| Float16
| 7GB
| RÃ¡pida
| 99%

| BFloat16
| 7GB
| RÃ¡pida
| 99.5%

| Int8 (Cuantizado)
| 3.5GB
| Muy rÃ¡pida
| 97%

| Int4 (Cuantizado)
| 1.75GB
| Extremadamente rÃ¡pida
| 95%
|===

=== 5.2 Formato GGUF y Ollama

GGUF es un formato **optimizado para CPU e inference rÃ¡pida**. Perfecto para ejecutar modelos en laptop sin GPU.

==== Â¿QuÃ© es GGUF?

[source, text]
----
GGUF = GPT-Generated Unified Format

DiseÃ±ado por ggml.ai para:
âœ… Inference rÃ¡pida en CPU
âœ… Bajo uso de memoria
âœ… CuantizaciÃ³n flexible
âœ… Portabilidad (Windows, Mac, Linux)

Comparativa:
- Formato HF (.safetensors): 14GB en GPU
- Formato GGUF q4_k_m:        3-4GB en CPU â† 4x mÃ¡s eficiente!
----

==== Convertir a GGUF con Unsloth

[source, python]
----
from unsloth import FastLanguageModel

# Cargar modelo fine-tuneado
modelo, tokenizer = FastLanguageModel.from_pretrained(
    model_name="./mi_modelo_finetuned",
)

# Fusionar primero (obligatorio para GGUF)
modelo = modelo.merge_and_unload()

# Convertir a GGUF
modelo.save_pretrained_gguf(
    "modelo_gguf",           # Carpeta salida
    quantization_method="q4_k_m"  # MÃ©todo cuantizaciÃ³n
)

# Resultado:
# modelo_gguf/
# â””â”€â”€ model.gguf  (3-4GB)
----

**MÃ©todos de CuantizaciÃ³n GGUF:**

[cols="15,15,20,20"]
|===
| Formato | TamaÃ±o | Velocidad | PrecisiÃ³n

| F32
| 14GB
| Lenta
| MÃ¡xima

| F16
| 7GB
| Media
| Muy buena

| Q8_0
| 5.5GB
| Buena
| Excelente

| Q5_K_M
| 4.3GB
| Buena
| Excelente

| Q4_K_M
| 3.5GB
| Muy buena
| Buena

| Q4_0
| 3.5GB
| Muy buena
| Aceptable

| Q3_K_M
| 2.6GB
| Excelente
| Aceptable

| Q2_K
| 2.1GB
| Excelente
| Pobre
|===

**RecomendaciÃ³n:**
- Laptop (8GB RAM): Usa q4_k_m
- Desktop (16GB+ RAM): Usa q5_k_m
- MÃ¡xima calidad: Usa q8_0
- MÃ¡xima velocidad: Usa q3_k_m

==== Ejecutar en Ollama

Ollama es una aplicaciÃ³n que ejecuta modelos GGUF **ultra fÃ¡cil**:

[source, bash]
----
# 1. Descargar Ollama desde ollama.ai
# 2. Instalar

# 3. Crear Modelfile
cat > Modelfile <<EOF
FROM ./modelo_gguf/model.gguf
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER stop "<|im_end|>"
EOF

# 4. Crear modelo en Ollama
ollama create mi-modelo -f Modelfile

# 5. Ejecutar
ollama run mi-modelo "Â¿CuÃ¡l es la capital de Francia?"

# Salida:
# Loading model...
# La capital de Francia es ParÃ­s.
----

**Crear Modelfile personalizado:**

[source, dockerfile]
----
# Modelfile
FROM ./mi_modelo.gguf

# ParÃ¡metros por defecto
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 50
PARAMETER num_predict 200

# InstrucciÃ³n del sistema
SYSTEM """Eres un asistente Ãºtil que responde en espaÃ±ol."""

# Tokens de parada (cuÃ¡ndo dejar de generar)
PARAMETER stop "<|im_end|>"
PARAMETER stop "[end]"

# Prompt template
TEMPLATE """[INST] {{ .Prompt }} [/INST]"""
----

==== PrÃ¡ctica: ExportaciÃ³n Completa

**Archivo: `5_exportar_modelo.py`**

[source, python]
----
"""
PrÃ¡ctica 5.1: Exportar modelo para producciÃ³n
Genera: LoRA adaptadores, modelo fusionado, y GGUF
"""

import torch
from unsloth import FastLanguageModel
import os

class ExportadorModelo:
    def __init__(self, modelo_path):
        print("â³ Cargando modelo...")
        self.modelo, self.tokenizer = FastLanguageModel.from_pretrained(
            model_name=modelo_path,
            max_seq_length=2048,
            load_in_4bit=True,
        )

    def guardar_lora(self, output_dir="./exports/lora"):
        """Guardar solo adaptadores LoRA (pequeÃ±o)"""
        print(f"\nğŸ’¾ Guardando LoRA en {output_dir}...")
        os.makedirs(output_dir, exist_ok=True)

        self.modelo.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)

        # Calcular tamaÃ±o
        import shutil
        size_mb = shutil.disk_usage(output_dir).used / 1024 / 1024
        print(f"âœ… LoRA guardado. TamaÃ±o: {size_mb:.1f}MB")

        return output_dir

    def guardar_fusionado(self, output_dir="./exports/merged"):
        """Guardar modelo completamente fusionado"""
        print(f"\nğŸ’¾ Fundiendo y guardando en {output_dir}...")
        os.makedirs(output_dir, exist_ok=True)

        # Fusionar
        modelo_fusionado = self.modelo.merge_and_unload()

        # Guardar en float16 (balance)
        modelo_fusionado = modelo_fusionado.to(torch.float16)
        modelo_fusionado.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)

        import shutil
        size_gb = shutil.disk_usage(output_dir).used / 1024 / 1024 / 1024
        print(f"âœ… Modelo fusionado guardado. TamaÃ±o: {size_gb:.1f}GB")

        return output_dir

    def guardar_gguf(self, output_dir="./exports/gguf"):
        """Convertir a GGUF (para Ollama)"""
        print(f"\nğŸ’¾ Convirtiendo a GGUF en {output_dir}...")
        os.makedirs(output_dir, exist_ok=True)

        modelo_fusionado = self.modelo.merge_and_unload()

        # Convertir
        modelo_fusionado.save_pretrained_gguf(
            output_dir,
            quantization_method="q4_k_m"
        )

        import os
        gguf_size = os.path.getsize(os.path.join(output_dir, "model.gguf")) / 1024 / 1024
        print(f"âœ… GGUF guardado. TamaÃ±o: {gguf_size:.1f}MB")

        return output_dir

    def crear_modelfile(self, gguf_dir, output_file="./exports/Modelfile"):
        """Crear Modelfile para Ollama"""
        print(f"\nğŸ“ Creando Modelfile...")

        modelfile_content = f"""FROM {os.path.abspath(os.path.join(gguf_dir, "model.gguf"))}

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 50
PARAMETER num_predict 200

SYSTEM "Eres un asistente Ãºtil."

PARAMETER stop "[INST]"
PARAMETER stop "INST]"

TEMPLATE "[INST] {{ .Prompt }} [/INST]"
"""

        with open(output_file, "w") as f:
            f.write(modelfile_content)

        print(f"âœ… Modelfile creado en {output_file}")
        print(f"\nPara usar con Ollama:")
        print(f"  ollama create mi-modelo -f {output_file}")
        print(f"  ollama run mi-modelo 'Tu prompt aquÃ­'")

# Usar:
if __name__ == "__main__":
    exportador = ExportadorModelo("./mi_modelo_finetuned")

    print("\nğŸ“¦ PROCESO DE EXPORTACIÃ“N")
    print("=" * 60)

    # Exportar en todos los formatos
    lora_dir = exportador.guardar_lora()
    merged_dir = exportador.guardar_fusionado()
    gguf_dir = exportador.guardar_gguf()

    # Crear Modelfile
    exportador.crear_modelfile(gguf_dir)

    print("\n" + "=" * 60)
    print("âœ… EXPORTACIÃ“N COMPLETADA")
    print("-" * 60)
    print(f"LoRA (para desarrollo):     {lora_dir}")
    print(f"Fusionado (para prod):      {merged_dir}")
    print(f"GGUF (para Ollama):         {gguf_dir}")
    print("-" * 60)
    print("\nğŸ“‹ PrÃ³ximos pasos:")
    print("1. Subir LoRA a Hugging Face Hub: push_to_hub()")
    print("2. Usar Ollama localmente con GGUF")
    print("3. Desplegar modelo fusionado en producciÃ³n")
----

**Checklist: Modelo Exportado âœ…**

[source, text]
----
â–¡ Adaptadores LoRA guardados
â–¡ Modelo fusionado generado
â–¡ ConversiÃ³n a GGUF exitosa
â–¡ Modelfile creado para Ollama
â–¡ Todos los formatos probados

Â¡EstÃ¡s listo para el MÃ³dulo 6 (TÃ©cnicas Avanzadas) ğŸš€
----

== MÃ³dulo 6: TÃ©cnicas Avanzadas

=== 6.1 DPO (Direct Preference Optimization)

DPO es una tÃ©cnica para hacer que los modelos produzcan respuestas que los humanos prefieren, sin necesitar un modelo de recompensa separado (como en RLHF).

==== SFT vs. RLHF vs. DPO: La ProgresiÃ³n

**SFT (Supervised Fine-Tuning) - Lo que hemos hecho:**

[source, python]
----
# SFT: Simplemente aprende a copiar respuestas de referencia
Dataset: {
    "prompt": "Â¿CuÃ¡l es 2+2?",
    "respuesta": "2+2 = 4"
}

Modelo aprende: prompt â†’ respuesta
LimitaciÃ³n: Si la respuesta de referencia es mediocre, el modelo aprende a ser mediocre
----

**RLHF (Reinforcement Learning from Human Feedback) - Tradicional:**

[source, python]
----
# RLHF: Usa un modelo de recompensa (reward model) para guiar al modelo
Paso 1: Entrenar modelo de recompensa (aprende quÃ© es "bueno")
Paso 2: Usar reward model para entrenar el modelo principal (RL)

Ventaja: El modelo mejora iterativamente
Desventaja: Complejo, 3 modelos implicados, lento
Ejemplo: Como lo hace ChatGPT (OpenAI)
----

**DPO (Direct Preference Optimization) - Moderno y simple:**

[source, python]
----
# DPO: Aprende directamente de comparaciones (sin reward model explÃ­cito)
Dataset: {
    "prompt": "Â¿CuÃ¡l es 2+2?",
    "respuesta_buena": "2+2 = 4",
    "respuesta_mala": "2+2 = 5"
}

El modelo aprende: "La respuesta buena es mejor que la mala"
Sin necesidad de un modelo de recompensa separado

Ventaja: MÃ¡s simple que RLHF, 1 modelo en lugar de 3
Ventaja: MÃ¡s eficiente en memoria y tiempo
Desventaja: Funciona mejor con SFT previo
----

==== PreparaciÃ³n de Datasets de Preferencia

**Formato de Dataset DPO:**

[source, json]
----
{
    "prompt": "Â¿CuÃ¡l es la capital de Francia?",
    "chosen": "La capital de Francia es ParÃ­s.",
    "rejected": "La capital de Francia es Londres."
}
----

**CÃ³mo crear datos de preferencia:**

OpciÃ³n 1: Usar un LLM mÃ¡s grande para generar ambas respuestas

[source, python]
----
import json
from transformers import AutoModelForCausalLM, AutoTokenizer

# Modelo grande (GPT-4, Llama 70B, Claude, etc.)
juez, juez_tokenizer = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-70b-chat")

prompts = [
    "Â¿CuÃ¡l es la capital de Francia?",
    "Explica machine learning en 1 lÃ­nea",
]

dataset_dpo = []

for prompt in prompts:
    # Generar 2 respuestas diferentes
    inputs = juez_tokenizer(prompt, return_tensors="pt")

    # Respuesta 1 (temperature alta = diversa)
    outputs1 = juez.generate(**inputs, temperature=0.9, max_new_tokens=50)
    respuesta1 = juez_tokenizer.decode(outputs1[0])

    # Respuesta 2 (temperature baja = conservadora)
    outputs2 = juez.generate(**inputs, temperature=0.1, max_new_tokens=50)
    respuesta2 = juez_tokenizer.decode(outputs2[0])

    # Evaluar cuÃ¡l es mejor (manualmente o con heurÃ­stica)
    # AquÃ­ asumimos que la respuesta con mayor probabilidad es "mejor"
    dataset_dpo.append({
        "prompt": prompt,
        "chosen": respuesta2,      # MÃ¡s probable
        "rejected": respuesta1,    # Menos probable
    })

# Guardar
with open("dpo_dataset.json", "w") as f:
    json.dump(dataset_dpo, f, indent=2, ensure_ascii=False)
----

OpciÃ³n 2: Usar evaluaciÃ³n humana

[source, python]
----
# Recopilar pares de respuestas
# Pedir a humanos que elijan la mejor
# Guardar como DPO dataset

dataset_dpo = []

# De evaluadores humanos:
pares_evaluados = [
    {
        "prompt": "Â¿CuÃ¡l es 2+2?",
        "respuesta_a": "2+2 = 4",
        "respuesta_b": "2+2 = 5",
        "mejor": "respuesta_a"  # Humano prefiere A
    },
    ...
]

for par in pares_evaluados:
    if par["mejor"] == "respuesta_a":
        dataset_dpo.append({
            "prompt": par["prompt"],
            "chosen": par["respuesta_a"],
            "rejected": par["respuesta_b"],
        })
    else:
        dataset_dpo.append({
            "prompt": par["prompt"],
            "chosen": par["respuesta_b"],
            "rejected": par["respuesta_a"],
        })
----

==== Entrenar con DPOTrainer

[source, python]
----
from trl import DPOTrainer
from transformers import TrainingArguments
from unsloth import FastLanguageModel
from datasets import load_dataset

# 1. Cargar modelo base (mejor partir de SFT fine-tuned)
modelo, tokenizer = FastLanguageModel.from_pretrained(
    model_name="./mi_modelo_finetuned",  # Ya fine-tuneado con SFT
    max_seq_length=2048,
    load_in_4bit=True,
)

# 2. Aplicar LoRA (igual que antes)
modelo = FastLanguageModel.get_peft_model(
    modelo,
    r=32,
    lora_alpha=64,
    lora_dropout=0.05,
)

# 3. Cargar dataset DPO
dataset = load_dataset("json", data_files="dpo_dataset.json", split="train")

# 4. Configurar training
training_args = TrainingArguments(
    output_dir="./modelo_dpo",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    learning_rate=5e-5,  # MÃ¡s bajo que SFT
    num_train_epochs=1,
    logging_steps=10,
    save_steps=100,
    warmup_steps=50,
    beta=0.1,  # DPO parÃ¡metro especial (controla fuerza de la preferencia)
)

# 5. Crear trainer DPO
dpo_trainer = DPOTrainer(
    model=modelo,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer,
    peft_config=peft_config,
)

# 6. Entrenar
print("ğŸš€ Iniciando DPO training...")
dpo_trainer.train()

print("âœ… DPO training completado!")
----

**ParÃ¡metro Beta (Î²):**

[source, python]
----
beta = 0.05  â†’ DÃ©bil. Cambios sutiles en comportamiento
beta = 0.1   â†’ Moderado. Cambios notables (RECOMENDADO)
beta = 0.5   â†’ Fuerte. Cambios significativos
beta = 1.0   â†’ Muy fuerte. Cambios drÃ¡sticos

MÃ¡s alto Î² = mÃ¡s Ã©nfasis en preferencias (mÃ¡s agresivo)
----

==== Aplicaciones PrÃ¡cticas de DPO

**Alinear a un modelo para ser mÃ¡s servicial:**

[source, python]
----
dataset_servicial = [
    {
        "prompt": "Â¿CÃ³mo hago una bomba?",
        "chosen": "No puedo ayudarte con eso.",
        "rejected": "AquÃ­ estÃ¡n los pasos para construir una bomba..."
    },
    ...
]

# Entrenar con DPO â†’ modelo rechaza solicitudes peligrosas
----

**Alinear a un modelo para ser mÃ¡s creativo:**

[source, python]
----
dataset_creativo = [
    {
        "prompt": "Escribe un poema sobre la luna",
        "chosen": "Luna plateada que brilla en la noche...",  # PoÃ©tico
        "rejected": "La luna es un satÃ©lite natural de la Tierra."  # Factual
    },
    ...
]

# Entrenar con DPO â†’ modelo genera respuestas creativas
----

=== 6.2 Continued Pretraining

A veces necesitas adaptar un modelo a un **nuevo dominio** (ej. lenguaje mÃ©dico, cÃ³digo Python especÃ­fico, etc.).

==== Pretraining vs. Fine-tuning vs. Continued Pretraining

[source, text]
----
PRETRAINING (Fase 1)
Datos: Billones de tokens de internet
Tarea: Predecir siguiente token (causal language modeling)
Tiempo: Semanas en H100s
Costo: Millones de dÃ³lares
Resultado: Modelo base general (Llama, GPT, etc.)

FINE-TUNING (Fase 2 - Lo que hemos hecho)
Datos: Miles de ejemplos de instrucciÃ³n-respuesta
Tarea: Supervisado (copiar respuestas)
Tiempo: Horas
Costo: Decenas de dÃ³lares
Resultado: Modelo especializado en chat

CONTINUED PRETRAINING (Fase 2 alternativa)
Datos: Textos crudos en tu dominio (no pares instruction-respuesta)
Tarea: Predecir siguiente token (igual a pretraining)
Tiempo: Horas
Costo: Decenas de dÃ³lares
Resultado: Modelo que entiende tu dominio especial
----

==== CuÃ¡ndo Usar Continued Pretraining

[source, text]
----
âœ… Usa Continued Pretraining si:
   - Tienes mucho texto en tu dominio (100K+ documentos)
   - Lenguaje tÃ©cnico especial (medicina, legal, cÃ³digo)
   - Quieres que el modelo entienda tu jerga
   - Tienes tiempo/recursos

âŒ No uses si:
   - Pocos ejemplos (<10K)
   - Dominio similar a internet (general knowledge)
   - Necesitas urgentemente
----

==== ImplementaciÃ³n

[source, python]
----
from transformers import TextDataset, DataCollatorForLanguageModeling
from transformers import Trainer, TrainingArguments
from unsloth import FastLanguageModel

# 1. Preparar datos crudos (plain text)
# archivo: "textos_medicos.txt"
# Contiene: pÃ¡rrafos de textos mÃ©dicos, uno por lÃ­nea

# 2. Cargar modelo
modelo, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-2-7b-bnb-4bit",
    max_seq_length=2048,
    load_in_4bit=True,
)

# 3. Preparar dataset de texto
train_dataset = TextDataset(
    tokenizer=tokenizer,
    file_path="textos_medicos.txt",
    block_size=2048,
)

# 4. Data collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,  # False = causal language modeling (no masked)
)

# 5. Training arguments
training_args = TrainingArguments(
    output_dir="./modelo_continued_pretrain",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-5,  # MÃ¡s bajo que fine-tuning
    num_train_epochs=3,
    save_steps=1000,
    logging_steps=100,
)

# 6. Entrenar
trainer = Trainer(
    model=modelo,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=data_collator,
)

trainer.train()
----

=== 6.3 Contexto Largo (Long Context)

Llama 2 soporta 4096 tokens. Â¿QuÃ© si necesitas 8192 o mÃ¡s?

==== RoPE Scaling

**RoPE = Rotary Position Embeddings**

Los modelos usan "posiciones" para saber dÃ³nde estÃ¡ cada token en la secuencia:

[source, python]
----
# Llama 2 original
max_position_embeddings = 4096
# El modelo "no sabe" quÃ© hacer mÃ¡s allÃ¡ de 4096 tokens

# Con RoPE Scaling
max_position_embeddings = 8192  # Duplicamos el contexto
rope_scaling = {
    "type": "linear",   # Linear scaling
    "factor": 2.0       # Escalar 2x
}

# El modelo aprende a interpolar posiciones para contexto mÃ¡s largo
----

**MÃ©todos de scaling:**

[source, python]
----
# Linear scaling: Simplemente escalar linealmente
# Funciona pero menos Ã³ptimo
rope_scaling = {"type": "linear", "factor": 2.0}

# NTK scaling: Ajuste inteligente del frequency
# Mejor que linear, recomendado
rope_scaling = {"type": "ntk", "factor": 2.0}

# YaRN: CombinaciÃ³n de linear + ntk
# El mejor, experimental
rope_scaling = {"type": "yarn", "factor": 2.0, "original_max_position_embeddings": 4096}
----

==== Fine-tuning con Contexto Largo

[source, python]
----
from unsloth import FastLanguageModel
import torch

# 1. Cargar con contexto largo
modelo, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-2-7b-bnb-4bit",
    max_seq_length=8192,  # 2x el original
    load_in_4bit=True,
)

# 2. Aplicar RoPE scaling automÃ¡ticamente
# Unsloth lo hace automÃ¡ticamente cuando usas max_seq_length > 4096

# 3. Aplicar LoRA
modelo = FastLanguageModel.get_peft_model(
    modelo,
    r=32,
    lora_alpha=64,
    lora_dropout=0.05,
)

# 4. Fine-tuning normal
# (Usando dataset con secuencias largas)

# 5. Usar el modelo
modelo = FastLanguageModel.for_inference(modelo)

long_prompt = "..." * 5000  # 5000 tokens de contexto

inputs = tokenizer(long_prompt, return_tensors="pt").to("cuda")
outputs = modelo.generate(**inputs, max_new_tokens=100)
respuesta = tokenizer.decode(outputs[0])
----

**Limitaciones:**

[source, text]
----
âš ï¸ Contextos muy largos consumen mucha memoria
max_seq_length = 4096: 8GB
max_seq_length = 8192: 16GB
max_seq_length = 32768: 64GB+

Soluciones:
- Usar quantizaciÃ³n 4-bit (visto antes)
- Usar flash attention 2 (Unsloth lo hace automÃ¡ticamente)
- Reducir batch size
- Usar gradient checkpointing
----

==== Caso de Uso: Resumen de Documentos Largos

[source, python]
----
# Documento muy largo (ej. 15 pÃ¡ginas = 5000 tokens)
documento_largo = "..." * 5000

prompt = f"""
Resumo el siguiente documento en 3 puntos principales:

{documento_largo}

Resumen:
"""

inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
outputs = modelo.generate(
    **inputs,
    max_new_tokens=200,
    temperature=0.3,  # DeterminÃ­stico para resumen
)

resumen = tokenizer.decode(outputs[0])
print(resumen)
----

**PrÃ¡ctica: TÃ©cnicas Avanzadas Completas**

**Archivo: `6_tecnicas_avanzadas.py`**

[source, python]
----
"""
PrÃ¡ctica 6.1: DPO, Continued Pretraining, Long Context
"""

from unsloth import FastLanguageModel
from trl import DPOTrainer
from transformers import TrainingArguments
from datasets import load_dataset
import torch

class EntrenadorAvanzado:
    def __init__(self, modelo_base):
        self.modelo_base = modelo_base

    def hacer_dpo(self, dataset_path, output_dir="./modelo_dpo"):
        """Entrenar con DPO"""
        print("ğŸ”„ DPO Training...")

        modelo, tokenizer = FastLanguageModel.from_pretrained(
            model_name=self.modelo_base,
            max_seq_length=2048,
            load_in_4bit=True,
        )

        modelo = FastLanguageModel.get_peft_model(
            modelo,
            r=32,
            lora_alpha=64,
            lora_dropout=0.05,
        )

        dataset = load_dataset("json", data_files=dataset_path, split="train")

        training_args = TrainingArguments(
            output_dir=output_dir,
            per_device_train_batch_size=4,
            gradient_accumulation_steps=2,
            learning_rate=5e-5,
            num_train_epochs=1,
            warmup_steps=50,
        )

        dpo_trainer = DPOTrainer(
            model=modelo,
            args=training_args,
            train_dataset=dataset,
            tokenizer=tokenizer,
        )

        dpo_trainer.train()
        print("âœ… DPO completado!")

        return modelo, tokenizer

    def hacer_continued_pretraining(self, texto_path, output_dir="./modelo_pretrain"):
        """Continued pretraining en dominio especÃ­fico"""
        print("ğŸ“š Continued Pretraining...")

        modelo, tokenizer = FastLanguageModel.from_pretrained(
            model_name=self.modelo_base,
            max_seq_length=2048,
            load_in_4bit=True,
        )

        from transformers import TextDataset, DataCollatorForLanguageModeling, Trainer

        dataset = TextDataset(
            tokenizer=tokenizer,
            file_path=texto_path,
            block_size=2048,
        )

        training_args = TrainingArguments(
            output_dir=output_dir,
            per_device_train_batch_size=4,
            learning_rate=2e-5,
            num_train_epochs=1,
        )

        trainer = Trainer(
            model=modelo,
            args=training_args,
            train_dataset=dataset,
            data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
        )

        trainer.train()
        print("âœ… Continued pretraining completado!")

        return modelo

    def usar_long_context(self, max_seq=8192):
        """Fine-tuning con contexto largo"""
        print(f"ğŸ”­ Usando contexto largo ({max_seq} tokens)...")

        modelo, tokenizer = FastLanguageModel.from_pretrained(
            model_name=self.modelo_base,
            max_seq_length=max_seq,  # Contexto largo
            load_in_4bit=True,
        )

        print(f"âœ… Modelo cargado con {max_seq} tokens de contexto mÃ¡ximo")
        print(f"   Memoria actual: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")

        return modelo, tokenizer

# Usar:
if __name__ == "__main__":
    entrenador = EntrenadorAvanzado("unsloth/llama-2-7b-bnb-4bit")

    print("\nğŸš€ TÃ‰CNICAS AVANZADAS")
    print("=" * 60)

    # DPO
    # modelo_dpo, tok_dpo = entrenador.hacer_dpo("dpo_dataset.json")

    # Continued pretraining
    # modelo_pretrain = entrenador.hacer_continued_pretraining("textos_medicos.txt")

    # Long context
    modelo_long, tok_long = entrenador.usar_long_context(max_seq=8192)

    print("\nâœ… Todas las tÃ©cnicas avanzadas disponibles!")
----

**Checklist: TÃ©cnicas Avanzadas Dominadas âœ…**

[source, text]
----
â–¡ DPO training entendido
â–¡ Datos de preferencia preparados
â–¡ Continued pretraining practicado
â–¡ Long context utilizado
â–¡ RoPE scaling comprendido

Â¡Has completado el MÃ³dulo 6! ğŸ‰
----

== Proyecto Final
* DefiniciÃ³n del problema.
* SelecciÃ³n del modelo base y dataset.
* EjecuciÃ³n del pipeline completo (SFT -> Export -> Demo).
* PresentaciÃ³n de resultados.

== Recursos Adicionales
* DocumentaciÃ³n oficial de Unsloth.
* Repositorios de Hugging Face recomendados.
* Comunidades de Discord y Reddit.
