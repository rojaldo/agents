<!doctype html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Curso de Fine-Tuning Eficiente con Unsloth</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/theme/white.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/atom-one-light.min.css">
    <style>
        .reveal { text-align: left; color: #555555; }
        .reveal section { text-align: left; padding: 40px; display: flex; flex-direction: column; justify-content: flex-start; }
        .reveal h1, .reveal h2, .reveal h3 { text-transform: none; text-align: left; color: #555555; }

        .reveal h1 { font-size: 1.05em; margin-bottom: 0.5em; }
        .reveal h2 { font-size: 1em; margin-bottom: 0.5em; }
        .reveal h3 { font-size: 0.75em; margin-bottom: 0.3em; }

        .reveal p { font-size: 0.6em; margin: 0.3em 0; color: #555555; }
        .reveal strong { font-size: 1em; font-weight: bold; }

        .reveal pre { background: #f8f8f8; border: 1px solid #ddd; width: 100%; padding: 0.5em; margin: 0.5em 0; }
        .reveal pre code { font-size: 0.7em; color: #555555; line-height: 1.2; }

        .reveal ul { font-size: 0.55em; text-align: left; margin-left: 0.5em; color: #555555; }
        .reveal li { margin: 0.3em 0; color: #555555; }

        .reveal table { font-size: 0.55em; margin: 0.5em 0; }
        .reveal table td { padding: 0.3em; }

        .title-slide { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white !important; }
        .title-slide h1, .title-slide h2, .title-slide h3, .title-slide p { color: white; }

        .highlight { background: #fff3cd; padding: 2px 4px; border-radius: 3px; }
        .success { color: #28a745; font-weight: bold; }
        .warning { color: #ffc107; font-weight: bold; }
        .error { color: #dc3545; font-weight: bold; }
        .info { color: #17a2b8; font-weight: bold; }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">

            <!-- INTRODUCCI√ìN -->
            <section class="title-slide">
                <h1>üöÄ Fine-Tuning Eficiente con Unsloth</h1>
                <p style="font-size: 0.8em; margin-top: 1em;">Entrenar Modelos de Lenguaje R√°pido y Econ√≥mico</p>
            </section>

            <section>
                <h2>¬øQu√© aprender√°s?</h2>
                <ul>
                    <li><strong>Fundamentos:</strong> C√≥mo funciona Unsloth y sus ventajas</li>
                    <li><strong>Configuraci√≥n:</strong> Setup en Colab y localmente</li>
                    <li><strong>Fine-tuning:</strong> LoRA y QLoRA paso a paso</li>
                    <li><strong>Entrenamiento:</strong> TrainingArguments y SFTTrainer</li>
                </ul>
            </section>

            <section>
                <h2>¬øQu√© aprender√°s? (Continuaci√≥n)</h2>
                <ul>
                    <li><strong>Evaluaci√≥n:</strong> Inferencia y m√©tricas</li>
                    <li><strong>Exportaci√≥n:</strong> GGUF, Ollama, Hugging Face</li>
                    <li><strong>Avanzadas:</strong> DPO, Continued Pretraining, Long Context</li>
                    <li><strong>Producci√≥n:</strong> Mejores pr√°cticas y deployment</li>
                </ul>
            </section>

            <section>
                <h2>¬øPor qu√© Unsloth?</h2>
                <ul>
                    <li><span class="success">‚úÖ 2x m√°s r√°pido</span> que m√©todos tradicionales</li>
                    <li><span class="success">‚úÖ 80% menos memoria</span> consumida</li>
                    <li><span class="success">‚úÖ Misma calidad</span> de entrenamiento</li>
                </ul>
            </section>

            <section>
                <h2>Comparativa: Con vs Sin Unsloth</h2>
                <table>
                    <tr>
                        <td><strong>M√©trica</strong></td>
                        <td><strong>Sin Unsloth</strong></td>
                        <td><strong>Con Unsloth</strong></td>
                    </tr>
                    <tr>
                        <td>Tiempo/epoch</td>
                        <td>120 min</td>
                        <td>60 min ‚ö°</td>
                    </tr>
                    <tr>
                        <td>Memoria GPU</td>
                        <td>40 GB</td>
                        <td>8 GB üíæ</td>
                    </tr>
                    <tr>
                        <td>Tokens/seg</td>
                        <td>500</td>
                        <td>1000 üìà</td>
                    </tr>
                    <tr>
                        <td>Costo/hora</td>
                        <td>$10.32</td>
                        <td>$3.06 üí∞</td>
                    </tr>
                </table>
            </section>

            <!-- M√ìDULO 1: FUNDAMENTOS -->
            <section>
                <h2>üìö M√≥dulo 1: Fundamentos</h2>
            </section>

            <section>
                <h3>¬øQu√© es Unsloth?</h3>
                <p><strong>Unsloth</strong> es una librer√≠a que optimiza LLM training:</p>
                <ul>
                    <li>Reescribe kernels CUDA</li>
                    <li>Combina operaciones (kernel fusion)</li>
                    <li>Reduce transferencias de memoria</li>
                    <li>Mantiene precisi√≥n original</li>
                </ul>
            </section>

            <section>
                <h3>El Problema del Fine-tuning Tradicional</h3>
                <p><strong>Consumo de memoria (Llama 2 7B):</strong></p>
                <ul>
                    <li>Par√°metros: 28 GB</li>
                    <li>Gradientes: 28 GB</li>
                    <li>Optimizer states: 56 GB</li>
                    <li>Activaciones: 12 GB</li>
                </ul>
                <p style="margin-top: 1em;"><strong>Total: 132 GB ‚ùå</strong></p>
                <p>Una GPU A100 con 80GB NO es suficiente</p>
            </section>

            <section>
                <h3>Soluciones Tradicionales (Lentas)</h3>
                <ul>
                    <li><strong>Gradient Checkpointing:</strong> M√°s lento</li>
                    <li><strong>Entrenar en CPU:</strong> Muy lento</li>
                    <li><strong>Reducir Batch Size:</strong> Entrenamiento inestable</li>
                    <li><strong>Full Fine-tuning:</strong> Imposible sin GPU potente</li>
                </ul>
                <p style="margin-top: 1em;"><span class="warning">‚ö†Ô∏è</span> Problemas: Costo, tiempo, accesibilidad</p>
            </section>

            <section>
                <h3>C√≥mo Unsloth Resuelve el Problema (1/2)</h3>
                <p><strong>Kernel Fusion:</strong></p>
                <ul>
                    <li>Antes: Atenci√≥n + FFN = 2 kernels separados</li>
                    <li>Ahora: Fusionado en 1 kernel</li>
                    <li>Beneficio: Sin copias de memoria intermedias</li>
                </ul>
                <p style="margin-top: 1em;"><strong>Precisi√≥n Selectiva:</strong></p>
                <ul>
                    <li>Float16 donde es seguro</li>
                    <li>Float32 donde es cr√≠tico</li>
                </ul>
            </section>

            <section>
                <h3>C√≥mo Unsloth Resuelve el Problema (2/2)</h3>
                <p><strong>RoPE Vectorizado:</strong></p>
                <ul>
                    <li>Embeddings optimizados</li>
                    <li>Menos operaciones</li>
                </ul>
                <p style="margin-top: 1em;"><strong>Resultado Final:</strong></p>
                <ul>
                    <li><span class="success">‚úÖ 8GB memoria</span> (vs 132GB)</li>
                    <li><span class="success">‚úÖ 2x velocidad</span></li>
                    <li><span class="success">‚úÖ Misma calidad</span></li>
                </ul>
            </section>

            <section>
                <h3>Modelos Soportados</h3>
                <p><strong>Optimizados completamente:</strong></p>
                <ul>
                    <li>Llama 2, 3, 3.1 (7B, 13B, 70B)</li>
                    <li>Mistral, Mixtral</li>
                    <li>Gemma, Gemma 2</li>
                    <li>Qwen, Yi, Phi</li>
                </ul>
            </section>

            <section>
                <h3>Requisitos de Hardware</h3>
                <table>
                    <tr>
                        <td><strong>GPU</strong></td>
                        <td><strong>VRAM</strong></td>
                        <td><strong>Modelo Max</strong></td>
                    </tr>
                    <tr>
                        <td>T4 (Free Colab)</td>
                        <td>16 GB</td>
                        <td>Llama 7B</td>
                    </tr>
                    <tr>
                        <td>L4 (Pro Colab)</td>
                        <td>24 GB</td>
                        <td>Llama 13B</td>
                    </tr>
                    <tr>
                        <td>RTX 3080</td>
                        <td>10 GB</td>
                        <td>Llama 7B</td>
                    </tr>
                    <tr>
                        <td>A100</td>
                        <td>80 GB</td>
                        <td>Llama 70B</td>
                    </tr>
                </table>
            </section>

            <section>
                <h3>Instalaci√≥n: Google Colab (Paso 1)</h3>
                <ul>
                    <li>Ir a <strong>colab.research.google.com</strong></li>
                    <li>Crear nuevo notebook</li>
                    <li>Men√∫: <strong>Runtime ‚Üí Change runtime type</strong></li>
                    <li>Seleccionar: <strong>GPU (T4 gratis, L4 Pro)</strong></li>
                </ul>
            </section>

            <section>
                <h3>Instalaci√≥n: Google Colab (Paso 2)</h3>
                <p><strong>Instalar dependencias:</strong></p>
                <pre><code class="language-bash">!pip install unsloth[colab-new] \
  @ git+https://github.com/unslothai/unsloth.git
!pip install xformers datasets</code></pre>
            </section>

            <section>
                <h3>Instalaci√≥n: Google Colab (Paso 3)</h3>
                <p><strong>Verificar instalaci√≥n:</strong></p>
                <pre><code class="language-python">from unsloth import FastLanguageModel
import torch

print(f"PyTorch: {torch.__version__}")
print(f"CUDA: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name()}")</code></pre>
            </section>

            <section>
                <h3>Instalaci√≥n: Local (Linux/WSL) - Paso 1</h3>
                <pre><code class="language-bash"># Crear entorno
conda create --name unsloth python=3.10
conda activate unsloth

# Instalar PyTorch con CUDA
conda install pytorch pytorch-cuda=11.8 \
  -c pytorch -c nvidia</code></pre>
            </section>

            <section>
                <h3>Instalaci√≥n: Local (Linux/WSL) - Paso 2</h3>
                <pre><code class="language-bash"># Instalar Unsloth
pip install unsloth[colab]

# Instalar dependencias adicionales
pip install xformers datasets transformers \
  accelerate trl</code></pre>
            </section>

            <section>
                <h3>Primera Pr√°ctica: "Hola Mundo"</h3>
                <pre><code class="language-python">from unsloth import FastLanguageModel

modelo, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-2-7b-bnb-4bit",
    max_seq_length=2048,
    load_in_4bit=True,
)

print("‚úÖ Modelo cargado exitosamente")</code></pre>
            </section>

            <section>
                <h3>Generaci√≥n B√°sica</h3>
                <pre><code class="language-python">prompt = "La inteligencia artificial"

inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

outputs = modelo.generate(**inputs, max_new_tokens=50)

print(tokenizer.decode(outputs[0]))</code></pre>
            </section>

            <!-- M√ìDULO 2: CONCEPTOS -->
            <section>
                <h2>üìö M√≥dulo 2: Conceptos de Fine-Tuning</h2>
            </section>

            <section>
                <h3>Full Fine-tuning Explicado</h3>
                <p><strong>¬øQu√© es?</strong> Actualizar TODOS los par√°metros</p>
                <ul>
                    <li>7.0 Billones de par√°metros entrenable</li>
                    <li>Memoria: 21-40 GB</li>
                    <li>Velocidad: Lenta</li>
                </ul>
                <p style="margin-top: 1em;"><strong>¬øCu√°ndo usar?</strong></p>
                <ul>
                    <li>‚úÖ Datasets enormes (>10M ejemplos)</li>
                    <li>‚úÖ Presupuesto ilimitado</li>
                    <li>‚ùå No en Colab, no en laptop</li>
                </ul>
            </section>

            <section>
                <h3>PEFT: Parameter-Efficient Fine-Tuning</h3>
                <p><strong>Idea clave:</strong> Solo adapta 0.08% de par√°metros</p>
                <ul>
                    <li>Memoria: 4-8 GB ‚úÖ</li>
                    <li>Velocidad: 2x m√°s r√°pido ‚ö°</li>
                    <li>Calidad: 98-99% igual</li>
                </ul>
                <p style="margin-top: 1em;"><strong>¬øCu√°ndo usar?</strong></p>
                <ul>
                    <li>‚úÖ Colab T4/L4</li>
                    <li>‚úÖ GPU local</li>
                    <li>‚úÖ Datasets medianos</li>
                </ul>
            </section>

            <section>
                <h3>LoRA: Low-Rank Adaptation (1/2)</h3>
                <p><strong>Problema matem√°tico:</strong> Matriz W es grande</p>
                <pre><code class="language-python">W_full: 4096 √ó 4096 = 16M params
# Imposible entrenar todos</code></pre>
                <p style="margin-top: 1em;"><strong>Soluci√≥n: Factorizaci√≥n de bajo rango</strong></p>
                <pre><code class="language-python">W_nuevo = W_original + BA
B: 4096 √ó 64
A: 64 √ó 4096
Total: 524K params (30x menos!)</code></pre>
            </section>

            <section>
                <h3>LoRA: Low-Rank Adaptation (2/2)</h3>
                <p><strong>Visualizaci√≥n del concepto:</strong></p>
                <ul>
                    <li>W_original: Congelado, no se actualiza</li>
                    <li>B, A: Matrices peque√±as, S√ç se entrenan</li>
                    <li>La suma BA + W_original: Modelo adaptado</li>
                </ul>
                <p style="margin-top: 1em;"><strong>Ventaja magistral:</strong></p>
                <ul>
                    <li>30x menos par√°metros</li>
                    <li>Casi la misma capacidad</li>
                    <li>8GB memoria en lugar de 40GB</li>
                </ul>
            </section>

            <section>
                <h3>QLoRA: LoRA + Cuantizaci√≥n 4-bit</h3>
                <p><strong>LoRA:</strong> 6M par√°metros entrenable</p>
                <p><strong>4-bit:</strong> Cuantizar modelo a 4-bit (8x menos)</p>
                <p><strong>Resultado:</strong> Entrenar modelos gigantes en GPU chica</p>
                <p style="margin-top: 1em;"><strong>Tradeoff:</strong></p>
                <ul>
                    <li>LoRA: 100% calidad</li>
                    <li>QLoRA: 98% calidad, 8x menos memoria</li>
                </ul>
            </section>

            <section>
                <h3>Comparativa: Full vs LoRA vs QLoRA</h3>
                <table>
                    <tr>
                        <td><strong>M√©todo</strong></td>
                        <td><strong>Memoria</strong></td>
                        <td><strong>Velocidad</strong></td>
                        <td><strong>Calidad</strong></td>
                    </tr>
                    <tr>
                        <td>Full</td>
                        <td>40+ GB</td>
                        <td>Lenta ‚ö†Ô∏è</td>
                        <td>100%</td>
                    </tr>
                    <tr>
                        <td>LoRA</td>
                        <td>24 GB</td>
                        <td>2x üöÄ</td>
                        <td>99%</td>
                    </tr>
                    <tr>
                        <td>QLoRA</td>
                        <td>8 GB</td>
                        <td>3x üöÄ</td>
                        <td>98%</td>
                    </tr>
                </table>
            </section>

            <section>
                <h3>Hiperpar√°metro r: Rango LoRA</h3>
                <p><strong>r controla el "tama√±o" de los adaptadores</strong></p>
                <ul>
                    <li><code>r=8</code>: Muy bajo (1M params)</li>
                    <li><code>r=16</code>: Bajo (2M params)</li>
                    <li><code>r=32</code>: Medio (4M params) ‚≠ê Recomendado</li>
                    <li><code>r=64</code>: Alto (8M params)</li>
                    <li><code>r=128</code>: Muy alto (16M params)</li>
                </ul>
                <p style="margin-top: 1em;"><strong>Regla pr√°ctica:</strong> r=32 funciona para casi todos</p>
            </section>

            <section>
                <h3>Hiperpar√°metro lora_alpha</h3>
                <p><strong>¬øQu√© hace?</strong> Controla el "peso" del adaptador</p>
                <pre><code class="language-python">output = W @ input + (lora_alpha/r) * BA @ input
         ^original         ^factor de escala</code></pre>
                <p style="margin-top: 1em;"><strong>Regla de oro:</strong> lora_alpha = r √ó 2</p>
                <ul>
                    <li>r=32 ‚Üí lora_alpha=64</li>
                    <li>r=16 ‚Üí lora_alpha=32</li>
                    <li>r=64 ‚Üí lora_alpha=128</li>
                </ul>
            </section>

            <section>
                <h3>Hiperpar√°metro lora_dropout</h3>
                <p><strong>Regularizaci√≥n:</strong> Evitar overfitting</p>
                <ul>
                    <li><code>0.0</code>: Sin dropout (riesgo overfitting)</li>
                    <li><code>0.05</code>: Recomendado ‚≠ê</li>
                    <li><code>0.1</code>: Para datasets peque√±os</li>
                    <li><code>0.2</code>: Muy agresivo</li>
                </ul>
                <p style="margin-top: 1em;"><strong>Efecto:</strong> M√°s dropout = menos overfitting pero convergencia m√°s lenta</p>
            </section>

            <section>
                <h3>Hiperpar√°metro target_modules</h3>
                <p><strong>¬øD√≥nde aplicar LoRA?</strong> Solo en ciertas capas</p>
                <ul>
                    <li><code>["q_proj", "v_proj"]</code>: Balance √≥ptimo ‚≠ê</li>
                    <li><code>["q_proj"]</code>: M√≠nimo, muy r√°pido</li>
                    <li><code>["q", "v", "k", "o", "up", "down"]</code>: M√°ximo, lento</li>
                </ul>
            </section>

            <section>
                <h3>Preparaci√≥n de Datos: Formato Alpaca</h3>
                <p><strong>Perfecto para:</strong> Q&A, clasificaci√≥n</p>
                <pre><code class="language-json">{
  "instruction": "¬øCu√°l es la capital de Espa√±a?",
  "input": "",
  "output": "Madrid"
}</code></pre>
            </section>

            <section>
                <h3>Preparaci√≥n de Datos: Formato ShareGPT</h3>
                <p><strong>Perfecto para:</strong> Conversaciones, chat</p>
                <pre><code class="language-json">{
  "conversations": [
    {"from": "human", "value": "Hola, ¬øc√≥mo est√°s?"},
    {"from": "gpt", "value": "¬°Muy bien, gracias!"},
    {"from": "human", "value": "¬øQu√© puedes hacer?"},
    {"from": "gpt", "value": "Puedo ayudarte con..."}
  ]
}</code></pre>
            </section>

            <section>
                <h3>Chat Templates: Por qu√© Importan</h3>
                <p><strong>SIN plantilla (MALO):</strong></p>
                <pre><code>"Usuario: Hola. Asistente: Hola, ¬øc√≥mo est√°s?"</code></pre>
                <p style="margin-top: 1em;"><strong>CON plantilla (BUENO):</strong></p>
                <pre><code>"[INST] Hola [/INST] Hola, ¬øc√≥mo est√°s?"</code></pre>
                <p style="margin-top: 1em;"><strong>Beneficio:</strong> Tokens especiales marcan roles claramente</p>
            </section>

            <!-- M√ìDULO 3: FINE-TUNING -->
            <section>
                <h2>üìö M√≥dulo 3: Fine-Tuning Completo</h2>
            </section>

            <section>
                <h3>Paso 1: Cargar Modelo Base</h3>
                <pre><code class="language-python">from unsloth import FastLanguageModel

modelo, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-2-7b-bnb-4bit",
    max_seq_length=2048,
    load_in_4bit=True,
)

print(f"Modelo: {modelo.config.model_type}")
print(f"Par√°metros: {sum(p.numel() for p in modelo.parameters())/1e9:.1f}B")</code></pre>
            </section>

            <section>
                <h3>Paso 2: Aplicar LoRA</h3>
                <pre><code class="language-python">modelo = FastLanguageModel.get_peft_model(
    modelo,
    r=32,
    lora_alpha=64,
    lora_dropout=0.05,
    target_modules=["q_proj", "v_proj"],
    bias="none",
    use_gradient_checkpointing=True,
)

modelo.print_trainable_parameters()
# trainable: 4M || total: 7.1B || 0.05%</code></pre>
            </section>

            <section>
                <h3>Paso 3a: TrainingArguments - Rutas</h3>
                <pre><code class="language-python">training_args = TrainingArguments(
    output_dir="./llama2_finetuned",
    save_strategy="steps",
    save_steps=100,
    save_total_limit=3,
    eval_strategy="steps",
    eval_steps=50,
)</code></pre>
            </section>

            <section>
                <h3>Paso 3b: TrainingArguments - Learning Rate</h3>
                <pre><code class="language-python">training_args = TrainingArguments(
    learning_rate=5e-4,
    lr_scheduler_type="cosine",
    warmup_steps=50,
    num_train_epochs=3,
)</code></pre>
                <p style="margin-top: 1em;"><strong>Recomendaciones:</strong></p>
                <ul>
                    <li>LoRA: 5e-4 a 1e-4</li>
                    <li>Full Fine-tuning: 2e-5 a 5e-5</li>
                </ul>
            </section>

            <section>
                <h3>Paso 3c: TrainingArguments - Batch</h3>
                <pre><code class="language-python">training_args = TrainingArguments(
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=4,
    # Batch efectivo = 4 √ó 4 = 16
)</code></pre>
                <p style="margin-top: 1em;"><strong>C√°lculo:</strong> batch_efectivo = per_device √ó accumulation</p>
            </section>

            <section>
                <h3>Paso 3d: TrainingArguments - Hardware</h3>
                <pre><code class="language-python">training_args = TrainingArguments(
    optim="adamw_8bit",
    fp16=True,
    # Usar float16 para GPU + quantized optimizer
    weight_decay=0.01,
    max_grad_norm=1.0,
)</code></pre>
            </section>

            <section>
                <h3>Learning Rate Scheduler Explicado</h3>
                <p><strong>LR muy alto:</strong> [2.5, 2.4, 5.3, NaN] ‚ùå Inestable</p>
                <p><strong>LR correcto:</strong> [2.5, 2.4, 2.2, 1.9, 1.7] ‚úÖ Convergencia</p>
                <p><strong>LR muy bajo:</strong> [2.5, 2.499, 2.498] ‚ö†Ô∏è Lento</p>
                <p style="margin-top: 1em;"><strong>Cosine Scheduler (Recomendado):</strong></p>
                <ul>
                    <li>Warmup inicial: 0 ‚Üí LR</li>
                    <li>Decay final: LR ‚Üí 0</li>
                    <li>Convergencia suave y estable</li>
                </ul>
            </section>

            <section>
                <h3>Batch Size y Accumulation Detallado</h3>
                <p><strong>Batch Size peque√±o (1-2):</strong></p>
                <ul>
                    <li>Ruido alto, inestable, muy lento</li>
                </ul>
                <p style="margin-top: 1em;"><strong>Batch Size medio (4-16):</strong></p>
                <ul>
                    <li>Balance √≥ptimo, estable, r√°pido</li>
                </ul>
                <p style="margin-top: 1em;"><strong>Batch Size grande (32+):</strong></p>
                <ul>
                    <li>R√°pido pero OOM en GPU peque√±a</li>
                </ul>
            </section>

            <section>
                <h3>Gradient Accumulation Ejemplo</h3>
                <p><strong>Sin accumulation:</strong> GPU memory overflow ‚ùå</p>
                <p><strong>Con accumulation:</strong></p>
                <ul>
                    <li>Paso 1-4: Forward + Backward (acumular gradientes)</li>
                    <li>Paso 4: Actualizar pesos (solo ahora!)</li>
                    <li>Efecto: Batch de 16 en memoria de 4 ‚úÖ</li>
                </ul>
            </section>

            <section>
                <h3>AdamW 8-bit vs Adam</h3>
                <p><strong>Adam (Normal):</strong></p>
                <ul>
                    <li>Mantiene 2 estados por par√°metro (momentum + variance)</li>
                    <li>7B modelo ‚Üí 14B memoria solo en estados ‚ùå</li>
                </ul>
                <p style="margin-top: 1em;"><strong>AdamW 8-bit (Cuantizado):</strong></p>
                <ul>
                    <li>Mismos estados pero en 8-bit</li>
                    <li>7B modelo ‚Üí 1.75B memoria ‚úÖ 8x menos</li>
                    <li>Calidad casi id√©ntica</li>
                </ul>
            </section>

            <section>
                <h3>Paso 4: Crear SFTTrainer</h3>
                <pre><code class="language-python">from trl import SFTTrainer

trainer = SFTTrainer(
    model=modelo,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    dataset_text_field="output",
    max_seq_length=2048,
)</code></pre>
            </section>

            <section>
                <h3>Paso 5: Entrenar</h3>
                <pre><code class="language-python">print("üöÄ Iniciando entrenamiento...")
trainer.train()

print("‚úÖ Entrenamiento completado!")
print(f"Modelo guardado en: {training_args.output_dir}")</code></pre>
            </section>

            <section>
                <h3>Interpretar Loss Durante Entrenamiento</h3>
                <ul>
                    <li><strong>Loss disminuye:</strong> ‚úÖ Entrenamiento funciona</li>
                    <li><strong>Val loss tambi√©n disminuye:</strong> ‚úÖ Generalizando bien</li>
                    <li><strong>Train loss baja, val sube:</strong> ‚ö†Ô∏è Overfitting detectado</li>
                    <li><strong>Loss no cambia:</strong> ‚ùå LR muy bajo</li>
                    <li><strong>Loss explota (NaN):</strong> ‚ùå LR muy alto</li>
                </ul>
            </section>

            <section>
                <h3>Monitoring con Weights & Biases</h3>
                <pre><code class="language-python">import wandb
wandb.login()

training_args = TrainingArguments(
    ...,
    report_to=["wandb"],
    run_name="llama2-v1",
)

# W&B crear√° dashboard autom√°ticamente
trainer.train()</code></pre>
                <p style="margin-top: 1em;"><strong>Ventajas:</strong> Gr√°ficas, comparaciones, historial</p>
            </section>

            <!-- M√ìDULO 4: INFERENCIA -->
            <section>
                <h2>üìö M√≥dulo 4: Inferencia y Evaluaci√≥n</h2>
            </section>

            <section>
                <h3>Cargar Modelo Entrenado</h3>
                <pre><code class="language-python">modelo, tokenizer = FastLanguageModel.from_pretrained(
    model_name="./mi_modelo_finetuned",
    max_seq_length=2048,
    load_in_4bit=True,
)</code></pre>
            </section>

            <section>
                <h3>Modo Inferencia (Cr√≠tico!)</h3>
                <pre><code class="language-python"># SIN for_inference:
# - C√°lculo de gradientes: ACTIVO
# - Memoria: 8GB
# - Velocidad: Lenta

modelo = FastLanguageModel.for_inference(modelo)

# CON for_inference:
# - C√°lculo de gradientes: INACTIVO
# - Memoria: 3GB
# - Velocidad: 2-3x m√°s r√°pido</code></pre>
            </section>

            <section>
                <h3>Generaci√≥n B√°sica (Paso a Paso)</h3>
                <ul>
                    <li><strong>Paso 1:</strong> Tokenizar input</li>
                    <li><strong>Paso 2:</strong> Pasar a GPU</li>
                    <li><strong>Paso 3:</strong> Llamar a generate()</li>
                    <li><strong>Paso 4:</strong> Decodificar output</li>
                </ul>
            </section>

            <section>
                <h3>Generaci√≥n: C√≥digo Completo</h3>
                <pre><code class="language-python">prompt = "La IA es"

inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

outputs = modelo.generate(
    **inputs,
    max_new_tokens=50,
    temperature=0.7,
    top_p=0.9,
)

respuesta = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(respuesta)</code></pre>
            </section>

            <section>
                <h3>Par√°metros: max_new_tokens</h3>
                <p><strong>¬øQu√© controla?</strong> Cu√°ntos tokens generar</p>
                <ul>
                    <li>1-50: Respuestas cortas</li>
                    <li>50-200: Respuestas medianas ‚≠ê</li>
                    <li>200-1000: Respuestas largas</li>
                </ul>
                <p style="margin-top: 1em;"><strong>Trade-off:</strong> M√°s tokens = m√°s tiempo</p>
            </section>

            <section>
                <h3>Par√°metros: temperature</h3>
                <p><strong>Controla:</strong> Aleatoriedad/creatividad</p>
                <ul>
                    <li>0.0: Determin√≠stico (siempre igual)</li>
                    <li>0.5: Bajo aleatoriedad (conservador)</li>
                    <li>0.7: Medio (balanceado) ‚≠ê</li>
                    <li>1.0: Alto aleatoriedad (creativo)</li>
                    <li>>1.0: Muy aleatorio (experimental)</li>
                </ul>
            </section>

            <section>
                <h3>Par√°metros: top_p (Nucleus Sampling)</h3>
                <p><strong>Controla:</strong> Diversidad de opciones</p>
                <ul>
                    <li>0.5: Solo tokens muy seguros</li>
                    <li>0.9: Considera 90% de probabilidad ‚≠ê</li>
                    <li>1.0: Todas las opciones</li>
                </ul>
            </section>

            <section>
                <h3>Configuraciones Recomendadas (1/2)</h3>
                <p><strong>Q&A Consistente:</strong></p>
                <pre><code class="language-python">config = {
    "max_new_tokens": 100,
    "temperature": 0.3,
    "top_p": 0.95,
    "do_sample": False,  # Determin√≠stico
}</code></pre>
            </section>

            <section>
                <h3>Configuraciones Recomendadas (2/2)</h3>
                <p><strong>Chat Balanceado:</strong></p>
                <pre><code class="language-python">config = {
    "max_new_tokens": 200,
    "temperature": 0.7,
    "top_p": 0.9,
    "do_sample": True,
}</code></pre>
                <p style="margin-top: 1em;"><strong>Escritura Creativa:</strong></p>
                <pre><code class="language-python">config = {
    "max_new_tokens": 500,
    "temperature": 0.9,
    "top_p": 0.95,
}</code></pre>
            </section>

            <section>
                <h3>Streaming de Respuestas</h3>
                <p><strong>¬øPor qu√©?</strong> Ver respuesta car√°cter por car√°cter (como ChatGPT)</p>
                <p><strong>Uso:</strong> Interfaces web, mejor UX</p>
            </section>

            <section>
                <h3>Implementar Streaming</h3>
                <pre><code class="language-python">from transformers import TextIteratorStreamer
from threading import Thread

streamer = TextIteratorStreamer(
    tokenizer,
    skip_special_tokens=True
)

thread = Thread(target=modelo.generate, kwargs={
    **inputs,
    streamer=streamer,
    max_new_tokens=200,
})
thread.start()

for text in streamer:
    print(text, end="", flush=True)

thread.join()</code></pre>
            </section>

            <section>
                <h3>M√©tricas: Perplexity</h3>
                <p><strong>¬øQu√© mide?</strong> Cu√°n "sorprendido" est√° el modelo</p>
                <ul>
                    <li>Perplexity 10: Muy seguro ‚úÖ</li>
                    <li>Perplexity 50: Algo seguro</li>
                    <li>Perplexity 100: Inseguro ‚ùå</li>
                </ul>
                <p style="margin-top: 1em;"><strong>Benchmark:</strong> GPT-2=20, BERT=76</p>
            </section>

            <section>
                <h3>M√©tricas: BLEU y ROUGE</h3>
                <p><strong>BLEU:</strong> Similitud exacta con referencia</p>
                <p><strong>ROUGE:</strong> Similitud con tokens solapados</p>
                <p style="margin-top: 1em;"><strong>Interpretaci√≥n:</strong></p>
                <ul>
                    <li>0-0.3: Malo ‚ùå</li>
                    <li>0.3-0.5: Aceptable ‚ö†Ô∏è</li>
                    <li>0.5-0.7: Bueno ‚úÖ</li>
                    <li>0.7+: Excelente ‚úÖ‚úÖ</li>
                </ul>
            </section>

            <section>
                <h3>LLM-as-a-Judge (1/2)</h3>
                <p><strong>Idea:</strong> Usar LLM m√°s poderoso para evaluar</p>
                <p><strong>Ventaja:</strong> M√°s realista que BLEU/ROUGE</p>
            </section>

            <section>
                <h3>LLM-as-a-Judge (2/2)</h3>
                <pre><code class="language-python">criterios = """
Eval√∫a 1-10:
- Precisi√≥n: ¬øfactualmente correcto?
- Claridad: ¬øf√°cil entender?
- Completitud: ¬øresponde todo?

Prompt: {prompt}
Respuesta: {respuesta}

Puntuaci√≥n:"""</code></pre>
            </section>

            <!-- M√ìDULO 5: EXPORTACI√ìN -->
            <section>
                <h2>üìö M√≥dulo 5: Exportaci√≥n y Despliegue</h2>
            </section>

            <section>
                <h3>Guardar LoRA (Peque√±o)</h3>
                <pre><code class="language-python">modelo.save_pretrained("./mi_modelo")
tokenizer.save_pretrained("./mi_modelo")

# Resultado: 4-50 MB ‚úÖ PEQUE√ëO!</code></pre>
                <p style="margin-top: 1em;"><strong>Ventajas:</strong> Compartible, r√°pido</p>
                <p><strong>Desventaja:</strong> Necesita modelo base original</p>
            </section>

            <section>
                <h3>Subir a Hugging Face Hub</h3>
                <pre><code class="language-python">from huggingface_hub import login

login()  # Pedir token

modelo.push_to_hub("usuario/llama2-finetuned")
tokenizer.push_to_hub("usuario/llama2-finetuned")</code></pre>
            </section>

            <section>
                <h3>Fusionar LoRA (Independiente)</h3>
                <pre><code class="language-python">modelo = modelo.merge_and_unload()

modelo.save_pretrained("./modelo_merged")
tokenizer.save_pretrained("./modelo_merged")

# Resultado: 14 GB ‚ùå GRANDE
# Ventaja: Completamente independiente</code></pre>
            </section>

            <section>
                <h3>Diferentes Formatos: Float Precision</h3>
                <table>
                    <tr>
                        <td><strong>Formato</strong></td>
                        <td><strong>Tama√±o</strong></td>
                        <td><strong>Velocidad</strong></td>
                    </tr>
                    <tr>
                        <td>Float32</td>
                        <td>14 GB</td>
                        <td>Lenta</td>
                    </tr>
                    <tr>
                        <td>Float16 ‚≠ê</td>
                        <td>7 GB</td>
                        <td>R√°pida</td>
                    </tr>
                    <tr>
                        <td>BFloat16</td>
                        <td>7 GB</td>
                        <td>Mejor</td>
                    </tr>
                    <tr>
                        <td>Int8</td>
                        <td>3.5 GB</td>
                        <td>Muy r√°pida</td>
                    </tr>
                </table>
            </section>

            <section>
                <h3>¬øQu√© es GGUF? (1/2)</h3>
                <p><strong>GGUF = GPT-Generated Unified Format</strong></p>
                <ul>
                    <li>Optimizado para CPU (no GPU)</li>
                    <li>Cuantizaci√≥n flexible</li>
                    <li>Portable (Windows, Mac, Linux)</li>
                </ul>
            </section>

            <section>
                <h3>¬øQu√© es GGUF? (2/2)</h3>
                <p><strong>Comparativa:</strong></p>
                <ul>
                    <li>HF .safetensors: 14GB GPU</li>
                    <li>GGUF q4_k_m: 3-4GB CPU ‚Üê 4x eficiente</li>
                </ul>
                <p style="margin-top: 1em;"><strong>Caso de uso:</strong> Ejecutar en laptop sin GPU</p>
            </section>

            <section>
                <h3>Convertir a GGUF</h3>
                <pre><code class="language-python">modelo = modelo.merge_and_unload()

modelo.save_pretrained_gguf(
    "modelo_gguf",
    quantization_method="q4_k_m"
)

# Resultado: modelo_gguf/model.gguf (3-4GB)</code></pre>
            </section>

            <section>
                <h3>M√©todos de Cuantizaci√≥n GGUF</h3>
                <table>
                    <tr>
                        <td><strong>M√©todo</strong></td>
                        <td><strong>Tama√±o</strong></td>
                        <td><strong>Calidad</strong></td>
                    </tr>
                    <tr>
                        <td>Q8_0</td>
                        <td>5.5 GB</td>
                        <td>Excelente</td>
                    </tr>
                    <tr>
                        <td>Q5_K_M</td>
                        <td>4.3 GB</td>
                        <td>Excelente</td>
                    </tr>
                    <tr>
                        <td>Q4_K_M ‚≠ê</td>
                        <td>3.5 GB</td>
                        <td>Buena</td>
                    </tr>
                    <tr>
                        <td>Q3_K_M</td>
                        <td>2.6 GB</td>
                        <td>Aceptable</td>
                    </tr>
                </table>
            </section>

            <section>
                <h3>Recomendaciones por Hardware</h3>
                <ul>
                    <li><strong>Laptop 8GB:</strong> q4_k_m</li>
                    <li><strong>Desktop 16GB:</strong> q5_k_m</li>
                    <li><strong>M√°xima calidad:</strong> q8_0</li>
                    <li><strong>M√°xima velocidad:</strong> q3_k_m</li>
                </ul>
            </section>

            <section>
                <h3>Ejecutar con Ollama (1/2)</h3>
                <p><strong>Paso 1:</strong> Descargar Ollama desde ollama.ai</p>
                <p><strong>Paso 2:</strong> Instalar</p>
                <p><strong>Paso 3:</strong> Crear Modelfile</p>
            </section>

            <section>
                <h3>Ejecutar con Ollama (2/2)</h3>
                <pre><code class="language-bash">cat > Modelfile <<EOF
FROM ./modelo_gguf/model.gguf
PARAMETER temperature 0.7
PARAMETER top_p 0.9
EOF

ollama create mi-modelo -f Modelfile
ollama run mi-modelo "¬øQu√© es IA?"</code></pre>
            
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
    <script>
        Reveal.initialize({
            hash: true,
            slideNumber: true,
            transition: 'slide',
            keyboard: true,
            center: false,
            margin: 0.1
        });

        // Highlight code blocks after reveal is ready
        Reveal.on('ready', function() {
            document.querySelectorAll('pre code').forEach(function(el) {
                hljs.highlightElement(el);
            });
        });
    </script>
</body>
</html>
