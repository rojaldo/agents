# ğŸš€ QuickStart - Comienza en 5 minutos

## â±ï¸ Pasos RÃ¡pidos

### Paso 1: Preparar Ollama (5 minutos)

```bash
# En MACOS/WINDOWS: Descarga desde https://ollama.ai y ejecuta

# En LINUX:
curl https://ollama.ai/install.sh | sh
ollama serve  # En terminal 1
```

### Paso 2: Descargar Modelo (3-5 minutos)

```bash
# En terminal 2
ollama pull mistral
# O mÃ¡s rÃ¡pido:
ollama pull neural-chat
```

### Paso 3: Verificar InstalaciÃ³n (1 minuto)

```bash
# En terminal 3
cd ejemplos/langchain
python test_imports.py   # DeberÃ­a mostrar âœ…
python test_syntax.py    # DeberÃ­a mostrar âœ…
```

### Paso 4: Ejecutar tu Primer Ejemplo (2-5 minutos)

```bash
python 01_basic_llm.py
```

Â¡Listo! DeberÃ­as ver respuestas del LLM.

## ğŸ“– Orden Recomendado para Aprender

```
1ï¸âƒ£  Ejecuta 01_basic_llm.py
    Aprende: LLMs, prompts, parsers, LCEL

2ï¸âƒ£  Lee MÃ³dulo 2 en docs/langchain.adoc
    (5 minutos)

3ï¸âƒ£  Ejecuta 02_chains_basics.py
    Aprende: Cadenas, composiciÃ³n, streaming

4ï¸âƒ£  Lee MÃ³dulo 3-4 en docs/langchain.adoc
    (10 minutos)

5ï¸âƒ£  Ejecuta 03_memory.py
    Aprende: Memoria conversacional

6ï¸âƒ£  Lee MÃ³dulo 5-6 en docs/langchain.adoc
    (10 minutos)

7ï¸âƒ£  Ejecuta 04_agents.py
    Aprende: Agentes autÃ³nomos y herramientas

8ï¸âƒ£  Lee MÃ³dulo 7-8 en docs/langchain.adoc
    (10 minutos)

9ï¸âƒ£  Ejecuta 05_embeddings_vectorstore.py
    Aprende: Embeddings y bÃºsqueda semÃ¡ntica

ğŸ”Ÿ  Ejecuta 06_rag_system.py
    Aprende: Sistema RAG completo
```

**Tiempo total estimado: 1.5-2 horas para entender todo**

## ğŸ†˜ Troubleshooting RÃ¡pido

| Problema | SoluciÃ³n |
|----------|----------|
| "No puedo conectar a Ollama" | Ejecuta `ollama serve` en otra terminal |
| "ModuleNotFoundError: langchain" | Ejecuta `python test_imports.py` |
| "Respuesta muy lenta" | Usa modelo mÃ¡s rÃ¡pido: `ollama pull neural-chat` |
| "Modelo no encontrado" | Descarga: `ollama pull mistral` |
| "CÃ³digo no funciona" | Ejecuta `python test_syntax.py` para verificar |

## ğŸ’¡ Ejemplos Clave en Cada Archivo

### 01_basic_llm.py
```
âœ“ Crear LLM con Ollama
âœ“ Prompts simples y con variables
âœ“ Output parsers
âœ“ ComposiciÃ³n LCEL
```

### 02_chains_basics.py
```
âœ“ Cadenas: prompt â†’ LLM â†’ parser
âœ“ RunnableLambda: funciones personalizadas
âœ“ RunnableParallel: procesamiento paralelo
âœ“ Stream: respuestas en tiempo real
âœ“ LÃ³gica condicional
```

### 03_memory.py
```
âœ“ Memoria simple
âœ“ Memoria de ventana
âœ“ Chatbot interactivo (descomenta para probar)
âœ“ Memoria personalizada
```

### 04_agents.py
```
âœ“ Herramientas simples (@tool)
âœ“ Herramientas personalizadas
âœ“ Agente ReAct
âœ“ Razonamiento del agente (verbose)
```

### 05_embeddings_vectorstore.py
```
âœ“ Generar embeddings
âœ“ Vector store (FAISS)
âœ“ BÃºsqueda por similitud
âœ“ Guardando/cargando vector stores
```

### 06_rag_system.py
```
âœ“ RAG simple
âœ“ RAG mejorado
âœ“ Filtrado por relevancia
âœ“ RAG iterativo
```

## ğŸ“š Recursos RÃ¡pidos

- **DocumentaciÃ³n oficial**: https://docs.langchain.com
- **Ollama**: https://ollama.ai
- **Temario completo**: Ver `docs/langchain.adoc`
- **GuÃ­a de instalaciÃ³n**: Ver `SETUP.md`
- **Preguntas frecuentes**: Ver `README.md`

## ğŸ¯ PrÃ³ximos Pasos DespuÃ©s de Aprender

1. **Personaliza los ejemplos** para tu caso de uso
2. **Combina mÃ³dulos**: Usa agentes con memoria
3. **Integra con tu aplicaciÃ³n**: FastAPI, Flask, etc.
4. **Cambia modelos**: Experimenta con neural-chat, llama2, etc.
5. **Explora integraciones**: Wikipedia, APIs externas, etc.

## âš¡ Cheat Sheet

```python
# LLM bÃ¡sico
from langchain_community.llms import Ollama
llm = Ollama(model="mistral")
respuesta = llm.invoke("Tu pregunta")

# Cadena simple
from langchain_core.prompts import PromptTemplate
prompt = PromptTemplate.from_template("Responde: {pregunta}")
cadena = prompt | llm
resultado = cadena.invoke({"pregunta": "Â¿QuÃ© es Python?"})

# Memoria
from langchain.memory import ConversationBufferMemory
memoria = ConversationBufferMemory()
memoria.save_context(
    {"input": "Hola"},
    {"output": "Â¿QuÃ© tal?"}
)

# Agente
from langchain.agents import create_react_agent, AgentExecutor
agente = create_react_agent(llm, herramientas, prompt)
executor = AgentExecutor(agent=agente, tools=herramientas)
resultado = executor.invoke({"input": "Tu pregunta"})

# Vector store
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings
embeddings = OllamaEmbeddings(model="mistral")
vector_store = FAISS.from_documents(docs, embeddings)
resultados = vector_store.similarity_search("Tu pregunta")
```

## âœ¨ Tips Pro

1. **Usa streaming para UI**: `cadena.stream({"pregunta": "..."})`
2. **Batch para mÃºltiples inputs**: `cadena.batch([...])`
3. **Verbose para debugging**: `executor = AgentExecutor(..., verbose=True)`
4. **Modelos pequeÃ±os para desarrollo rÃ¡pido**: `neural-chat` es 10x mÃ¡s rÃ¡pido
5. **Caching automÃ¡tico**: LangChain cachea respuestas por defecto

---

**Â¿Preguntas?** Consulta `README.md` o `SETUP.md`
