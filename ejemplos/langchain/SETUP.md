# Gu√≠a de Configuraci√≥n e Instalaci√≥n

## Requisitos del Sistema

- **Python**: 3.10 o superior
- **Ollama**: √öltima versi√≥n
- **RAM**: M√≠nimo 4GB (recomendado 8GB)
- **Espacio en disco**: M√≠nimo 10GB para modelos

## 1. Instalar Ollama

### Windows/macOS
Descarga desde https://ollama.ai e instala el ejecutable.

### Linux (Arch)
```bash
sudo pacman -S ollama
```

### Linux (otras distribuciones)
```bash
curl https://ollama.ai/install.sh | sh
```

## 2. Descargar Modelos

Abre una terminal y ejecuta:

```bash
# Modelo recomendado (4.1GB, buena relaci√≥n velocidad/calidad)
ollama pull mistral

# Alternativas m√°s r√°pidas
ollama pull neural-chat      # 3.8GB, muy r√°pido
ollama pull openchat         # 3.5GB, ideal para producci√≥n

# Listar modelos disponibles
ollama list
```

## 3. Iniciar Ollama

```bash
# En macOS y Windows, la aplicaci√≥n gr√°fica lo inicia autom√°ticamente

# En Linux (en una terminal dedicada):
ollama serve

# Ollama estar√° disponible en http://localhost:11434
```

## 4. Instalar Dependencias de Python

```bash
# Opci√≥n 1: Con pip directamente (requiere permisos)
pip install langchain langchain-community langchain-core pydantic --break-system-packages

# Opci√≥n 2: Con entorno virtual (recomendado)
python3 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
pip install langchain langchain-community langchain-core pydantic
```

## 5. Verificar la Instalaci√≥n

```bash
# Verificar dependencias
python test_imports.py

# Verificar sintaxis de ejemplos
python test_syntax.py

# Ejecutar un ejemplo simple
python 01_basic_llm.py
```

## Soluci√≥n de Problemas

### "Error: ModuleNotFoundError"
```bash
# Aseg√∫rate de que las dependencias est√°n instaladas
python test_imports.py

# Si faltan, instala:
pip install langchain langchain-community langchain-core pydantic --break-system-packages
```

### "Error: No puedo conectar a http://localhost:11434"
```bash
# Verifica que Ollama est√° ejecut√°ndose
curl http://localhost:11434/api/tags

# Si no funciona, inicia Ollama en otra terminal
ollama serve
```

### "La respuesta es muy lenta"
- Usa un modelo m√°s peque√±o: `ollama pull neural-chat`
- Aseg√∫rate de tener suficiente RAM disponible
- Los primeros ejemplos son lentos porque cargan el modelo por primera vez

### "El modelo no existe"
```bash
# Descarga el modelo
ollama pull mistral

# O elige otro disponible
ollama pull neural-chat
ollama pull llama2
ollama pull openchat
```

## Comando R√°pido para Empezar

```bash
# 1. En terminal 1, inicia Ollama
ollama serve

# 2. En terminal 2, descarga el modelo
ollama pull mistral

# 3. En terminal 3, ejecuta un ejemplo
cd /home/rojaldo/cursos/agents/ejemplos/langchain
python 01_basic_llm.py
```

## Variables de Entorno (Opcional)

Crea un archivo `.env` en el directorio de ejemplos:

```env
# Configuraci√≥n de Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=mistral

# Otros modelos disponibles
# OLLAMA_MODEL=neural-chat
# OLLAMA_MODEL=llama2
# OLLAMA_MODEL=openchat
```

## Estructura Recomendada para Desarrollo

```
mi_proyecto_langchain/
‚îú‚îÄ‚îÄ venv/                    # Entorno virtual (opcional)
‚îú‚îÄ‚îÄ ejemplos/
‚îÇ   ‚îî‚îÄ‚îÄ langchain/          # Este directorio
‚îú‚îÄ‚îÄ datos/
‚îÇ   ‚îî‚îÄ‚îÄ documentos/         # Tus documentos para RAG
‚îî‚îÄ‚îÄ main.py                 # Tu aplicaci√≥n
```

## Pr√≥ximos Pasos

1. ‚úÖ Instala Ollama y descarga un modelo
2. ‚úÖ Instala las dependencias de Python
3. üöÄ Ejecuta los ejemplos en orden:
   - `python 01_basic_llm.py`
   - `python 02_chains_basics.py`
   - `python 03_memory.py`
   - `python 04_agents.py`
   - `python 05_embeddings_vectorstore.py`
   - `python 06_rag_system.py`

## Recursos √ötiles

- [Documentaci√≥n de LangChain](https://docs.langchain.com)
- [Documentaci√≥n de Ollama](https://github.com/jmorganca/ollama)
- [Temario completo](../../docs/langchain.adoc)
