# GuÃ­a DidÃ¡ctica: EvaluaciÃ³n y Testing de Agentes IA

## ğŸ“š IntroducciÃ³n

La evaluaciÃ³n rigurosa de agentes es crÃ­tica para garantizar que funcionen correctamente, de forma confiable y segura. Esta guÃ­a cubre los conceptos fundamentales y proporciona ejemplos prÃ¡cticos usando LangChain y Ollama.

## ğŸ¯ Objetivos de Aprendizaje

DespuÃ©s de completar esta guÃ­a, podrÃ¡s:

- Definir mÃ©tricas apropiadas para evaluar agentes
- Crear benchmarks de evaluaciÃ³n con anotaciÃ³n manual
- Escribir tests a mÃºltiples niveles (unit, integration, functional, stress)
- Debuggear comportamientos inesperados
- Usar LLMs para evaluaciÃ³n automÃ¡tica

## ğŸ“– Estructura

```
01_metricas_desempeno.py      â†’ MÃ©tricas de efectividad, eficiencia, robustez
02_benchmarks_datasets.py     â†’ Crear y gestionar benchmarks
03_testing_agentes.py         â†’ Unit, integration, functional, stress tests
04_testing_comportamiento.py  â†’ Testing de propiedades e invariantes
05_debugging_agentes.py       â†’ Logging, profiling, reproducciÃ³n de ejecuciones
06_llm_como_juez.py          â†’ LLMs como evaluadores automÃ¡ticos
```

---

## 1ï¸âƒ£ MÃ³dulo 1: MÃ©tricas de DesempeÃ±o

### Concepto

Las mÃ©tricas nos permiten **medir objetivamente** quÃ© tan bien funciona un agente.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MÃ‰TRICAS DE AGENTES                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  EFECTIVIDAD: Â¿QUÃ‰ TAN BIEN HACE SU TRABAJO?          â”‚
â”‚  â”œâ”€ Accuracy: % de predicciones correctas             â”‚
â”‚  â”œâ”€ Precision: De los positivos, cuÃ¡ntos acertÃ³       â”‚
â”‚  â”œâ”€ Recall: De los positivos reales, cuÃ¡ntos encontrÃ³ â”‚
â”‚  â”œâ”€ F1-Score: Balance entre precision y recall        â”‚
â”‚  â””â”€ AUC-ROC: Robustez a diferentes umbrales           â”‚
â”‚                                                         â”‚
â”‚  EFICIENCIA: Â¿CUÃNTO CUESTA EN RECURSOS?              â”‚
â”‚  â”œâ”€ Latencia: Tiempo respuesta (p95, p99)            â”‚
â”‚  â”œâ”€ Throughput: Requests por segundo                  â”‚
â”‚  â”œâ”€ CPU: Uso de procesador                           â”‚
â”‚  â”œâ”€ Memoria: RAM consumida                           â”‚
â”‚  â””â”€ Costo: Dinero por query (si APIs pagas)          â”‚
â”‚                                                         â”‚
â”‚  ROBUSTEZ: Â¿QUÃ‰ TAN BIEN MANEJA FALLOS?              â”‚
â”‚  â”œâ”€ Error Rate: % de requests que fallan              â”‚
â”‚  â”œâ”€ MTBF: Tiempo promedio entre fallos               â”‚
â”‚  â”œâ”€ Recovery Time: CuÃ¡nto tarda en recuperarse        â”‚
â”‚  â””â”€ Consistency: Respuestas consistentes              â”‚
â”‚                                                         â”‚
â”‚  SEGURIDAD: Â¿ES SEGURO USARLO?                        â”‚
â”‚  â”œâ”€ Violation Rate: CuÃ¡ntas veces violÃ³ restricciÃ³n   â”‚
â”‚  â”œâ”€ Fairness: Equidad entre grupos                    â”‚
â”‚  â””â”€ Adversarial Robustness: Resistencia a ataques    â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ejemplo PrÃ¡ctico

```bash
python 01_metricas_desempeno.py
```

Esto demuestra:
- Calcular matriz de confusiÃ³n (TP, TN, FP, FN)
- Computar accuracy, precision, recall, F1-score
- Medir latencias y percentiles (p95, p99)
- Evaluar robustez con mÃºltiples intentos

### Caso de Uso Real

**Agente Q&A Empresa:**
```
âœ“ Accuracy: 92% (busca balance)
âœ“ Latencia p95: 150ms (aceptable)
âœ“ Error Rate: 0.1% (muy bueno)
âœ“ Fairness: 0.95/1.0 (equitativo)

DECISIÃ“N: Deploy en producciÃ³n
```

---

## 2ï¸âƒ£ MÃ³dulo 2: Benchmarks y Datasets

### Concepto

Un **benchmark** es un conjunto de ejemplos para evaluar agentes de forma justa y reproducible.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ANATOMÃA DE UN BENCHMARK BUENO             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  REPRESENTATIVO                                         â”‚
â”‚  â””â”€ Cubre casos tÃ­picos Y edge cases                  â”‚
â”‚  â””â”€ Distribucion similar a producciÃ³n                â”‚
â”‚  â””â”€ Suficientemente grande (1000+ ejemplos)          â”‚
â”‚                                                         â”‚
â”‚  DESAFIANTE                                            â”‚
â”‚  â””â”€ No trivial de completar                          â”‚
â”‚  â””â”€ Discrimina entre agentes buenos y malos          â”‚
â”‚  â””â”€ Evita saturaciÃ³n (todo >99%)                     â”‚
â”‚                                                         â”‚
â”‚  REPRODUCIBLE                                          â”‚
â”‚  â””â”€ Resultados consistentes                          â”‚
â”‚  â””â”€ Random seeds fijos                               â”‚
â”‚  â””â”€ DocumentaciÃ³n clara                              â”‚
â”‚                                                         â”‚
â”‚  INTERPRETABLE                                         â”‚
â”‚  â””â”€ FÃ¡cil analizar resultados                        â”‚
â”‚  â””â”€ Errores trazables                                â”‚
â”‚  â””â”€ Fallos informativos                              â”‚
â”‚                                                         â”‚
â”‚  PÃšBLICO O COMPARTIBLE                                â”‚
â”‚  â””â”€ Ãštil cientÃ­ficamente                             â”‚
â”‚  â””â”€ Facilita colaboraciÃ³n                            â”‚
â”‚  â””â”€ Tracking de progreso                             â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### AnotaciÃ³n por MÃºltiples Evaluadores

**Â¿Por quÃ© mÃºltiples anotadores?**
- Detecta ambigÃ¼edades en los ejemplos
- Mide la calidad del dataset
- Proporciona verdad consensuada

**Cohen's Kappa: Medida de Acuerdo**
```
Kappa = 1.0    â†’ Acuerdo perfecto
Kappa > 0.80   â†’ Excelente (usar dataset)
Kappa 0.60-0.80 â†’ Sustancial (mejorar instrucciones)
Kappa < 0.60   â†’ Pobre (revisar dataset)
```

### Ejemplo PrÃ¡ctico

```bash
python 02_benchmarks_datasets.py
```

Esto demuestra:
- Crear dataset con ejemplos
- Anotar por mÃºltiples anotadores
- Calcular Cohen's Kappa
- Medir sesgo en el benchmark
- Dividir en train/val/test

---

## 3ï¸âƒ£ MÃ³dulo 3: Testing de Agentes

### PirÃ¡mide de Testing

```
                    â–²
                   / \
                  /   \
                 / S.T.\        Stress Tests (lenta, integral)
                /-------\       - 1000 requests
               /         \      - Uso de memoria
              /  I.T.    \      Integration Tests (medianos)
             /___________\      - Multi-componentes
            /             \
           / U.T.          \    Unit Tests (rÃ¡pidos)
          /_________________\   - Componentes individuales
          Velocidad â†â†’ Cobertura
```

### Tipos de Tests

| Tipo | Scope | Velocidad | Uso |
|------|-------|-----------|-----|
| **Unit** | FunciÃ³n individual | âš¡ 1-10ms | Debugging rÃ¡pido |
| **Integration** | MÃºltiples componentes | â±ï¸ 10-100ms | Verificar interfaces |
| **Functional** | End-to-end | ğŸŒ 100ms-10s | Validar casos uso |
| **Stress** | Bajo carga extrema | ğŸŒğŸŒ 10s+ | LÃ­mites del sistema |

### Ejemplo PrÃ¡ctico

```bash
python 03_testing_agentes.py
```

Esto demuestra:
- Escribir unit tests con fixtures
- Tests de integraciÃ³n
- Functional tests (casos de uso)
- Stress tests (muchas queries)

---

## 4ï¸âƒ£ MÃ³dulo 4: Testing de Comportamiento

### Propiedades Invariantes

```
Un invariante es una propiedad que SIEMPRE debe ser verdadera
```

**Ejemplos:**
```python
def propiedad_nunca_excede_budget():
    """Agente nunca excede su presupuesto"""
    assert agent.spent <= agent.max_budget

def propiedad_encuentra_si_existe():
    """Si objetivo existe, lo encuentra"""
    assert agent.find(target) == target

def propiedad_no_contradicion():
    """No contradice decisiÃ³n previa sin razÃ³n"""
    assert consistent_decision_making()
```

### Edge Cases

```
Valores lÃ­mite que deben manejar:
â”œâ”€ VacÃ­o: [], "", None
â”œâ”€ MÃ¡ximo: len(text) > 1M, 1000+ requests
â”œâ”€ InvÃ¡lido: tipos incorrectos, formatos rotos
â””â”€ Combinatorios: interacciones inesperadas
```

### Ejemplo PrÃ¡ctico

```bash
python 04_testing_comportamiento.py
```

Esto demuestra:
- Property-based testing
- Edge cases y boundary values
- Tests de consistency
- Reproducibilidad con seeds

---

## 5ï¸âƒ£ MÃ³dulo 5: Debugging de Agentes

### TÃ©cnicas de Logging

```
DEBUG    â†’ Muy verbose, para desarrollo
INFO     â†’ InformaciÃ³n general importante
WARNING  â†’ Algo inesperado
ERROR    â†’ Problema serio, pero recoverable
CRITICAL â†’ Fallo del sistema
```

### Herramientas de Debugging

```python
# 1. LOGGING ESTRATÃ‰GICO
logger.debug(f"Decidiendo con: {percepts}")
logger.warning(f"Threat level alto: {threat}")
logger.error(f"Fallo en bÃºsqueda: {error}")

# 2. SNAPSHOTS DE ESTADO
snapshot = {
    'beliefs': agent.beliefs,
    'goals': agent.goals,
    'health': agent.health,
    'timestamp': time.time()
}

# 3. PROFILING
profiler = cProfile.Profile()
profiler.enable()
agent.run()
profiler.disable()
stats = pstats.Stats(profiler)
stats.print_stats(10)  # Top 10 funciones

# 4. REPRODUCCIÃ“N
# Guardar todos los eventos + inputs/outputs
# Permite replay exacto para debugging offline
```

### Post-Mortem Analysis

```
Cuando falla en producciÃ³n:
1. Recolectar datos (logs, traces, state snapshots)
2. Construir timeline de eventos
3. Identificar anomalÃ­a (cuÃ¡ndo cambiÃ³)
4. Rastrear causa raÃ­z (quÃ© causÃ³ cambio)
5. Proponer fix (cÃ³mo prevenir)
```

### Ejemplo PrÃ¡ctico

```bash
python 05_debugging_agentes.py
```

Esto demuestra:
- Configurar logging en mÃºltiples niveles
- Capturar snapshots de estado
- Profiling de funciones
- ReproducciÃ³n de ejecuciones

---

## 6ï¸âƒ£ MÃ³dulo 6: LLMs como Jueces Evaluadores

### Â¿CuÃ¡ndo Usar LLM para EvaluaciÃ³n?

| Caso | Â¿LLM? | Â¿Manual? | Hybrid |
|------|-------|---------|--------|
| Presencia de palabra clave | âœ“ | - | - |
| Relevancia/coherencia | âœ“ | - | **âœ“** |
| Exactitud factual | - | âœ“ | **âœ“** |
| ComparaciÃ³n pares | - | âœ“ | - |
| Escala masiva | **âœ“** | - | - |

### Variabilidad en LLMs

```
Fuentes de variabilidad:
â”œâ”€ Temperatura: 0.0 (determinÃ­stico) â†’ 1.0 (aleatorio)
â”œâ”€ Prompt phrasing: pequeÃ±os cambios = resultados diferentes
â”œâ”€ Context window: quÃ© informaciÃ³n ve
â”œâ”€ Modelo: diferentes LLMs, versiones
â””â”€ CalibraciÃ³n: sesgos inherentes

MitigaciÃ³n:
â”œâ”€ Usa temperatura baja (0.1-0.3)
â”œâ”€ Ejecuta mÃºltiples trials
â”œâ”€ Usa ensemble de LLMs
â”œâ”€ Valida contra manual en muestra
â””â”€ Implementa appeal process
```

### CalibraciÃ³n LLM vs Manual

```
Cohen's Kappa (LLM vs Humano):
< 0.60 â†’ Pobre, requiere ajuste
0.60-0.80 â†’ Aceptable, monitorear
> 0.80 â†’ Bueno, usar en producciÃ³n

Benchmarkiar en ~50-100 ejemplos
antes de producciÃ³n
```

### Ejemplo PrÃ¡ctico

```bash
python 06_llm_como_juez.py
```

Esto demuestra:
- DiseÃ±ar prompts para evaluaciÃ³n
- Evaluar respuestas con LLM
- Comparar con evaluaciÃ³n manual
- Medir variabilidad vs temperatura

---

## ğŸš€ Ejemplos Combinados

### Flujo Completo: EvaluaciÃ³n de Agente

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. CREAR BENCHMARK (MÃ³dulo 2)                      â”‚
â”‚     â””â”€ 100 ejemplos anotados por 3 humanos         â”‚
â”‚     â””â”€ Cohen's Kappa: 0.85 âœ“                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  2. DEFINIR MÃ‰TRICAS (MÃ³dulo 1)                     â”‚
â”‚     â””â”€ Efectividad: Accuracy, Precision, Recall    â”‚
â”‚     â””â”€ Eficiencia: Latencia p95, Throughput        â”‚
â”‚     â””â”€ Robustez: Error Rate, Recovery Time         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  3. ESCRIBIR TESTS (MÃ³dulos 3 & 4)                 â”‚
â”‚     â””â”€ Unit tests: 50+ tests                       â”‚
â”‚     â””â”€ Behavioral tests: propiedades invariantes    â”‚
â”‚     â””â”€ Stress tests: 1000+ queries                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  4. EJECUTAR EVALUACIÃ“N                             â”‚
â”‚     â”œâ”€ Tests: 95% pass âœ“                           â”‚
â”‚     â”œâ”€ Accuracy: 92% âœ“                             â”‚
â”‚     â”œâ”€ Latencia p95: 150ms âœ“                       â”‚
â”‚     â””â”€ Error Rate: 0.05% âœ“                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  5. DEBUGGEAR SI FALLA (MÃ³dulo 5)                  â”‚
â”‚     â””â”€ Analizar logs y traces                      â”‚
â”‚     â””â”€ Reproducir escenario problemÃ¡tico           â”‚
â”‚     â””â”€ Usar profiler para encontrar cuello botella â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  6. EVALUACIÃ“N CON LLM (MÃ³dulo 6)                  â”‚
â”‚     â””â”€ Usar LLM para evaluaciÃ³n rÃ¡pida (pre-screen)â”‚
â”‚     â””â”€ Validar muestra con humanos                 â”‚
â”‚     â””â”€ CorrelaciÃ³n LLM vs Manual: 0.82 âœ“           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  7. DEPLOY & MONITOREO                             â”‚
â”‚     â””â”€ Setup mÃ©tricas en producciÃ³n                â”‚
â”‚     â””â”€ Alertas si degrada                          â”‚
â”‚     â””â”€ ValidaciÃ³n periÃ³dica (cada mes)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Checklist de Testing

```
ANTES DE PRODUCCIÃ“N:
â–¡ Â¿Definiste mÃ©tricas claras?
â–¡ Â¿Tienes benchmark representativo (kappa > 0.80)?
â–¡ Â¿Pasaron todos los unit tests?
â–¡ Â¿Cobertura de cÃ³digo > 80%?
â–¡ Â¿Funciona bajo estrÃ©s (stress test ok)?
â–¡ Â¿Propiedades invariantes verificadas?
â–¡ Â¿Logs y debugging implementados?
â–¡ Â¿EvaluaciÃ³n LLM calibrada vs manual?
â–¡ Â¿SLAs definidos y monitoreados?
â–¡ Â¿Plan de rollback si falla?

REGULARMENTE EN PRODUCCIÃ“N:
â–¡ Revisar mÃ©tricas diariamente
â–¡ Validar evaluaciÃ³n LLM cada mes
â–¡ Tests de regresiÃ³n con cada cambio
â–¡ Post-mortem de incidentes
â–¡ Actualizar benchmark (evitar data drift)
```

---

## ğŸ’¡ Mejores PrÃ¡cticas

### 1. MÃ©tricas

âœ“ **Alineadas con objetivos de negocio**
- No solo accuracy, tambiÃ©n latencia y cost

âœ“ **MÃºltiples perspectivas**
- Efectividad, eficiencia, robustez

âœ“ **Computables eficientemente**
- MÃ©tricas que se pueden medir en tiempo real

âœ“ **Interpretables para stakeholders**
- Evita mÃ©tricas matemÃ¡ticas complejas

### 2. Benchmarks

âœ“ **Versionados**
```
benchmark_v1.0.json  â†’ Original
benchmark_v1.1.json  â†’ Agregado 50 ejemplos
benchmark_v2.0.json  â†’ RediseÃ±o completo
```

âœ“ **Documentados**
```
{
  "name": "qa_benchmark",
  "version": "1.0",
  "description": "Q&A en espaÃ±ol",
  "num_examples": 1000,
  "annotation_guidelines": "...",
  "inter_annotator_agreement": {
    "cohen_kappa": 0.85,
    "annotators": 3
  }
}
```

âœ“ **Divididos correctamente**
```
Train: 70% (700 ejemplos)
Val: 15% (150 ejemplos)
Test: 15% (150 ejemplos, NUNCA vistos durante entrenamiento)
```

### 3. Testing

âœ“ **Automated en CI/CD**
```bash
# Cada commit
pytest test_suite/ --cov=src/ --cov-report=html
```

âœ“ **DeterminÃ­stico**
```python
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
```

âœ“ **Coverage > 80%**
- Encuentra bugs antes de producciÃ³n

### 4. Debugging

âœ“ **Estructura logs**
```json
{
  "timestamp": "2024-01-15T10:30:45Z",
  "level": "WARNING",
  "agent_id": "agent_001",
  "function": "decide",
  "message": "Low confidence in decision",
  "data": {"confidence": 0.35, "options": 5}
}
```

âœ“ **Reproducible**
- Guarda todos los inputs/outputs
- Permite replay exacto

### 5. EvaluaciÃ³n

âœ“ **MÃºltiples mÃ©todos**
- LLM rÃ¡pido para grandes volÃºmenes
- Manual para validaciÃ³n y casos difÃ­ciles

âœ“ **Calibrado**
- Valida LLM contra manual regularmente
- Ajusta umbrales basado en datos reales

âœ“ **Monitoreado**
- Alertas si calibraciÃ³n se degrada
- Appeal process para usuarios

---

## ğŸ“š Recursos Adicionales

### Tutoriales
- Ejemplos funcionales en `ejemplos/evaluacion/`
- Ejecuta: `python 01_metricas_desempeno.py`

### Referencias
- "The Art of Software Testing" - Myers
- "Benchmarking Machine Learning" - paper
- "AI Safety and Alignment" - paper

### Herramientas
- `pytest` - Framework de testing
- `pytest-cov` - Cobertura de cÃ³digo
- `cProfile` - Profiling de Python
- `logging` - Logging estÃ¡ndar

---

## â“ Preguntas Frecuentes

**P: Â¿CuÃ¡ntas mÃ©tricas necesito?**
R: MÃ­nimo 5: accuracy, precision, recall, latencia p95, error rate

**P: Â¿QuÃ© tamaÃ±o debe tener mi benchmark?**
R: MÃ­nimo 100 para desarrollo, 1000+ para producciÃ³n

**P: Â¿CÃ³mo sÃ© si mi agente estÃ¡ listo para producciÃ³n?**
R: Cuando pasa todos los tests, mÃ©tricas cumplen SLAs, y evaluaciÃ³n LLM estÃ¡ calibrada (kappa > 0.80)

**P: Â¿Puedo usar solo LLM para evaluaciÃ³n?**
R: Para producciÃ³n, valida contra manual en ~50-100 ejemplos primero

**P: Â¿Con quÃ© frecuencia debo validar?**
R: Diario en producciÃ³n, mensual para re-calibraciÃ³n

---

## ğŸ“ ConclusiÃ³n

La evaluaciÃ³n rigurosa es la diferencia entre:
- âœ— Agentes que "parecen" funcionar
- âœ“ Agentes que **garantizadamente** funcionan

Usa estas tÃ©cnicas sistemÃ¡ticamente para construir agentes confiables.

---

**Ãšltima actualizaciÃ³n:** 2024-11-13
**Autor:** Curso de Agentes de IA
