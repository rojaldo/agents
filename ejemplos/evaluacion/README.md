# Ejemplos PrÃ¡cticos: EvaluaciÃ³n y Testing de Agentes IA

> **Aprende a evaluar, testear y debuggear agentes IA de forma sistemÃ¡tica**

## ğŸ“‚ Contenidos

Este directorio contiene 6 ejemplos prÃ¡cticos funcionales que implementan los conceptos del mÃ³dulo de evaluaciÃ³n y testing:

```
evaluacion/
â”œâ”€â”€ 01_metricas_desempeno.py      (200+ lÃ­neas)  â†’ MÃ³dulo 1
â”œâ”€â”€ 02_benchmarks_datasets.py     (280+ lÃ­neas)  â†’ MÃ³dulo 2
â”œâ”€â”€ 03_testing_agentes.py         (340+ lÃ­neas)  â†’ MÃ³dulo 3
â”œâ”€â”€ 04_testing_comportamiento.py  (310+ lÃ­neas)  â†’ MÃ³dulo 4
â”œâ”€â”€ 05_debugging_agentes.py       (280+ lÃ­neas)  â†’ MÃ³dulo 5
â”œâ”€â”€ 06_llm_como_juez.py          (320+ lÃ­neas)  â†’ MÃ³dulo 6
â”œâ”€â”€ GUIA_EVALUACION_AGENTES.md    (Completa)   â†’ GuÃ­a DidÃ¡ctica
â””â”€â”€ README.md                     (Este archivo)
```

**Total: ~1,900 lÃ­neas de cÃ³digo funcional + documentaciÃ³n completa**

---

## ğŸš€ Inicio RÃ¡pido

### 1. Ver contenidos

```bash
ls -la /home/rojaldo/cursos/agents/ejemplos/evaluacion/
```

### 2. Ejecutar Ejemplo 1 (sin dependencias externas)

```bash
python /home/rojaldo/cursos/agents/ejemplos/evaluacion/01_metricas_desempeno.py
```

### 3. Ejecutar otros ejemplos

```bash
# Ejemplo 2: Benchmarks
python 02_benchmarks_datasets.py

# Ejemplo 3: Testing
python 03_testing_agentes.py

# Ejemplo 4: Comportamiento
python 04_testing_comportamiento.py

# Ejemplo 5: Debugging
python 05_debugging_agentes.py

# Ejemplo 6: LLM Juez (requiere Ollama)
python 06_llm_como_juez.py
```

### 4. Leer guÃ­a completa

```bash
cat GUIA_EVALUACION_AGENTES.md
```

---

## ğŸ“– DescripciÃ³n de MÃ³dulos

### MÃ³dulo 1: MÃ©tricas de DesempeÃ±o (01_metricas_desempeno.py)

Implementa framework integral de mÃ©tricas:
- **Efectividad**: Accuracy, Precision, Recall, F1-Score, AUC-ROC
- **Eficiencia**: Latencia (p50, p95, p99), Throughput, CPU/Memoria
- **Robustez**: Error Rate, MTBF (Mean Time Between Failures), Recovery Time
- **Seguridad**: Tasa de violaciÃ³n, Fairness Score

**Conceptos**: Matriz de confusiÃ³n, percentiles, distribuciones

---

### MÃ³dulo 2: Benchmarks y Datasets (02_benchmarks_datasets.py)

GestiÃ³n completa de benchmarks:
- Crear datasets con ejemplos de evaluaciÃ³n
- AnotaciÃ³n por mÃºltiples anotadores (inter-annotator agreement)
- Medir Cohen's Kappa (debe ser > 0.80)
- Detectar y mitigar sesgo (selection bias, annotation bias)
- Dividir en train/val/test manteniendo proporciones

**Conceptos**: AnotaciÃ³n, concordancia, versionado de datasets

---

### MÃ³dulo 3: Testing de Agentes (03_testing_agentes.py)

Testing en mÃºltiples niveles:
- **Unit Tests**: Componentes individuales (rÃ¡pido)
- **Integration Tests**: MÃºltiples componentes juntos
- **Functional Tests**: Casos de uso end-to-end
- **Stress Tests**: Bajo carga extrema (1000+ requests)

**Conceptos**: Test suites, assertions, mocks, code coverage

---

### MÃ³dulo 4: Testing de Comportamiento (04_testing_comportamiento.py)

Testing de propiedades y comportamiento:
- **Propiedades invariantes**: Lo que SIEMPRE debe ser verdadero
- **Edge cases**: Valores lÃ­mite (vacÃ­o, mÃ¡ximo, invÃ¡lido)
- **Consistency**: Determinismo y reproducibilidad
- **Property-based testing**: Genera datos automÃ¡ticamente

**Conceptos**: Invariantes, generadores, seeds, boundary values

---

### MÃ³dulo 5: Debugging de Agentes (05_debugging_agentes.py)

Herramientas y tÃ©cnicas de debugging:
- **Logging estratÃ©gico**: DEBUG, INFO, WARNING, ERROR, CRITICAL
- **Snapshots de estado**: Captura estado en puntos especÃ­ficos
- **Profiling**: Encuentra quÃ© funciones tardan mÃ¡s
- **ReproducciÃ³n**: Replay de ejecuciones para debugging offline
- **Post-mortem**: AnÃ¡lisis de errores despuÃ©s de ocurrir

**Conceptos**: Event logging, profiling, state inspection, tracing

---

### MÃ³dulo 6: LLMs como Jueces (06_llm_como_juez.py)

EvaluaciÃ³n automÃ¡tica con LLMs (vÃ­a Ollama):
- Usar LLM para evaluaciÃ³n rÃ¡pida y escalable
- MÃ©tricas cualitativas (relevancia, exactitud) vs cuantitativas
- Variabilidad en evaluaciones (temperatura, prompts)
- CalibraciÃ³n LLM vs evaluaciÃ³n manual
- MitigaciÃ³n de sesgos

**Conceptos**: Prompts estructurados, temperatura, Cohen's Kappa, ensemble

---

## ğŸ¯ Flujo de Aprendizaje

```
SEMANA 1: Fundamentos
â”œâ”€ Lee GUIA_EVALUACION_AGENTES.md (secciones 1-2)
â”œâ”€ Ejecuta 01_metricas_desempeno.py
â””â”€ Experimenta modificando valores

SEMANA 2: Benchmarks y Testing
â”œâ”€ Lee secciones 2-3 de guÃ­a
â”œâ”€ Ejecuta 02_benchmarks_datasets.py
â”œâ”€ Ejecuta 03_testing_agentes.py
â””â”€ Practica: Crea tu propio benchmark

SEMANA 3: Comportamiento y Debugging
â”œâ”€ Lee secciones 4-5 de guÃ­a
â”œâ”€ Ejecuta 04_testing_comportamiento.py
â”œâ”€ Ejecuta 05_debugging_agentes.py
â””â”€ Practica: Debuggea un escenario real

SEMANA 4: EvaluaciÃ³n Avanzada
â”œâ”€ Lee secciÃ³n 6 de guÃ­a
â”œâ”€ Configura Ollama (instrucciones abajo)
â”œâ”€ Ejecuta 06_llm_como_juez.py
â””â”€ Proyecto: EvalÃºa tu propio agente
```

---

## âš™ï¸ Setup Ollama (Opcional)

Para ejemplos con LLM, necesitas Ollama:

```bash
# 1. Instala Ollama (https://ollama.ai)

# 2. Inicia servicio (en terminal separada)
ollama serve

# 3. Descarga modelo (en otra terminal)
ollama pull mistral

# 4. Verifica
curl http://localhost:11434/api/generate -d '{"model":"mistral","prompt":"test"}'
```

Sin Ollama: Los ejemplos funcionan en modo simulaciÃ³n automÃ¡tico.

---

## ğŸ“Š CaracterÃ­sticas de los Ejemplos

âœ“ **Completamente funcionales**: No requieren datos externos
âœ“ **Autodocumentados**: CÃ³digo claro con comentarios
âœ“ **DidÃ¡cticos**: DiseÃ±ados para aprender
âœ“ **Modulares**: CÃ³digo reutilizable en tus proyectos
âœ“ **Sin dependencias pesadas**: Funcionan con dependencias mÃ­nimas
âœ“ **Con salidas visuales**: Formatos claros y fÃ¡ciles de entender

---

## ğŸ“š Recursos

- **GUIA_EVALUACION_AGENTES.md**: GuÃ­a didÃ¡ctica completa (recomendado leer primero)
- **CÃ³digo comentado**: Cada ejemplo tiene docstrings y comentarios explicativos
- Ejemplos en este directorio

---

## âœ… Checklist de EjecuciÃ³n

```
â–¡ 01_metricas_desempeno.py      âœ“ (independiente)
â–¡ 02_benchmarks_datasets.py     âœ“ (independiente)
â–¡ 03_testing_agentes.py         âœ“ (independiente)
â–¡ 04_testing_comportamiento.py  âœ“ (independiente)
â–¡ 05_debugging_agentes.py       âœ“ (independiente)
â–¡ 06_llm_como_juez.py          âœ“ (con/sin Ollama)
â–¡ GUIA_EVALUACION_AGENTES.md    âœ“ (lectura recomendada)
```

---

## ğŸ“– GuÃ­a de Uso de Cada MÃ³dulo

### Para Aprender
1. Lee la secciÃ³n correspondiente en GUIA_EVALUACION_AGENTES.md
2. Ejecuta el ejemplo
3. Modifica valores para experimentar
4. Estudia el cÃ³digo

### Para Aplicar a Tu Proyecto
1. Copia el cÃ³digo como base
2. AdÃ¡ptalo a tu agente especÃ­fico
3. Ejecuta para validar
4. Integra en CI/CD

### Para Referencia
- Usa como template para tus propios tests
- Consulta cuando necesites especÃ­ficas

---

## ğŸ“ ConclusiÃ³n

DespuÃ©s de completar estos ejemplos:
- EntenderÃ¡s evaluaciÃ³n rigurosa de agentes
- PodrÃ¡s escribir tests en mÃºltiples niveles
- SabrÃ¡s cÃ³mo debuggear comportamientos inesperados
- EstarÃ¡s listo para producciÃ³n

**PrÃ³ximo paso**: Aplicar estas tÃ©cnicas a tu agente.

---

**VersiÃ³n**: 1.0
**Fecha**: 2024-11-13
**Total de cÃ³digo**: ~1,900 lÃ­neas
**Tiempo estimado**: 4 semanas

Â¡Feliz aprendizaje!
