# Ejemplos DidÃ¡cticos: Memoria y Contexto en Agentes de IA

Sistema completo de ejemplos funcionales sobre memoria y contexto en agentes, basados en LangChain + Ollama ejecutable localmente.

## ğŸ“š Contenidos

Este directorio contiene 7 ejemplos progresivos que cubren el temario del mÃ³dulo `02-memoria-contexto.adoc`:

### 1. **01_tipos_memoria.py** - Tipos de Memoria en Agentes
**Conceptos**: InspiraciÃ³n neurobiolÃ³gica, taxonomÃ­a de memoria

Implementa los 5 tipos principales de memoria humana aplicados a agentes:

- **Memoria Sensorial**: Buffer muy breve (milisegundos), gran capacidad
- **Memoria de Trabajo**: Limitada (4-7 items), consciente, actualmente procesada
- **Memoria EpisÃ³dica**: Eventos ordenados cronolÃ³gicamente con contexto
- **Memoria SemÃ¡ntica**: Conocimiento abstracto descontextualizado
- **Memoria Procedural**: Habilidades que mejoran con la prÃ¡ctica

**Detalles tÃ©cnicos**:
- Clase `MemoriaSensorial`: Simula buffer con expiraciÃ³n temporal
- Clase `MemoriaTrabajoLimitada`: Implementa limitaciÃ³n de capacidad y envejecimiento
- Clase `MemoriaEpisodica`: Timeline de eventos con recuperaciÃ³n temporal
- Clase `MemoriaSemantica`: Grafo de conocimiento simple (hechos + relaciones)
- Clase `MemoriaProcedural`: Registro de habilidades con mejora de tasa de Ã©xito

```bash
python 01_tipos_memoria.py
```

**Salida esperada**: DemostraciÃ³n de todos los 5 tipos funcionando simultÃ¡neamente

---

### 2. **02_gestion_estado.py** - GestiÃ³n de Estado en Agentes
**Conceptos**: Identidad, posiciÃ³n, recursos, objetivos, creencias, event sourcing

Demuestra cÃ³mo un agente mantiene y persiste su estado completo:

- **Identidad**: ID, nombre, tipo de agente, versiÃ³n
- **PosiciÃ³n**: UbicaciÃ³n (x, y, z) en el ambiente
- **Recursos**: Items que posee (CPU, memoria, energÃ­a)
- **Objetivos**: Metas con prioridad y progreso
- **Creencias**: Conocimiento del mundo con confianza
- **Relaciones**: VÃ­nculos con otros agentes

**Detalles tÃ©cnicos**:
- Clase `EstadoAgenteLLM`: Estado completo del agente
- **Event Sourcing**: Registro inmutable de todos los cambios
- **Persistencia**: Guardado a JSON (snapshot + event log)
- **RecuperaciÃ³n**: Carga de estado desde archivos

```bash
python 02_gestion_estado.py
```

**CaracterÃ­sticas especiales**:
- Snapshots para recuperaciÃ³n rÃ¡pida
- Event log para auditorÃ­a completa
- Versionado de estado
- Historial de cambios

---

### 3. **03_buffer_contexto.py** - Buffer de Contexto y LÃ­mites en LLMs
**Conceptos**: Ventana de contexto, compresiÃ³n, limitaciones de tokens

Maneja la restricciÃ³n crÃ­tica de los LLMs: contexto limitado.

- **Buffer de contexto**: Ventana mÃ³vil de informaciÃ³n reciente
- **GestiÃ³n de tokens**: Tracking de uso y disponibilidad
- **Estrategias de eliminaciÃ³n**: FIFO, LRU, importancia, relevancia
- **CompresiÃ³n**: Resumen extractivo y abstractivo

**Detalles tÃ©cnicos**:
- Clase `BufferContexto`: Buffer circular con lÃ­mite de tokens
- Clase `CompresorContexto`: CompresiÃ³n y resumen de contenido
- Estrategia `FIFO`: Descarta lo mÃ¡s antiguo
- Estrategia `LRU`: Descarta lo menos recientemente usado
- Estrategia `RELEVANCIA`: Descarta lo menos importante vs recency

```bash
python 03_buffer_contexto.py
```

**ParÃ¡metros personalizables**:
- `max_tokens`: LÃ­mite total (por defecto 2048)
- `margen_seguridad`: Reserva de seguridad (90%)
- `estrategia`: MÃ©todo de eliminaciÃ³n

---

### 4. **04_embeddings_busqueda.py** - Embeddings y BÃºsqueda SemÃ¡ntica
**Conceptos**: RepresentaciÃ³n vectorial, similitud coseno, bÃºsqueda hÃ­brida

Implementa bÃºsqueda semÃ¡ntica sin dependencias externas (demostraciÃ³n educativa):

- **GeneraciÃ³n de embeddings**: ConversiÃ³n de texto a vectores
- **BÃºsqueda vectorial**: Similitud coseno entre vectores
- **BÃºsqueda por palabras clave**: Full-text search
- **BÃºsqueda hÃ­brida**: CombinaciÃ³n de ambas

**Detalles tÃ©cnicos**:
- Clase `GeneradorEmbeddings`: TF-IDF simplificado (sin dependencias)
- Clase `IndiceVectorial`: Ãndice simple de bÃºsqueda
- Clase `BuscadorHibrido`: Combina keyword + semantic
- MÃ©trica: Similitud coseno (0.0 a 1.0)

```bash
python 04_embeddings_busqueda.py
```

**MÃ©tricas de similitud implementadas**:
- Coseno: Para bÃºsqueda vectorial semÃ¡ntica
- Jaccard: Para conjuntos de palabras (keyword)
- Euclidiana: Distancia en espacio vectorial

**Nota sobre producciÃ³n**: Para usar modelos reales de embeddings:
```python
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
embedding = model.encode(texto)
```

---

### 5. **05_rag_retrieval.py** - RAG (Retrieval-Augmented Generation)
**Conceptos**: Pipeline completo de recuperaciÃ³n aumentada

Implementa el flujo completo de RAG:

1. **RecuperaciÃ³n**: Buscar documentos relevantes
2. **ConstrucciÃ³n de contexto**: Ensamblar fragmentos de documentos
3. **CreaciÃ³n de prompt**: Enriquecer pregunta con contexto
4. **GeneraciÃ³n**: LLM responde basÃ¡ndose en contexto
5. **Postprocesamiento**: Formatear respuesta

**Detalles tÃ©cnicos**:
- Clase `BaseConocimiento`: Almacena documentos indexados
- Clase `PipelineRAG`: Orquesta el flujo completo
- BÃºsqueda por palabras clave simple
- ConstrucciÃ³n inteligente de contexto (max_chars)
- GeneraciÃ³n de respuestas simuladas

```bash
python 05_rag_retrieval.py
```

**Ventajas de RAG**:
- InformaciÃ³n actualizada (basada en documentos)
- Cita de fuentes
- Menos alucinaciones
- Customizable por dominio

**IntegraciÃ³n con Ollama** (pseudocÃ³digo incluido):
```python
from langchain.llms import Ollama
ollama = Ollama(model="mistral")
response = ollama(prompt_enriquecido)
```

---

### 6. **06_memoria_conversacional.py** - Memoria en Agentes Conversacionales
**Conceptos**: Historial acumulativo, resoluciÃ³n de referencias, privacidad

Demuestra cÃ³mo agentes mantienen conversaciones coherentes:

- **Historial de conversaciÃ³n**: Contexto acumulativo de turnos
- **Seguimiento de entidades**: NER bÃ¡sica con tipos
- **ResoluciÃ³n de referencias anafÃ³ricas**: "Ã©l", "ella", "lo" -> entidad
- **PersonalizaciÃ³n**: Adaptar respuestas segÃºn usuario
- **Privacidad**: Filtrado de datos sensibles (GDPR)

**Detalles tÃ©cnicos**:
- Clase `SeguimientoEntidades`: Extrae y rastrea entidades
- Clase `HistorialConversacion`: Gestiona turno a turno
- NER bÃ¡sica para: email, telÃ©fono, productos, nÃºmeros
- ResoluciÃ³n heurÃ­stica de pronombres
- Conformidad GDPR: marcar y limpiar datos sensibles

```bash
python 06_memoria_conversacional.py
```

**CaracterÃ­sticas de privacidad**:
- IdentificaciÃ³n de PII (Personally Identifiable Information)
- Filtrado de datos sensibles en contexto
- MÃ©todo `limpiar_datos_sensibles()` para cumplimiento GDPR
- RedacciÃ³n automÃ¡tica de informaciÃ³n sensible

---

### 7. **07_memoria_jerarquica.py** - Sistema de Memoria JerÃ¡rquica Avanzada
**Conceptos**: ConsolidaciÃ³n, compresiÃ³n, interferencia, olvido adaptativo

Arquitectura escalable con tres niveles jerÃ¡rquicos:

- **Nivel 1 (EpisÃ³dico)**: Detalles especÃ­ficos de eventos
- **Nivel 2 (TÃ¡ctico)**: Patrones y regularidades
- **Nivel 3 (EstratÃ©gico)**: Reglas abstractas generales

**Flujo de consolidaciÃ³n** (como "sueÃ±o" en humanos):
1. Registrar episodios con detalles especÃ­ficos
2. Extraer patrones (frecuencia >= 2)
3. Generar reglas desde patrones confiables
4. Envejecer episodios (olvido natural)
5. Eliminar informaciÃ³n insignificante

**Detalles tÃ©cnicos**:
- Clase `MemoriaJerarquica`: Gestiona 3 niveles
- `RegistroEpisodico`: Detalles + timestamp + importancia
- `PatronTactico`: Agrupa episodios similares
- `ReglaBstracia`: GeneralizaciÃ³n de patrones
- Olvido adaptativo: Score = importancia Ã— exp(-edad/30)

```bash
python 07_memoria_jerarquica.py
```

**Beneficios del diseÃ±o jerÃ¡rquico**:
- **Escalabilidad**: Millones de episodios -> pocos patrones -> reglas
- **Eficiencia**: 6 episodios -> 2 patrones -> 2 reglas (compresiÃ³n)
- **Coherencia**: RecuperaciÃ³n comienza en nivel abstracto
- **Privacidad**: Olvido selectivo de episodios antiguos

---

## ğŸš€ Quickstart

### Requisitos Previos

```bash
# 1. Instalar Python 3.8+
python --version

# 2. Instalar dependencias para ejemplos
pip install langchain ollama sentence-transformers pydantic

# 3. (Opcional) Instalar Ollama para integraciÃ³n real
# Ver: https://ollama.ai
```

### Ejecutar Ejemplos Individuales

```bash
# Cambiar al directorio
cd ejemplos/memoria

# Ejecutar ejemplo 1: Tipos de memoria
python 01_tipos_memoria.py

# Ejecutar ejemplo 2: GestiÃ³n de estado
python 02_gestion_estado.py

# ... etc
```

### Script de EjecuciÃ³n Secuencial

```bash
# Ejecutar TODOS los ejemplos en orden
for i in {1..7}; do
    echo "=== Ejemplo $i ==="
    python 0${i}_*.py
    echo ""
done
```

---

## ğŸ“Š Comparativa de Ejemplos

| Ejemplo | Concepto | Complejidad | Tokens | Dependencias |
|---------|----------|-------------|--------|--------------|
| 01 | 5 tipos de memoria | â­ | ~100 | ninguna |
| 02 | Estado + Event sourcing | â­â­ | ~300 | pydantic |
| 03 | Buffer + CompresiÃ³n | â­â­ | ~250 | ninguna |
| 04 | Embeddings + BÃºsqueda | â­â­â­ | ~400 | numpy |
| 05 | RAG completo | â­â­â­ | ~350 | langchain |
| 06 | ConversaciÃ³n + NER | â­â­ | ~280 | ninguna |
| 07 | JerarquÃ­a + ConsolidaciÃ³n | â­â­â­ | ~320 | ninguna |

---

## ğŸ”— IntegraciÃ³n con Ollama y LangChain

Cada ejemplo estÃ¡ diseÃ±ado para integraciÃ³n fÃ¡cil con Ollama:

```python
# InstalaciÃ³n
pip install langchain ollama

# Ejecutar Ollama
ollama serve

# En otra terminal, descargar modelo
ollama pull mistral  # o neural-chat, orca-mini, etc

# Usar en cÃ³digo
from langchain.llms import Ollama
from langchain.memory import ConversationBufferWindowMemory

llm = Ollama(model="mistral")
memory = ConversationBufferWindowMemory(k=3)

# RAG + Memory + LLM
from langchain.chains import RetrievalQA
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,  # De ejemplo 04
    memory=memory
)
```

---

## ğŸ“– Mapeo a Temario

Cada ejemplo cubre directamente secciones del temario:

```
02-memoria-contexto.adoc
â”œâ”€â”€ MÃ³dulo 1: Tipos de Memoria
â”‚   â””â”€â”€ 01_tipos_memoria.py âœ“
â”‚
â”œâ”€â”€ MÃ³dulo 2: GestiÃ³n de Estado
â”‚   â””â”€â”€ 02_gestion_estado.py âœ“
â”‚
â”œâ”€â”€ MÃ³dulo 3: Memoria a Corto Plazo (Buffer)
â”‚   â””â”€â”€ 03_buffer_contexto.py âœ“
â”‚
â”œâ”€â”€ MÃ³dulo 4: Memoria a Largo Plazo (IndexaciÃ³n)
â”‚   â”œâ”€â”€ 04_embeddings_busqueda.py âœ“
â”‚   â””â”€â”€ 05_rag_retrieval.py âœ“
â”‚
â”œâ”€â”€ MÃ³dulo 5: RecuperaciÃ³n de InformaciÃ³n
â”‚   â””â”€â”€ 04_embeddings_busqueda.py (hÃ­brida) âœ“
â”‚
â”œâ”€â”€ MÃ³dulo 6: Memoria Conversacional
â”‚   â””â”€â”€ 06_memoria_conversacional.py âœ“
â”‚
â””â”€â”€ MÃ³dulo 7: Arquitecturas Avanzadas
    â””â”€â”€ 07_memoria_jerarquica.py âœ“
```

---

## ğŸ¯ PropÃ³sitos DidÃ¡cticos

Cada ejemplo demuestra:

### 01 - Fundamentos BiolÃ³gicos
- CÃ³mo la biologÃ­a inspira arquitecturas de IA
- Importancia de mÃºltiples escalas temporales

### 02 - Persistencia y Recuperabilidad
- CÃ³mo guardar y recuperar estado completo
- AuditorÃ­a mediante event sourcing

### 03 - Restricciones PrÃ¡cticas
- CÃ³mo los LLMs tienen limitaciones de contexto
- Estrategias de compresiÃ³n

### 04 - BÃºsqueda Inteligente
- De palabras clave a semÃ¡ntica
- MÃ©tricas de similitud

### 05 - GeneraciÃ³n Fundamentada
- RAG reduce alucinaciones
- Cita de fuentes

### 06 - InteracciÃ³n Natural
- Coherencia conversacional
- Privacidad en sistemas reales

### 07 - Escalabilidad
- CÃ³mo escalar a millones de interacciones
- ConsolidaciÃ³n de conocimiento

---

## ğŸ”§ PersonalizaciÃ³n

### Aumentar Capacidad de Memoria

```python
# Cambiar lÃ­mite de tokens en buffer
buffer = BufferContexto(max_tokens=4096)

# Aumentar capacidad de memoria de trabajo
mem_trabajo = MemoriaTrabajoLimitada(capacidad=10)

# MÃ¡s documentos en base de conocimiento
for doc in documentos_adicionales:
    base.agregar_documento(doc)
```

### Usar Diferentes Estrategias

```python
# Cambiar estrategia de eliminaciÃ³n en buffer
buffer = BufferContexto(
    estrategia=EstrategiaEliminacion.IMPORTANCIA
)

# Cambiar pesos en bÃºsqueda hÃ­brida
resultados = buscador.busqueda_hibrida(
    query,
    peso_keyword=0.5,
    peso_semantica=0.5
)
```

### Integrar Modelos Reales

```python
# Usar embeddings reales
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')

# Reemplazar GeneradorEmbeddings.generar_embedding()
def generar_embedding(self, texto):
    return model.encode(texto).tolist()
```

---

## ğŸ“‹ Checklist de Aprendizaje

- [ ] EjecutÃ© todos los 7 ejemplos
- [ ] Entiendo cÃ³mo los 5 tipos de memoria funcionan
- [ ] SÃ© cÃ³mo persistir estado de agentes
- [ ] Entiendo el problema de lÃ­mites de contexto
- [ ] Puedo explicar bÃºsqueda semÃ¡ntica vs keywords
- [ ] SÃ© cÃ³mo funciona RAG
- [ ] Entiendo resoluciÃ³n de referencias anafÃ³ricas
- [ ] Conozco consolidaciÃ³n jerÃ¡rquica de memoria
- [ ] Puedo integrar con Ollama en mis proyectos
- [ ] Considero privacidad en sistemas de memoria

---

## ğŸ› Troubleshooting

**P: Los ejemplos son muy lentos**
R: Aumenta `margen_seguridad` en buffer o reduce documentos en RAG

**P: Â¿CÃ³mo uso esto con mi propio LLM?**
R: Reemplaza la funciÃ³n `_generar_respuesta()` en RAG para llamar a tu LLM

**P: Â¿CÃ³mo garantizo privacidad?**
R: Usa `limpiar_datos_sensibles()` en conversaciones y marca datos como sensibles

**P: Â¿Debo usar todos los niveles de memoria?**
R: Depende de tu caso. ConversaciÃ³n simple = solo episÃ³dico. Agente complejo = todos.

---

## ğŸ“š Recursos Adicionales

- **LangChain Docs**: https://python.langchain.com
- **Ollama**: https://ollama.ai
- **Sentence Transformers**: https://www.sbert.net
- **Paper RAG**: "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"

---

## ğŸ“ Licencia

Ejemplos educativos para el Curso de Agentes de IA

---

## ğŸ’¡ Tips para Instructores

Para usar estos ejemplos en clase:

1. **Semana 1**: Ejecutar ejemplos 1-3 (conceptos)
2. **Semana 2**: Ejecutar ejemplos 4-5 (bÃºsqueda y RAG)
3. **Semana 3**: Ejecutar ejemplos 6-7 (avanzado)
4. **Proyecto**: Combinar todos en un agente conversacional

Cada ejemplo toma ~15 minutos para ejecutar y comprender.

---

**Creado**: Noviembre 2024
**VersiÃ³n**: 1.0
**Compatibilidad**: Python 3.8+
