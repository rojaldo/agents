# Ejemplos AutoGen - M√≥dulos 6-11

Colecci√≥n completa de ejemplos de c√≥digo funcionales para los m√≥dulos 6-11 del curso de AutoGen. Todos los ejemplos est√°n dise√±ados para funcionar con **Ollama** como proveedor local de LLM.

## üìã Requisitos Previos

### Obligatorio: Ollama
1. **Instalar Ollama**: https://ollama.ai
2. **Ejecutar servidor**: `ollama serve`
3. **Descargar modelo**: `ollama pull mistral` (u otro modelo)

### Python
- Python 3.8+
- Librer√≠as: `requests` (instalado autom√°ticamente en la mayor√≠a de entornos)

```bash
pip install requests
```

## üìÅ Estructura de Directorios

```
ejemplos/autogen/
‚îú‚îÄ‚îÄ modulo_6/          # Optimizaci√≥n y Costos
‚îÇ   ‚îú‚îÄ‚îÄ 01_token_manager.py
‚îÇ   ‚îú‚îÄ‚îÄ 02_prompt_optimizer.py
‚îÇ   ‚îú‚îÄ‚îÄ 03_ollama_integration.py
‚îÇ   ‚îú‚îÄ‚îÄ 04_cache_manager.py
‚îÇ   ‚îú‚îÄ‚îÄ 05_semantic_cache.py
‚îÇ   ‚îú‚îÄ‚îÄ 06_advanced_ollama_models.py
‚îÇ   ‚îú‚îÄ‚îÄ 07_realtime_monitor.py
‚îÇ   ‚îî‚îÄ‚îÄ 08_cost_optimizer.py
‚îÇ
‚îú‚îÄ‚îÄ modulo_7/          # Casos de Uso Pr√°cticos
‚îÇ   ‚îú‚îÄ‚îÄ 01_code_generator.py
‚îÇ   ‚îú‚îÄ‚îÄ 02_code_reviewer.py
‚îÇ   ‚îú‚îÄ‚îÄ 03_programming_team.py
‚îÇ   ‚îî‚îÄ‚îÄ 04_data_analyzer.py
‚îÇ
‚îú‚îÄ‚îÄ modulo_8/          # Testing y Debugging
‚îÇ   ‚îî‚îÄ‚îÄ 01_unit_test_generator.py
‚îÇ
‚îú‚îÄ‚îÄ modulo_9/          # Despliegue en Producci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ 01_production_deployment.py
‚îÇ
‚îú‚îÄ‚îÄ modulo_10/         # Integraciones
‚îÇ   ‚îî‚îÄ‚îÄ 01_framework_integration.py
‚îÇ
‚îú‚îÄ‚îÄ modulo_11/         # Proyecto Final
‚îÇ   ‚îî‚îÄ‚îÄ 01_final_project.py
‚îÇ
‚îî‚îÄ‚îÄ run_all_examples.py  # Script para ejecutar todos los ejemplos
```

## üöÄ Gu√≠a de Ejecuci√≥n

### Opci√≥n 1: Ejecutar un ejemplo espec√≠fico

```bash
# Ejemplo: Ejecutar Token Manager
cd modulo_6
python 01_token_manager.py

# Ejemplo: Ejecutar Code Generator con Ollama
cd modulo_7
python 01_code_generator.py
```

### Opci√≥n 2: Ejecutar todos los ejemplos

```bash
# Desde el directorio ra√≠z ejemplos/autogen/
python run_all_examples.py
```

Este script:
- Verifica disponibilidad de Ollama
- Ejecuta todos los ejemplos secuencialmente
- Genera reporte de resultados en `execution_results.json`
- Muestra estad√≠sticas de √©xito/fallo

### Opci√≥n 3: Con variables de entorno

```bash
# Especificar URL de Ollama
export OLLAMA_URL="http://localhost:11434"

# Especificar modelo
export OLLAMA_MODEL="mistral"

# Especificar ambiente
export ENVIRONMENT="production"

python 01_token_manager.py
```

## üìö Descripci√≥n de M√≥dulos

### M√≥dulo 6: Optimizaci√≥n y Costos

Enfoque en reducir costos y optimizar el uso de recursos:

| Ejemplo | Descripci√≥n |
|---------|-------------|
| `01_token_manager.py` | Gesti√≥n b√°sica de tokens y c√°lculo de costos |
| `02_prompt_optimizer.py` | Optimizaci√≥n de prompts para reducir tokens |
| `03_ollama_integration.py` | Cliente avanzado de Ollama con fallback |
| `04_cache_manager.py` | Cach√© en memoria con estad√≠sticas |
| `05_semantic_cache.py` | Cach√© inteligente con b√∫squeda de similitud |
| `06_advanced_ollama_models.py` | Gestor de m√∫ltiples modelos Ollama |
| `07_realtime_monitor.py` | Monitoreo en tiempo real de tokens |
| `08_cost_optimizer.py` | Optimizaci√≥n de costos con estrategia de cach√© |

**Conceptos Clave:**
- Estimaci√≥n de tokens
- Control de presupuesto
- Cach√© para reutilizaci√≥n
- Selecci√≥n autom√°tica de modelos
- Monitoreo de costos

### M√≥dulo 7: Casos de Uso Pr√°cticos

Aplicaciones reales de AutoGen:

| Ejemplo | Descripci√≥n |
|---------|-------------|
| `01_code_generator.py` | Generaci√≥n autom√°tica de c√≥digo |
| `02_code_reviewer.py` | An√°lisis y revisi√≥n de c√≥digo |
| `03_programming_team.py` | Sistema colaborativo de desarrollo |
| `04_data_analyzer.py` | An√°lisis autom√°tico de datasets |

**Conceptos Clave:**
- Generaci√≥n de c√≥digo
- Revisi√≥n autom√°tica
- An√°lisis de datos
- Trabajo colaborativo entre agentes

### M√≥dulo 8: Testing y Debugging

Automatizaci√≥n de testing y debugging:

| Ejemplo | Descripci√≥n |
|---------|-------------|
| `01_unit_test_generator.py` | Generaci√≥n autom√°tica de tests unitarios |

**Conceptos Clave:**
- Generaci√≥n de tests
- Cobertura de c√≥digo
- Debugging colaborativo
- An√°lisis de errores

### M√≥dulo 9: Despliegue en Producci√≥n

Configuraci√≥n y despliegue en producci√≥n:

| Ejemplo | Descripci√≥n |
|---------|-------------|
| `01_production_deployment.py` | Configuraci√≥n multiambiente para producci√≥n |

**Conceptos Clave:**
- Configuraci√≥n por ambiente
- Health checks
- Rate limiting
- Seguridad en producci√≥n

### M√≥dulo 10: Integraciones

Integraci√≥n con otros frameworks y servicios:

| Ejemplo | Descripci√≥n |
|---------|-------------|
| `01_framework_integration.py` | Integraci√≥n con LangChain y otros frameworks |

**Conceptos Clave:**
- Integraci√≥n con frameworks
- API Gateway
- Chain of prompts
- Cadenas de razonamiento

### M√≥dulo 11: Proyecto Final

Sistema completo integrando todos los conceptos:

| Ejemplo | Descripci√≥n |
|---------|-------------|
| `01_final_project.py` | Sistema de an√°lisis de c√≥digo completo |

**Conceptos Clave:**
- Proyecto integral
- M√∫ltiples agentes
- Flujos complejos
- Reportes y m√©tricas

## ‚öôÔ∏è Configuraci√≥n de Ollama

### Instalaci√≥n

```bash
# macOS
brew install ollama

# Linux
curl https://ollama.ai/install.sh | sh

# Windows
# Descargar desde https://ollama.ai/download

```

### Ejecutar servidor

```bash
ollama serve

# En otra terminal, descargar un modelo
ollama pull mistral
```

### Modelos recomendados

- **mistral**: R√°pido, buena calidad (2.2 GB)
- **llama2**: Muy potente pero lento (3.8 GB)
- **neural-chat**: Muy r√°pido pero menor calidad (1.3 GB)

## üîç Verificar disponibilidad de Ollama

```bash
# Comprobar que Ollama est√° ejecut√°ndose
curl http://localhost:11434/api/tags

# Deber√≠a devolver algo como:
# {"models":[{"name":"mistral:latest",...}]}
```

## üìä Resultados de Ejecuci√≥n

Despu√©s de ejecutar `run_all_examples.py`, se genera un archivo `execution_results.json`:

```json
{
  "total": 15,
  "successful": 14,
  "failed": 1,
  "modules": {
    "modulo_6": {
      "total": 8,
      "successful": 8,
      "failed": 0
    },
    ...
  },
  "timestamp": "2024-11-08T10:30:00"
}
```

## üêõ Soluci√≥n de Problemas

### Error: "No se puede conectar a Ollama"

**Problema**: Ollama no est√° ejecut√°ndose o no est√° en `http://localhost:11434`

**Soluci√≥n**:
```bash
# Terminal 1: Ejecutar Ollama
ollama serve

# Terminal 2: Ejecutar el ejemplo
python ejemplo.py

# O especificar URL diferente
export OLLAMA_URL="http://192.168.1.100:11434"
```

### Error: "Model not found"

**Problema**: El modelo no est√° descargado

**Soluci√≥n**:
```bash
# Descargar el modelo
ollama pull mistral

# Listar modelos disponibles
ollama list
```

### Error: "Timeout"

**Problema**: Ollama tarda demasiado en responder

**Soluci√≥n**:
- Usar un modelo m√°s peque√±o (neural-chat)
- Aumentar timeout en el c√≥digo
- Verificar recursos de m√°quina

### Error: "CUDA out of memory"

**Problema**: GPU sin suficiente memoria

**Soluci√≥n**:
```bash
# Ejecutar solo en CPU
CUDA_VISIBLE_DEVICES="" ollama serve

# O usar modelo m√°s peque√±o
ollama pull neural-chat
```

## üìù Ejemplos de Uso

### Ejecutar un ejemplo simple

```bash
# 1. Aseg√∫rate que Ollama est√° ejecut√°ndose
ollama serve

# 2. En otra terminal, ejecuta el ejemplo
python modulo_6/01_token_manager.py
```

### Ejecutar con configuraci√≥n personalizada

```python
# En el c√≥digo
from modulo_6.codigo_ejemplo import ClienteOllama

client = ClienteOllama(
    base_url="http://mi-servidor:11434",
    model="llama2"
)
```

### Procesar m√∫ltiples ejemplos

```bash
#!/bin/bash

for modulo in modulo_6 modulo_7 modulo_8; do
    echo "Procesando $modulo..."
    cd $modulo
    for ejemplo in *.py; do
        echo "  - Ejecutando $ejemplo"
        python $ejemplo
    done
    cd ..
done
```

## üìà M√©tricas y Monitoreo

### Monitorear rendimiento

```python
# Los ejemplos incluyen estad√≠sticas autom√°ticas:
manager = TokenManager()
# ... uso del manager ...
manager.print_summary()  # Muestra estad√≠sticas

# Salida t√≠pica:
# ==================================================
# RESUMEN DE TOKENS Y COSTOS
# ==================================================
# Modelo: mistral
# Total de tokens usados: 450
# Total de costo: $0.0045
# ...
```

## üîó Referencias

- **AutoGen**: https://microsoft.github.io/autogen/
- **Ollama**: https://ollama.ai/
- **Mistral AI**: https://mistral.ai/
- **Documentaci√≥n completa**: Ver `/home/rojaldo/cursos/agents/autogen.adoc`

## üìû Soporte

Si encuentras problemas:

1. Verifica que Ollama est√° ejecut√°ndose
2. Comprueba que tienes el modelo descargado
3. Revisa los logs del servidor Ollama
4. Intenta con otro modelo m√°s simple

## üìÑ Licencia

Ejemplos educativos para el curso de AutoGen.

---

**√öltima actualizaci√≥n**: 8 de Noviembre de 2024
**Estado**: ‚úÖ Completo - Todos los m√≥dulos 6-11 incluidos
