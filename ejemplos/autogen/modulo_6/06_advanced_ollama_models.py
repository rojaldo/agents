"""
M√≥dulo 6: Optimizaci√≥n y Costos
Ejemplo 6: Advanced Ollama Models - Gestor de modelos multi-proveedor
"""

import requests
from enum import Enum
from datetime import datetime


class LLMProvider(Enum):
    """Proveedores de LLM disponibles"""
    OLLAMA = "ollama"
    LOCAL = "local"
    HUGGINGFACE = "huggingface"


class OllamaModelManager:
    """Gestor de modelos Ollama con selecci√≥n autom√°tica seg√∫n tarea"""

    def __init__(self, base_url="http://localhost:11434"):
        self.base_url = base_url
        self.available_models = []
        self.model_characteristics = {
            "mistral": {"speed": "fast", "quality": "high", "specialty": ["code", "analysis"]},
            "neural-chat": {"speed": "very_fast", "quality": "medium", "specialty": ["general", "fast"]},
            "llama2": {"speed": "medium", "quality": "high", "specialty": ["analysis", "reasoning"]},
            "orca-mini": {"speed": "very_fast", "quality": "low", "specialty": ["fast", "summary"]},
        }
        self.task_preferences = {
            "code": ["mistral", "llama2", "neural-chat"],
            "analysis": ["mistral", "llama2", "orca-mini"],
            "creative": ["neural-chat", "mistral"],
            "fast": ["orca-mini", "neural-chat"],
            "reasoning": ["llama2", "mistral"],
            "general": ["mistral", "neural-chat"]
        }
        self.usage_stats = {}
        self.refresh_models()

    def refresh_models(self):
        """Actualizar lista de modelos disponibles"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                data = response.json()
                self.available_models = [m['name'].split(':')[0] for m in data.get('models', [])]
                print(f"‚úì Modelos disponibles: {', '.join(self.available_models) if self.available_models else 'Ninguno'}")
            else:
                print(f"‚úó Error al obtener modelos: {response.status_code}")
                self.available_models = []
        except requests.exceptions.ConnectionError:
            print("‚úó Error: No se puede conectar a Ollama")
            print("‚úó Aseg√∫rate de ejecutar: ollama serve")
            self.available_models = []
        except Exception as e:
            print(f"‚úó Error: {e}")
            self.available_models = []

    def select_best_model(self, task_type="general"):
        """Seleccionar mejor modelo seg√∫n tarea"""
        preferred = self.task_preferences.get(task_type, ["mistral"])

        for model in preferred:
            if any(m.startswith(model) for m in self.available_models):
                return model

        return self.available_models[0] if self.available_models else None

    def get_model_characteristics(self, model):
        """Obtener caracter√≠sticas del modelo"""
        return self.model_characteristics.get(model, {})

    def generate_with_fallback(self, prompt, task_type="general", temperature=0.7):
        """Generar con fallback autom√°tico a otro modelo si falla"""
        model = self.select_best_model(task_type)

        if not model:
            return "Error: No hay modelos disponibles"

        try:
            response = self.generate(prompt, model, temperature)
            self._record_usage(model, task_type, True)
            return response
        except Exception as e:
            print(f"‚ö†Ô∏è  Fallback: Modelo {model} fall√≥, intentando otro...")
            # Intentar con otro modelo
            for alt_model in self.available_models:
                if alt_model != model:
                    try:
                        response = self.generate(prompt, alt_model, temperature)
                        self._record_usage(alt_model, task_type, True)
                        return response
                    except:
                        continue

            return f"Error: Todos los modelos fallaron"

    def generate(self, prompt, model=None, temperature=0.7):
        """Generar respuesta con modelo espec√≠fico"""
        if model is None:
            model = self.select_best_model()

        payload = {
            "model": model,
            "prompt": prompt,
            "temperature": temperature,
            "stream": False
        }

        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                return result.get('response', '')
            else:
                return f"Error del servidor: {response.status_code}"

        except requests.exceptions.ConnectionError:
            return "Error: No se puede conectar a Ollama. ¬øEst√° ejecut√°ndose 'ollama serve'?"
        except requests.exceptions.Timeout:
            return "Error: Timeout esperando respuesta de Ollama"
        except Exception as e:
            return f"Error: {str(e)}"

    def _record_usage(self, model, task_type, success):
        """Registrar uso del modelo"""
        if model not in self.usage_stats:
            self.usage_stats[model] = {"calls": 0, "failures": 0}

        self.usage_stats[model]["calls"] += 1
        if not success:
            self.usage_stats[model]["failures"] += 1

    def get_usage_stats(self):
        """Obtener estad√≠sticas de uso"""
        return self.usage_stats

    def print_stats(self):
        """Imprimir estad√≠sticas"""
        print("\n" + "="*60)
        print("ESTAD√çSTICAS DE MODELOS")
        print("="*60)
        print(f"Modelos disponibles: {len(self.available_models)}")

        for model in self.available_models:
            chars = self.get_model_characteristics(model)
            print(f"\n{model}:")
            if chars:
                print(f"  Velocidad: {chars.get('speed', 'N/A')}")
                print(f"  Calidad: {chars.get('quality', 'N/A')}")
                print(f"  Especialidades: {', '.join(chars.get('specialty', []))}")

            if model in self.usage_stats:
                stats = self.usage_stats[model]
                success_rate = ((stats['calls'] - stats['failures']) / stats['calls'] * 100) if stats['calls'] > 0 else 0
                print(f"  Llamadas: {stats['calls']}, √âxito: {success_rate:.1f}%")

        print("="*60 + "\n")


def main():
    """Demostraci√≥n del Gestor de Modelos"""
    print("Demostraci√≥n: Advanced Ollama Models Manager")
    print("-" * 60)

    # Crear gestor
    manager = OllamaModelManager()

    # Verificar disponibilidad
    if not manager.available_models:
        print("\n‚ö†Ô∏è  Ollama no est√° disponible.")
        print("Para usar este ejemplo:")
        print("1. Instala Ollama: https://ollama.ai")
        print("2. Ejecuta: ollama serve")
        print("3. Descarga un modelo: ollama pull mistral")
        return

    # Pruebas de generaci√≥n
    test_cases = [
        ("code", "def fibonacci(n):\n    # Tu c√≥digo aqu√≠"),
        ("analysis", "Analiza el impacto de la IA"),
        ("creative", "Escribe un haiku sobre la naturaleza"),
        ("fast", "¬øQu√© es Python?"),
    ]

    print(f"\nüìù Usando gestor de modelos...")
    print(f"Modelos disponibles: {manager.available_models}\n")

    for task_type, prompt in test_cases:
        print(f"[Tarea: {task_type}]")
        print(f"Prompt: {prompt}")

        best_model = manager.select_best_model(task_type)
        print(f"Modelo seleccionado: {best_model}")

        response = manager.generate_with_fallback(prompt, task_type)

        if response.startswith("Error"):
            print(f"‚ùå {response}")
        else:
            print(f"‚úì Respuesta: {response[:80]}...")

        print()

    # Mostrar estad√≠sticas
    manager.print_stats()


if __name__ == "__main__":
    main()
