"""
M√ìDULO 8: Chat con Memoria
Sistema de chat RAG que mantiene contexto de conversaci√≥n
"""

import json
from typing import List
from datetime import datetime

# ============================================================================
# GESTOR DE MEMORIA DE CONVERSACI√ìN
# ============================================================================

class MemoriaConversacion:
    """Gestiona el historial de conversaci√≥n"""

    def __init__(self, max_mensajes: int = 10):
        self.mensajes = []
        self.max_mensajes = max_mensajes

    def agregar_mensaje(self, rol: str, contenido: str):
        """Agregar un mensaje al historial"""
        self.mensajes.append({
            "rol": rol,  # "usuario" o "asistente"
            "contenido": contenido,
            "timestamp": datetime.now().isoformat()
        })

        # Mantener solo los √∫ltimos N mensajes
        if len(self.mensajes) > self.max_mensajes:
            self.mensajes = self.mensajes[-self.max_mensajes:]

    def obtener_contexto(self) -> str:
        """Obtener el historial como contexto"""
        contexto = "Historial de conversaci√≥n:\n"
        for msg in self.mensajes:
            rol = "Usuario" if msg["rol"] == "usuario" else "Asistente"
            contexto += f"\n{rol}: {msg['contenido']}"
        return contexto

    def obtener_ultimos_mensajes(self, n: int = 3) -> List[dict]:
        """Obtener los √∫ltimos N mensajes"""
        return self.mensajes[-n:]

    def limpiar(self):
        """Limpiar historial"""
        self.mensajes = []


# ============================================================================
# DEMOSTRACI√ìN DE CHAT CON MEMORIA
# ============================================================================

def demostrar_chat_con_memoria():
    """Demostraci√≥n de un sistema de chat RAG con memoria"""

    print("=" * 70)
    print("M√ìDULO 8: Chat con Memoria")
    print("=" * 70)

    # Crear memoria
    memoria = MemoriaConversacion(max_mensajes=20)

    # Base de conocimiento simulada
    documentos_base = [
        {
            "titulo": "Python Basics",
            "contenido": "Python fue creado por Guido van Rossum en 1989. Es conocido por su sintaxis simple. Se usa para web, data science y automatizaci√≥n."
        },
        {
            "titulo": "RAG Systems",
            "contenido": "RAG combina recuperaci√≥n con generaci√≥n. Mejora la precisi√≥n de respuestas usando contexto. Ideal para QA sobre bases de conocimiento."
        },
        {
            "titulo": "Machine Learning",
            "contenido": "ML permite a m√°quinas aprender de datos sin programaci√≥n expl√≠cita. Incluye supervisado, no supervisado y refuerzo."
        },
        {
            "titulo": "LLMs",
            "contenido": "Los Modelos de Lenguaje Grande (LLMs) como GPT pueden generar texto coherente. Ollama permite ejecutarlos localmente sin APIs."
        }
    ]

    # Simulaci√≥n de conversaci√≥n
    conversacion = [
        "¬øQu√© es Python?",
        "¬øPara qu√© sirve?",
        "¬øQui√©n lo cre√≥?",
        "¬øC√≥mo se relaciona con Machine Learning?",
        "¬øPuedo usar RAG con Ollama?",
        "¬øC√≥mo mantengo la memoria en un chat?",
    ]

    print("\n" + "=" * 70)
    print("SIMULACI√ìN DE CHAT CON MEMORIA")
    print("=" * 70)

    resultados_chat = []

    for i, pregunta in enumerate(conversacion, 1):
        print(f"\nüì® TURNO {i}")
        print("-" * 70)

        # Usuario
        print(f"Usuario: {pregunta}")
        memoria.agregar_mensaje("usuario", pregunta)

        # Buscar documentos relevantes
        palabras_clave = pregunta.lower().split()
        doc_relevante = None
        for doc in documentos_base:
            if any(palabra in doc["contenido"].lower() for palabra in palabras_clave):
                doc_relevante = doc
                break

        # Generar respuesta con contexto y memoria
        contexto_conversacion = memoria.obtener_contexto()

        respuesta_simulada = f"""Bas√°ndome en el historial y el contexto:

Contexto: {doc_relevante["contenido"] if doc_relevante else "Informaci√≥n general"}

La respuesta es: {generar_respuesta_contextual(pregunta, doc_relevante)}"""

        print(f"Asistente: {respuesta_simulada[:100]}...")
        memoria.agregar_mensaje("asistente", respuesta_simulada)

        # Guardar resultado
        resultados_chat.append({
            "turno": i,
            "pregunta": pregunta,
            "documentos_utilizados": doc_relevante["titulo"] if doc_relevante else "Ninguno",
            "respuesta_preview": respuesta_simulada[:100]
        })

    # ========================================================================
    # MOSTRAR ESTADO DE MEMORIA
    # ========================================================================
    print("\n" + "=" * 70)
    print("ESTADO ACTUAL DE LA MEMORIA")
    print("=" * 70)

    print(f"Total de mensajes en memoria: {len(memoria.mensajes)}")
    print(f"Capacidad m√°xima: {memoria.max_mensajes}")
    print("\n√öltimos 3 mensajes:")

    for msg in memoria.obtener_ultimos_mensajes(3):
        rol = "üë§ Usuario" if msg["rol"] == "usuario" else "ü§ñ Asistente"
        contenido_preview = msg["contenido"][:60] + "..."
        print(f"  {rol}: {contenido_preview}")

    # ========================================================================
    # VENTAJAS DE MEMORIA EN CHAT
    # ========================================================================
    print("\n" + "=" * 70)
    print("VENTAJAS DE MEMORIA EN CHAT")
    print("=" * 70)

    ventajas = {
        "Contexto": "El chat entiende referencias a mensajes anteriores",
        "Coherencia": "Las respuestas son m√°s consistentes",
        "Personalizaci √≥n": "El asistente recuerda preferencias del usuario",
        "Reducci√≥n": "Se evita repetir informaci√≥n",
        "Eficiencia": "Menos tokens necesarios para el LLM"
    }

    for ventaja, descripci√≥n in ventajas.items():
        print(f"  ‚úì {ventaja}: {descripci√≥n}")

    # ========================================================================
    # C√ìDIGO PARA USAR CON OLLAMA
    # ========================================================================
    print("\n" + "=" * 70)
    print("C√ìDIGO PARA USAR CON OLLAMA")
    print("=" * 70)

    codigo = '''
from langchain_ollama import OllamaLLM
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

# Crear LLM
llm = OllamaLLM(model="mistral")

# Crear memoria
memory = ConversationBufferMemory()

# Crear cadena con memoria
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)

# Usar en conversaci√≥n
respuesta = conversation.predict(input="¬øQu√© es RAG?")
respuesta = conversation.predict(input="¬øPuedo usarlo en Ollama?")
    '''

    print(codigo)

    # ========================================================================
    # GUARDAR RESULTADOS
    # ========================================================================
    resultado_final = {
        "sistema": "Chat con Memoria",
        "turnos": len(conversacion),
        "mensajes_totales": len(memoria.mensajes),
        "conversacion": resultados_chat,
        "historial_completo": [
            {
                "rol": msg["rol"],
                "contenido": msg["contenido"][:100],
                "timestamp": msg["timestamp"]
            }
            for msg in memoria.mensajes
        ]
    }

    with open("chat_memoria.json", "w", encoding="utf-8") as f:
        json.dump(resultado_final, f, ensure_ascii=False, indent=2)

    print("\n‚úì Resultados guardados en chat_memoria.json")
    print("‚úÖ Chat con memoria demostrado exitosamente")


def generar_respuesta_contextual(pregunta: str, documento: dict) -> str:
    """Generar una respuesta contextual (simulado)"""
    respuestas_simuladas = {
        "python": "Python es un lenguaje vers√°til creado en 1989.",
        "machine": "Machine Learning es una rama de la IA que aprende de datos.",
        "rag": "RAG es una t√©cnica que combina b√∫squeda con generaci√≥n de texto.",
        "ollama": "Ollama permite ejecutar modelos de lenguaje localmente.",
        "memoria": "La memoria permite al chat mantener contexto de conversaci√≥n."
    }

    for clave, respuesta in respuestas_simuladas.items():
        if clave in pregunta.lower():
            return respuesta

    return "Esa es una pregunta interesante sobre temas complejos."


# ============================================================================
# MAIN
# ============================================================================

if __name__ == "__main__":
    demostrar_chat_con_memoria()
