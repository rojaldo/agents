"""
M√ìDULO 6: RAG B√°sico con Ollama
Sistema RAG completo y funcional con Ollama local
"""

import json
from typing import List
import subprocess
import sys

# ============================================================================
# VERIFICAR OLLAMA
# ============================================================================

def verificar_ollama():
    """Verificar si Ollama est√° corriendo"""
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags")
        if response.status_code == 200:
            modelos = response.json().get("models", [])
            print("‚úì Ollama est√° corriendo")
            print(f"  Modelos disponibles: {len(modelos)}")
            for modelo in modelos[:3]:
                print(f"    - {modelo.get('name', 'desconocido')}")
            return True
        else:
            print("‚úó Ollama no responde")
            return False
    except Exception as e:
        print(f"‚úó No se puede conectar a Ollama: {e}")
        return False


# ============================================================================
# SIMULACI√ìN DE RAG SIN OLLAMA (para pruebas)
# ============================================================================

class RAGSimple:
    """Sistema RAG simplificado para demostraci√≥n"""

    def __init__(self):
        self.documentos = [
            "RAG es Retrieval-Augmented Generation. Combina b√∫squeda de documentos con generaci√≥n de texto.",
            "Los embeddings convierten texto en vectores num√©ricos para b√∫squeda sem√°ntica.",
            "LangChain es un framework para construir aplicaciones con modelos de lenguaje.",
            "Ollama permite ejecutar modelos de lenguaje en tu computadora local.",
            "Los vector stores como ChromaDB almacenan y recuperan embeddings eficientemente."
        ]

        self.embeddings_doc = [
            [0.7, 0.2, 0.1, 0.3, 0.4, 0.5, 0.2],
            [0.2, 0.8, 0.3, 0.1, 0.2, 0.3, 0.4],
            [0.3, 0.2, 0.7, 0.2, 0.3, 0.4, 0.5],
            [0.1, 0.3, 0.2, 0.8, 0.5, 0.2, 0.3],
            [0.4, 0.3, 0.2, 0.3, 0.7, 0.1, 0.2],
        ]

    def buscar_documentos_relevantes(self, pregunta: str, top_k: int = 2) -> List[str]:
        """Buscar documentos relevantes (simulado)"""
        # En un sistema real, usar√≠amos embeddings
        # Aqu√≠ simulamos bas√°ndonos en palabras clave

        palabras_clave = pregunta.lower().split()
        scores = []

        for i, doc in enumerate(self.documentos):
            score = sum(1 for palabra in palabras_clave if palabra in doc.lower())
            scores.append((i, score))

        # Ordenar por score
        scores.sort(key=lambda x: x[1], reverse=True)

        # Retornar top K documentos
        documentos_relevantes = [self.documentos[i] for i, _ in scores[:top_k] if _ > 0]

        if not documentos_relevantes:
            # Si no hay coincidencias exactas, devolver los m√°s relevantes
            documentos_relevantes = [self.documentos[scores[j][0]] for j in range(top_k)]

        return documentos_relevantes

    def generar_respuesta(self, pregunta: str, contexto: str) -> str:
        """Generar respuesta basada en contexto (simulado)"""
        prompt = f"""Bas√°ndote en el siguiente contexto, responde la pregunta de forma concisa.

Contexto:
{contexto}

Pregunta: {pregunta}

Respuesta:"""

        # Simulaci√≥n de respuesta (en producci√≥n usar√≠amos Ollama)
        respuesta_simulada = f"""Bas√°ndome en el contexto proporcionado:

{contexto}

La respuesta es: Los concepto mencionados en el contexto son fundamentales para entender RAG. """

        return respuesta_simulada


# ============================================================================
# DEMOSTRACI√ìN DE RAG
# ============================================================================

def demostrar_rag():
    """Demostraci√≥n completa del sistema RAG"""

    print("=" * 70)
    print("M√ìDULO 6: RAG B√°sico con Ollama")
    print("=" * 70)

    # Verificar Ollama
    print("\nüîç VERIFICACI√ìN DE OLLAMA")
    print("-" * 70)
    ollama_disponible = verificar_ollama()

    # Crear sistema RAG
    rag = RAGSimple()

    # Preguntas de prueba
    preguntas = [
        "¬øQu√© es RAG?",
        "¬øC√≥mo funcionan los embeddings?",
        "¬øQu√© es LangChain?"
    ]

    print("\n" + "=" * 70)
    print("PREGUNTAS Y RESPUESTAS CON RAG")
    print("=" * 70)

    resultados = []

    for pregunta in preguntas:
        print(f"\n‚ùì PREGUNTA: {pregunta}")
        print("-" * 70)

        # 1. Recuperar documentos relevantes
        print("1Ô∏è‚É£ Buscando documentos relevantes...")
        documentos = rag.buscar_documentos_relevantes(pregunta, top_k=2)

        print(f"   ‚úì {len(documentos)} documentos encontrados")
        for i, doc in enumerate(documentos, 1):
            print(f"   {i}. {doc[:60]}...")

        # 2. Crear contexto
        contexto = "\n".join([f"- {doc}" for doc in documentos])

        # 3. Generar respuesta
        print("\n2Ô∏è‚É£ Generando respuesta con contexto...")
        respuesta = rag.generar_respuesta(pregunta, contexto)

        print("   ‚úì Respuesta generada")
        print(f"\n   {respuesta}")

        # Guardar resultado
        resultados.append({
            "pregunta": pregunta,
            "documentos_utilizados": documentos,
            "respuesta": respuesta
        })

    # ========================================================================
    # ARQUITECTURA DE RAG
    # ========================================================================
    print("\n" + "=" * 70)
    print("FLUJO DE RAG EXPLICADO")
    print("=" * 70)

    flujo = """
    1. INDEXACI√ìN (una sola vez):
       ‚îú‚îÄ Cargar documentos
       ‚îú‚îÄ Dividir en chunks
       ‚îú‚îÄ Crear embeddings
       ‚îî‚îÄ Almacenar en Vector DB

    2. RECUPERACI√ìN (en cada pregunta):
       ‚îú‚îÄ Crear embedding de la pregunta
       ‚îú‚îÄ Buscar similitud en Vector DB
       ‚îî‚îÄ Recuperar documentos relevantes

    3. GENERACI√ìN:
       ‚îú‚îÄ Construir prompt con contexto
       ‚îú‚îÄ Enviar al LLM
       ‚îî‚îÄ Retornar respuesta contextualizada
    """

    print(flujo)

    # ========================================================================
    # C√ìDIGO PARA USAR CON OLLAMA REAL
    # ========================================================================
    print("\n" + "=" * 70)
    print("C√ìDIGO PARA USAR CON OLLAMA REAL")
    print("=" * 70)

    codigo = '''
from langchain_ollama import OllamaLLM, OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter

# 1. PREPARAR DOCUMENTOS
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_text(documento)

# 2. CREAR VECTOR STORE
embeddings = OllamaEmbeddings(model="nomic-embed-text")
vector_store = Chroma.from_texts(chunks, embeddings)

# 3. CREAR CADENA RAG
llm = OllamaLLM(model="mistral")

def responder_pregunta(pregunta):
    docs = vector_store.similarity_search(pregunta, k=2)
    contexto = "\\n".join([doc.page_content for doc in docs])

    prompt = f"Bas√°ndote en: {contexto}\\n\\nPregunta: {pregunta}"
    return llm.invoke(prompt)

# 4. USAR
respuesta = responder_pregunta("¬øQu√© es RAG?")
print(respuesta)
    '''

    print(codigo)

    # ========================================================================
    # VENTAJAS DE RAG
    # ========================================================================
    print("\n" + "=" * 70)
    print("VENTAJAS DE RAG")
    print("=" * 70)

    ventajas = {
        "Precisi√≥n": "Respuestas basadas en datos reales",
        "Actualizaci√≥n": "F√°cil agregar nuevos documentos",
        "Privacidad": "Datos sensibles se quedan locales",
        "Trazabilidad": "Se pueden citar fuentes",
        "Eficiencia": "Modelos m√°s peque√±os funcionan mejor",
        "Local": "Todo corre en tu m√°quina"
    }

    for ventaja, descripci√≥n in ventajas.items():
        print(f"  ‚úì {ventaja}: {descripci√≥n}")

    # ========================================================================
    # GUARDAR RESULTADOS
    # ========================================================================
    resultado_final = {
        "ollama_disponible": ollama_disponible,
        "sistema": "RAG B√°sico",
        "modelo_recomendado": "mistral" if ollama_disponible else "N/A",
        "preguntas_respondidas": len(resultados),
        "resultados": resultados
    }

    with open("rag_resultado.json", "w", encoding="utf-8") as f:
        json.dump(resultado_final, f, ensure_ascii=False, indent=2)

    print("\n‚úì Resultados guardados en rag_resultado.json")
    print("‚úÖ RAG B√°sico demostrado exitosamente")


# ============================================================================
# MAIN
# ============================================================================

if __name__ == "__main__":
    demostrar_rag()
