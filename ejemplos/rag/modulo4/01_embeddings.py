"""
M√ìDULO 4: Embeddings
Convertir texto en vectores num√©ricos para b√∫squeda sem√°ntica
"""

import json
from typing import List

# ============================================================================
# SIMULACI√ìN DE EMBEDDINGS (sin LangChain)
# ============================================================================

class SimpleEmbedding:
    """Simulaci√≥n simple de embeddings usando t√©cnicas b√°sicas"""

    @staticmethod
    def crear_embedding_tf(texto: str, dimensi√≥n: int = 50) -> List[float]:
        """
        Crear embedding usando Term Frequency
        (simplificado para fines educativos)
        """
        palabras = texto.lower().split()
        vector = [0.0] * dimensi√≥n

        # Asignar valores basados en frecuencia de palabras
        freq_palabras = {}
        for palabra in palabras:
            freq_palabras[palabra] = freq_palabras.get(palabra, 0) + 1

        # Mapear a dimensiones
        for i, (palabra, freq) in enumerate(freq_palabras.items()):
            idx = hash(palabra) % dimensi√≥n
            vector[idx] += freq / len(palabras)

        # Normalizar
        norma = sum(v * v for v in vector) ** 0.5
        if norma > 0:
            vector = [v / norma for v in vector]

        return vector

    @staticmethod
    def similitud_coseno(v1: List[float], v2: List[float]) -> float:
        """Calcular similitud coseno entre dos vectores"""
        producto_punto = sum(a * b for a, b in zip(v1, v2))
        norma_v1 = sum(a * a for a in v1) ** 0.5
        norma_v2 = sum(b * b for b in v2) ** 0.5

        if norma_v1 == 0 or norma_v2 == 0:
            return 0.0

        return producto_punto / (norma_v1 * norma_v2)


# ============================================================================
# DEMOSTRACI√ìN CON LANGCHAIN Y OLLAMA
# ============================================================================

def demostrar_embeddings():
    """Demostraci√≥n de embeddings para RAG"""

    print("=" * 70)
    print("M√ìDULO 4: Embeddings")
    print("=" * 70)

    # Textos de ejemplo
    documentos = [
        "Python es un lenguaje de programaci√≥n poderoso y vers√°til",
        "El aprendizaje autom√°tico utiliza algoritmos para aprender de datos",
        "RAG combina b√∫squeda con generaci√≥n de texto inteligente",
        "Los embeddings convierten texto en vectores num√©ricos",
        "La similitud coseno mide la similitud entre dos vectores"
    ]

    pregunta = "¬øC√≥mo funcionan los embeddings?"

    print("\nüìù DOCUMENTOS DISPONIBLES:")
    print("-" * 70)
    for i, doc in enumerate(documentos, 1):
        print(f"{i}. {doc}")

    print(f"\n‚ùì PREGUNTA: {pregunta}")

    # ========================================================================
    # CREAR EMBEDDINGS (SIMULADO)
    # ========================================================================
    print("\n" + "=" * 70)
    print("CREANDO EMBEDDINGS")
    print("=" * 70)

    embedding_generator = SimpleEmbedding()

    # Generar embeddings para documentos
    embeddings_docs = []
    for i, doc in enumerate(documentos):
        embedding = embedding_generator.crear_embedding_tf(doc)
        embeddings_docs.append({
            "id": i,
            "texto": doc,
            "embedding": embedding,
            "dimensi√≥n": len(embedding)
        })
        print(f"‚úì Embedding {i + 1} creado (dimensi√≥n: {len(embedding)})")

    # Generar embedding para la pregunta
    embedding_pregunta = embedding_generator.crear_embedding_tf(pregunta)
    print(f"‚úì Embedding de pregunta creado (dimensi√≥n: {len(embedding_pregunta)})")

    # ========================================================================
    # B√öSQUEDA SEM√ÅNTICA
    # ========================================================================
    print("\n" + "=" * 70)
    print("B√öSQUEDA SEM√ÅNTICA - Encontrar documentos relevantes")
    print("=" * 70)

    similitudes = []
    for doc_data in embeddings_docs:
        similitud = embedding_generator.similitud_coseno(
            embedding_pregunta,
            doc_data["embedding"]
        )
        similitudes.append({
            "documento": doc_data["texto"],
            "similitud": similitud
        })

    # Ordenar por similitud
    similitudes.sort(key=lambda x: x["similitud"], reverse=True)

    print(f"\nDocumentos ordenados por similitud a: '{pregunta}'")
    print("-" * 70)
    for i, item in enumerate(similitudes, 1):
        barra = "‚ñà" * int(item["similitud"] * 20)
        print(f"{i}. [{item['similitud']:.3f}] {barra}")
        print(f"   {item['documento']}")

    # ========================================================================
    # INFORMACI√ìN T√âCNICA
    # ========================================================================
    print("\n" + "=" * 70)
    print("INFORMACI√ìN T√âCNICA")
    print("=" * 70)

    info = {
        "Tipo de embedding": "Term Frequency (simplificado)",
        "Dimensi√≥n": len(embedding_pregunta),
        "Similitud m√°xima": max(s["similitud"] for s in similitudes),
        "Similitud m√≠nima": min(s["similitud"] for s in similitudes),
        "Documentos procesados": len(documentos),
        "M√©todo de b√∫squeda": "Similitud Coseno"
    }

    for clave, valor in info.items():
        print(f"  ‚Ä¢ {clave}: {valor}")

    # ========================================================================
    # C√ìMO USAR CON OLLAMA EN PRODUCCI√ìN
    # ========================================================================
    print("\n" + "=" * 70)
    print("USANDO EMBEDDINGS REALES CON OLLAMA")
    print("=" * 70)

    codigo_ejemplo = '''
# Con LangChain y Ollama
from langchain_ollama import OllamaEmbeddings

# Crear embeddings con Ollama
embeddings = OllamaEmbeddings(model="nomic-embed-text")

# Crear embedding de un texto
vector = embeddings.embed_query("RAG es incre√≠ble")
print(f"Dimensi√≥n del vector: {len(vector)}")

# Crear embeddings de m√∫ltiples documentos
doc_vectors = embeddings.embed_documents([
    "Texto 1",
    "Texto 2",
    "Texto 3"
])
    '''

    print("\nC√≥digo Python para usar embeddings reales:")
    print("-" * 70)
    print(codigo_ejemplo)

    # ========================================================================
    # GUARDAR RESULTADOS
    # ========================================================================
    resultados = {
        "embeddings": [
            {
                "id": e["id"],
                "texto": e["texto"],
                "dimensi√≥n": e["dimensi√≥n"],
                "embedding_preview": e["embedding"][:5]  # Primeros 5 valores
            }
            for e in embeddings_docs
        ],
        "b√∫squeda": {
            "pregunta": pregunta,
            "resultados": similitudes[:3]  # Top 3
        }
    }

    with open("embeddings_result.json", "w", encoding="utf-8") as f:
        json.dump(resultados, f, ensure_ascii=False, indent=2)

    print("\n‚úì Resultados guardados en embeddings_result.json")
    print("‚úÖ Embeddings demostrados exitosamente")


# ============================================================================
# MAIN
# ============================================================================

if __name__ == "__main__":
    demostrar_embeddings()
