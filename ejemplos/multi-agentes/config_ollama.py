"""
config_ollama.py
Configuraci√≥n centralizada para LangChain + Ollama
"""

import os
from typing import Optional
from langchain_ollama import OllamaLLM

# ============================================================================
# CONFIGURACI√ìN GLOBAL
# ============================================================================

# URL del servidor Ollama local
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")

# Modelos disponibles (en orden de peso: m√°s ligero ‚Üí m√°s pesado)
MODELOS = {
    "tinyllama": {
        "nombre": "TinyLlama (1.1B)",
        "descripcion": "Ultraligero, muy r√°pido",
        "recomendado_para": "tests r√°pidos, desarrollo",
        "ram_minima": 2,  # GB
    },
    "orca-mini": {
        "nombre": "Orca Mini (3.3B)",
        "descripcion": "Ligero, buen balance",
        "recomendado_para": "desarrollo normal",
        "ram_minima": 4,
    },
    "neural-chat": {
        "nombre": "Neural Chat (7B)",
        "descripcion": "Conversacional, bueno para di√°logos",
        "recomendado_para": "agentes conversacionales",
        "ram_minima": 8,
    },
    "mistral": {
        "nombre": "Mistral (7B)",
        "descripcion": "R√°pido, muy capaz",
        "recomendado_para": "producci√≥n peque√±a",
        "ram_minima": 8,
    },
    "llama2": {
        "nombre": "Llama 2 (7B/13B)",
        "descripcion": "Potente, versatil",
        "recomendado_para": "tareas complejas",
        "ram_minima": 12,
    },
}

# Modelo por defecto
MODELO_DEFECTO = os.getenv("OLLAMA_MODEL", "mistral")

# Par√°metros de generaci√≥n
GENERATION_PARAMS = {
    "temperature": 0.7,  # Creatividad (0=determinista, 1=aleatorio)
    "top_p": 0.9,  # Nucleus sampling
    "num_ctx": 2048,  # Tama√±o contexto
}

# ============================================================================
# FACTORY FUNCTION
# ============================================================================

def obtener_llm(modelo: Optional[str] = None, temperatura: float = 0.7) -> OllamaLLM:
    """
    Factory para obtener instancia de LLM configurada.
    
    Args:
        modelo: nombre del modelo (si None, usa default)
        temperatura: valor entre 0 y 1
    
    Returns:
        OllamaLLM configurado y conectado
    
    Ejemplo:
        >>> llm = obtener_llm()
        >>> respuesta = llm.invoke("Hola mundo")
    """
    modelo_a_usar = modelo or MODELO_DEFECTO
    
    if modelo_a_usar not in MODELOS:
        modelos_disponibles = ", ".join(MODELOS.keys())
        raise ValueError(
            f"Modelo '{modelo_a_usar}' no reconocido.\n"
            f"Opciones: {modelos_disponibles}"
        )
    
    return OllamaLLM(
        model=modelo_a_usar,
        base_url=OLLAMA_BASE_URL,
        temperature=temperatura,
    )

# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def listar_modelos() -> None:
    """Muestra modelos disponibles con informaci√≥n."""
    print("\n" + "="*70)
    print("MODELOS DISPONIBLES EN OLLAMA")
    print("="*70)
    
    for key, info in MODELOS.items():
        print(f"\nüì¶ {key.upper()}")
        print(f"   Nombre: {info['nombre']}")
        print(f"   Descripci√≥n: {info['descripcion']}")
        print(f"   Recomendado: {info['recomendado_para']}")
        print(f"   RAM m√≠nima: {info['ram_minima']} GB")
        
        if key == MODELO_DEFECTO:
            print("   ‚≠ê MODELO POR DEFECTO")
    
    print("\n" + "="*70 + "\n")

def obtener_info_modelo(modelo: str) -> dict:
    """Retorna informaci√≥n de un modelo espec√≠fico."""
    if modelo not in MODELOS:
        raise ValueError(f"Modelo '{modelo}' no encontrado")
    return MODELOS[modelo]

# ============================================================================
# PROMPTS REUTILIZABLES
# ============================================================================

PROMPTS = {
    "respuesta_corta": (
        "Responde de forma concisa en m√°ximo 2 frases. "
        "Pregunta: {pregunta}"
    ),
    
    "respuesta_estructurada": (
        "Responde estructuradamente:\n"
        "1. Resumen en 1 l√≠nea\n"
        "2. Detalles clave (3-5 puntos)\n"
        "3. Conclusi√≥n\n\n"
        "Pregunta: {pregunta}"
    ),
    
    "razonamiento_paso_a_paso": (
        "Razona paso a paso:\n"
        "1. Analiza el problema\n"
        "2. Identifica pasos necesarios\n"
        "3. Explica cada paso\n"
        "4. Conclusi√≥n\n\n"
        "Problema: {pregunta}"
    ),
    
    "rol_agente": (
        "Eres un {rol}. Tu objetivo es {objetivo}.\n"
        "Responde mantiendo este rol.\n\n"
        "Pregunta: {pregunta}"
    ),
    
    "decision_agente": (
        "Eres un agente aut√≥nomo que debe tomar una decisi√≥n.\n"
        "Contexto: {contexto}\n"
        "Opciones: {opciones}\n"
        "Criterios: {criterios}\n\n"
        "¬øCu√°l es tu decisi√≥n y por qu√©?"
    ),
}

def obtener_prompt(template: str, **kwargs) -> str:
    """
    Obtiene prompt personalizado del diccionario.
    
    Args:
        template: clave del prompt en PROMPTS
        **kwargs: variables para reemplazar en el template
    
    Returns:
        Prompt formateado
    
    Ejemplo:
        >>> prompt = obtener_prompt("respuesta_corta", pregunta="¬øQu√© es IA?")
    """
    if template not in PROMPTS:
        plantillas_disponibles = ", ".join(PROMPTS.keys())
        raise ValueError(
            f"Template '{template}' no encontrado.\n"
            f"Opciones: {plantillas_disponibles}"
        )
    
    return PROMPTS[template].format(**kwargs)

# ============================================================================
# TEST
# ============================================================================

if __name__ == "__main__":
    print("üß™ TEST: config_ollama.py\n")
    
    # Test 1: Listar modelos
    print("‚úÖ Test 1: Listar modelos")
    listar_modelos()
    
    # Test 2: Obtener LLM
    print("‚úÖ Test 2: Obtener LLM (esto tomar√° unos segundos...)")
    try:
        llm = obtener_llm()
        print(f"   LLM obtenido: {MODELO_DEFECTO}")
        
        # Test 3: Invocar modelo
        print("\n‚úÖ Test 3: Invocar modelo")
        respuesta = llm.invoke("Di 'Hola mundo' en una sola l√≠nea")
        print(f"   Respuesta: {respuesta}\n")
        
    except Exception as e:
        print(f"   ‚ùå Error: {e}")
        print("\n   üí° Aseg√∫rate de que Ollama est√° corriendo:")
        print("      ollama serve")
        print("      (en otra terminal)")
        print("\n   üí° Y que el modelo est√° descargado:")
        print(f"      ollama pull {MODELO_DEFECTO}")
    
    # Test 4: Prompts reutilizables
    print("‚úÖ Test 4: Prompts reutilizables")
    prompt = obtener_prompt("respuesta_corta", pregunta="¬øQu√© es machine learning?")
    print(f"   Template 'respuesta_corta':\n   {prompt}\n")
    
    print("‚úÖ Todos los tests completados")
